{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_750_1300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CIBlQSpl0iaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a2ct7YLfVT5",
        "outputId": "faf6a7b1-8f2c-49b4-d772-34bf3e602514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t4_1762376560 step 550 loss 3.2604\n"
          ]
        }
      ],
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "log = json.loads((run/\"metrics\"/\"phase1_loss_log.json\").read_text())\n",
        "print(run.name, \"step\", log[-1][\"step\"], \"loss\", round(float(log[-1][\"loss\"]),4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "ck = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)[0]\n",
        "state = json.loads((ck/\"trainer_state.json\").read_text())\n",
        "ev = [d for d in state.get(\"log_history\", []) if \"eval_loss\" in d]\n",
        "print(ev[-1] if ev else \"no eval yet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWBAGHBE05lD",
        "outputId": "c5db6825-ef90-4537-e8b4-5e2d799b4fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 10.527472527472527, 'eval_loss': 7.096441268920898, 'eval_runtime': 13.6439, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'step': 600}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "snapshot"
      ],
      "metadata": {
        "id": "llry5ube8_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual snapshot\n",
        "import pathlib, shutil, time\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "src = latest_ckpt_with_weights(RUN)\n",
        "assert src is not None, \"No valid checkpoint with weights found yet.\"\n",
        "SNAPS = RUN/\"snapshots\"; SNAPS.mkdir(exist_ok=True, parents=True)\n",
        "dst = SNAPS/f\"snap_{int(time.time())}_{src.name}\"\n",
        "if not dst.exists():\n",
        "    shutil.copytree(src, dst)\n",
        "print(\"[SNAP] Saved\", dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqEsKdkn8_R8",
        "outputId": "6653c150-b5f6-4d7d-c969-76b50ed2daa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] Saved /content/drive/MyDrive/wake2vec/runs/t4_1762376560/snapshots/snap_1762565503_checkpoint-300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentry mirror"
      ],
      "metadata": {
        "id": "61EtZyYu9GZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newest full checkpoint + metrics\n",
        "import pathlib, shutil\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name; SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "ck = latest_ckpt_with_weights(RUN)\n",
        "if ck is None:\n",
        "    print(\"[SENTRY] No full checkpoint yet.\")\n",
        "else:\n",
        "    dst = SENTRY/ck.name\n",
        "    if not dst.exists():\n",
        "        shutil.copytree(ck, dst)\n",
        "        print(f\"[SENTRY] Mirrored {ck.name} → {dst}\")\n",
        "    else:\n",
        "        print(\"[SENTRY] Already mirrored:\", dst)\n",
        "\n",
        "# mirror metrics JSONs\n",
        "mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "for f in (RUN/\"metrics\").glob(\"*.json\"):\n",
        "    shutil.copy2(f, mdst/f.name)\n",
        "print(\"[SENTRY] Metrics mirrored →\", mdst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikqpbn0P9IZI",
        "outputId": "134a49e7-7430-468b-82b6-248f58c83e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirrored checkpoint-300 → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss and checkpoint"
      ],
      "metadata": {
        "id": "CzGNXVVp9jbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heartbeat\n",
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "# loss\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    print(f\"[LOSS] step={logs[-1]['step']}  loss={float(logs[-1]['loss']):.4f}\")\n",
        "else:\n",
        "    print(\"[LOSS] no metrics yet\")\n",
        "\n",
        "# checkpoints\n",
        "def scan(base):\n",
        "    rows=[]\n",
        "    for ck in sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        s=int(ck.name.split(\"-\")[-1])\n",
        "        rows.append((\n",
        "            s,\n",
        "            (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(),\n",
        "            (ck/\"trainer_state.json\").exists(),\n",
        "            (ck/\"optimizer.pt\").exists(),\n",
        "        ))\n",
        "    return rows\n",
        "\n",
        "print(\"\\n[RUNS]\")\n",
        "for s,w,t,o in scan(RUN):\n",
        "    print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "if SENTRY.exists():\n",
        "    print(\"\\n[SENTRY]\")\n",
        "    for s,w,t,o in scan(SENTRY):\n",
        "        print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "else:\n",
        "    print(\"\\n[SENTRY] none\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjpiKxqN9mFs",
        "outputId": "5c78aab1-bdc2-43c9-a0e1-ba56cadd382a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS] step=550  loss=3.2604\n",
            "\n",
            "[RUNS]\n",
            "   100  weights=True   state=True   opt=True \n",
            "   200  weights=True   state=True   opt=True \n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=True   opt=True \n",
            "   700  weights=False  state=True   opt=True \n",
            "\n",
            "[SENTRY]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean drive"
      ],
      "metadata": {
        "id": "CqniUpma9tAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Light flush to help sync small files\n",
        "import os, time, pathlib\n",
        "RUN = max((pathlib.Path(\"/content/drive/MyDrive/wake2vec\")/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "(open(RUN/\"_touch.sync\",\"w\")).write(str(time.time()))\n",
        "os.sync()\n",
        "print(\"[SYNC] touched + sync hinted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CW2IWyO9uUH",
        "outputId": "eab2fc1d-3869-4f53-d45f-912aae37715b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] touched + sync hinted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save if trainer is in scope"
      ],
      "metadata": {
        "id": "f-RvQNxm92Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trainer.save_model()\n",
        "    if hasattr(trainer, \"_save_checkpoint\"):\n",
        "        trainer._save_checkpoint(model=trainer.model, trial=None)\n",
        "    print(\"[TRAINER] save requested\")\n",
        "except NameError:\n",
        "    print(\"[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwlnYfrg-MEZ",
        "outputId": "b060273e-f977-4641-ebaf-2750fe52d87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RUN = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "\n",
        "# Loss heartbeat\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    print(f\"[LOSS] last step={logs[-1]['step']}  loss={float(logs[-1]['loss']):.4f}\")\n",
        "else:\n",
        "    print(\"[LOSS] no metrics yet\")\n",
        "\n",
        "# Checkpoint table\n",
        "def scan(base):\n",
        "    rows=[]\n",
        "    for ck in sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        s=int(ck.name.split(\"-\")[-1])\n",
        "        rows.append((s,\n",
        "                     (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(),\n",
        "                     (ck/\"trainer_state.json\").exists(),\n",
        "                     (ck/\"optimizer.pt\").exists()))\n",
        "    return rows\n",
        "\n",
        "print(\"\\n[RUNS]\")\n",
        "for s,w,t,o in scan(RUN):\n",
        "    print(f\"  {s:>4}  weights={w!s:5}  state={t!s:5}  opt={o!s:5}\")\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "if SENTRY.exists():\n",
        "    print(\"\\n[SENTRY]\")\n",
        "    for s,w,t,o in scan(SENTRY):\n",
        "        print(f\"  {s:>4}  weights={w!s:5}  state={t!s:5}  opt={o!s:5}\")\n",
        "else:\n",
        "    print(\"\\n[SENTRY] none\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hEMASYx_ZIr",
        "outputId": "e620cf55-77bd-4710-dce9-6d1a1021f072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS] last step=550  loss=3.2604\n",
            "\n",
            "[RUNS]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n",
            "\n",
            "[SENTRY]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted.\")"
      ],
      "metadata": {
        "id": "VuVTCDxICx2A",
        "outputId": "43839b01-93c6-42fa-d07b-44d68a1bb5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERBOSE mirror of latest full checkpoint (+ metrics) to sentry_backups\n",
        "import pathlib, shutil, time, os\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "CANDIDATES = [pathlib.Path(\"/content/runs\"), DRIVE/\"runs\"]\n",
        "\n",
        "def latest_run():\n",
        "    runs = []\n",
        "    for root in CANDIDATES:\n",
        "        if root.exists():\n",
        "            for p in root.glob(\"t4_*\"):\n",
        "                try:\n",
        "                    runs.append((p.stat().st_mtime, p))\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "    if not runs: return None\n",
        "    runs.sort(reverse=True)\n",
        "    return runs[0][1]\n",
        "\n",
        "def latest_ckpt_with_weights(run):\n",
        "    if not run: return None\n",
        "    cks = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cks:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "RUN = latest_run()\n",
        "print(\"[INFO] Active run:\", RUN if RUN else \"none\")\n",
        "CK  = latest_ckpt_with_weights(RUN)\n",
        "print(\"[INFO] Latest full ckpt:\", CK if CK else \"none\")\n",
        "\n",
        "if CK is None:\n",
        "    print(\"[SENTRY] No checkpoint with weights yet — will retry after next save.\")\n",
        "else:\n",
        "    SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "    SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "    DST = SENTRY/CK.name\n",
        "\n",
        "    src_mtime = time.ctime(CK.stat().st_mtime)\n",
        "    print(f\"[SENTRY] Source: {CK} (mtime {src_mtime})\")\n",
        "    if DST.exists():\n",
        "        # check if dest is older/stale by file count or mtime\n",
        "        src_files = sum(1 for _ in CK.rglob(\"*\"))\n",
        "        dst_files = sum(1 for _ in DST.rglob(\"*\"))\n",
        "        print(f\"[SENTRY] Already exists: {DST} (files src={src_files} dst={dst_files})\")\n",
        "        if dst_files < src_files:\n",
        "            print(\"[SENTRY] Detected partial mirror; refreshing…\")\n",
        "            shutil.rmtree(DST)\n",
        "            shutil.copytree(CK, DST)\n",
        "            print(\"[SENTRY] Re-mirrored:\", DST)\n",
        "        else:\n",
        "            print(\"[SENTRY] Mirror up-to-date.\")\n",
        "    else:\n",
        "        shutil.copytree(CK, DST)\n",
        "        print(\"[SENTRY] Mirrored:\", DST)\n",
        "\n",
        "    # Mirror metrics verbosely\n",
        "    msrc = RUN/\"metrics\"\n",
        "    mdst = SENTRY/\"metrics\"\n",
        "    mdst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    if msrc.exists():\n",
        "        for f in msrc.glob(\"*.json\"):\n",
        "            shutil.copy2(f, mdst/f.name)\n",
        "            copied += 1\n",
        "    print(f\"[SENTRY] Metrics mirrored → {mdst} ({copied} files)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJdM-MCcJoVP",
        "outputId": "8122a071-095e-4bb2-96ed-4f51e0f3c907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Active run: /content/drive/MyDrive/wake2vec/runs/t4_1762376560\n",
            "[INFO] Latest full ckpt: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Source: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 (mtime Wed Nov  5 22:45:46 2025)\n",
            "[SENTRY] Already exists: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300 (files src=12 dst=12)\n",
            "[SENTRY] Mirror up-to-date.\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "BASE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN = BASE/\"runs\"/\"t4_1762376560\"  # adjust if different\n",
        "SENTRY = BASE/\"sentry_backups\"/\"t4_1762376560\"\n",
        "\n",
        "def audit(root):\n",
        "    print(f\"\\n[{root}]\")\n",
        "    for ck in sorted(root.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        step = int(ck.name.split(\"-\")[-1])\n",
        "        w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "        t = (ck/\"trainer_state.json\").exists()\n",
        "        o = (ck/\"optimizer.pt\").exists()\n",
        "        print(f\"{step:>5}  weights={w:<5}  state={t:<5}  opt={o:<5}  → {ck.name}\")\n",
        "\n",
        "if RUN.exists():   audit(RUN)\n",
        "if SENTRY.exists(): audit(SENTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLwaBc-GKRK8",
        "outputId": "9c33c77a-c5bf-49ba-f91c-130eb5454270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[/content/drive/MyDrive/wake2vec/runs/t4_1762376560]\n",
            "  100  weights=1      state=1      opt=1      → checkpoint-100\n",
            "  200  weights=1      state=1      opt=1      → checkpoint-200\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=1      opt=1      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n",
            "\n",
            "[/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560]\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=0      opt=0      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "weights didnt save...retrain from 300"
      ],
      "metadata": {
        "id": "5-M1trtPOdoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "LOCAL_RUN = Path(f\"/content/runs/{RUN_ID}\")\n",
        "SENTRY = DRIVE / \"sentry_backups\" / RUN_ID\n",
        "EMB_SNAPS = DRIVE / \"emb_snaps\" / RUN_ID\n",
        "DATASETS = DRIVE / \"datasets\"\n",
        "\n",
        "# ensure local out exists\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "EMB_SNAPS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# sanity checks\n",
        "for p in [DRIVE, DRIVE/\"runs\", DRIVE/\"sentry_backups\", DRIVE/\"emb_snaps\", DRIVE/\"datasets\"]:\n",
        "    assert p.exists(), f\"Missing required path: {p}\"\n",
        "\n",
        "assert RESUME_FROM.exists(), f\"Checkpoint not found: {RESUME_FROM}\"\n",
        "assert (DATASETS/\"train_ds\").exists(), \"Missing train_ds\"\n",
        "assert (DATASETS/\"valid_ds\").exists(), \"Missing valid_ds\"\n",
        "\n",
        "print(\"[OK] Paths in place.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfuGraRZ2PRI",
        "outputId": "f1dcaed8-a52e-494b-caed-ac3e55dfeaa0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[OK] Paths in place.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pin + shim\n",
        "import sys, subprocess, importlib, os\n",
        "\n",
        "def pin(pkg, ver):\n",
        "    try:\n",
        "        m = importlib.import_module(pkg)\n",
        "        print(f\"[HAVE] {pkg} {m.__version__}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(f\"[PIN] {pkg}=={ver}\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
        "\n",
        "pin(\"transformers\", \"4.57.1\")\n",
        "pin(\"accelerate\",   \"1.2.1\")\n",
        "pin(\"datasets\",     \"2.21.0\")\n",
        "\n",
        "import accelerate\n",
        "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
        "    _orig = accelerate.Accelerator.unwrap_model\n",
        "    def _shim(self, model, *a, **kw):\n",
        "        kw.pop(\"keep_torch_compile\", None)\n",
        "        return _orig(self, model, *a, **kw)\n",
        "    accelerate.Accelerator.unwrap_model = _shim\n",
        "    accelerate.Accelerator._w2v_patched = True\n",
        "    print(\"[PATCH] unwrap_model shim active\")\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "ujRBiBquxgLi",
        "outputId": "22d0ed01-ed19-4eb4-f8f5-71e431b9a1d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HAVE] transformers 4.57.1\n",
            "[PIN] transformers==4.57.1\n",
            "[HAVE] accelerate 1.11.0\n",
            "[PIN] accelerate==1.2.1\n",
            "[HAVE] datasets 4.0.0\n",
            "[PIN] datasets==2.21.0\n",
            "[PATCH] unwrap_model shim active\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'expandable_segments:True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check t4\n",
        "import torch\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggFlClWMxpkr",
        "outputId": "7e46095a-aa0e-4f10-f8d4-2a74c373059d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resume from 750\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "from pathlib import Path\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = DRIVE/\"runs\"/RUN_ID/\"checkpoint-750-rebuilt\"\n",
        "assert RESUME_FROM.exists(), f\"Missing: {RESUME_FROM}\"\n",
        "print(\"[RESUME FROM]\", RESUME_FROM)\n",
        "\n",
        "LOCAL_RUN = Path(\"/content/runs\")/RUN_ID\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATASETS = DRIVE/\"datasets\"\n",
        "assert (DATASETS/\"train_ds\").exists() and (DATASETS/\"valid_ds\").exists(), \"Datasets missing.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIte54fMxwuq",
        "outputId": "2b56bfec-679a-465e-b56d-9aeb06557e8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[RESUME FROM] /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a valid trainer_state.json for a true resume at step 750\n",
        "from pathlib import Path\n",
        "import json\n",
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "RESUME_FROM = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "LAST_STEP = 750\n",
        "TARGET = 1300\n",
        "\n",
        "state = TrainerState()\n",
        "state.global_step = int(LAST_STEP)\n",
        "state.max_steps = int(TARGET)\n",
        "state.is_hyper_param_search = False\n",
        "state.log_history = []\n",
        "try:\n",
        "    # fyi\n",
        "    state.train_batch_size = int(args.per_device_train_batch_size * args.gradient_accumulation_steps)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "ts = RESUME_FROM / \"trainer_state.json\"\n",
        "ts.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# HF's own saver\n",
        "if hasattr(state, \"save_to_json\"):\n",
        "    state.save_to_json(str(ts))\n",
        "else:\n",
        "    payload = state.to_dict() if hasattr(state, \"to_dict\") else {\n",
        "        \"best_metric\": None,\n",
        "        \"best_model_checkpoint\": None,\n",
        "        \"epoch\": None,\n",
        "        \"global_step\": int(state.global_step),\n",
        "        \"is_hyper_param_search\": bool(state.is_hyper_param_search),\n",
        "        \"log_history\": list(state.log_history),\n",
        "        \"max_steps\": int(state.max_steps),\n",
        "        \"train_batch_size\": getattr(state, \"train_batch_size\", None),\n",
        "        \"total_flos\": getattr(state, \"total_flos\", 0),\n",
        "    }\n",
        "    ts.write_text(json.dumps(payload, indent=2))\n",
        "\n",
        "print(\"[RESUME] wrote schema-correct trainer_state.json at\", ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgDmdjrR7Hge",
        "outputId": "ac1f3210-71e8-4e8d-9b9d-8c93986dbe9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESUME] wrote schema-correct trainer_state.json at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt/trainer_state.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta→real load hotfix\n",
        "import torch, inspect\n",
        "_orig_load_sd = torch.nn.Module.load_state_dict\n",
        "def _load_state_dict_assign(self, state_dict, *args, **kwargs):\n",
        "    has_meta = any(getattr(p, \"is_meta\", False) for p in self.parameters())\n",
        "    if has_meta and \"assign\" in inspect.signature(_orig_load_sd).parameters:\n",
        "        kwargs.setdefault(\"assign\", True)\n",
        "    return _orig_load_sd(self, state_dict, *args, **kwargs)\n",
        "torch.nn.Module.load_state_dict = _load_state_dict_assign\n",
        "print(\"[META] load_state_dict hotfix active (assign=True on meta params)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IADHsVpK7VoH",
        "outputId": "686368fc-2c0f-49fd-d5e0-66acc6270d52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] load_state_dict hotfix active (assign=True on meta params)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy HF datasets from Drive\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "DATASETS_DRIVE = DRIVE / \"datasets\"\n",
        "DATASETS_LOCAL = Path(\"/content/datasets\")\n",
        "DATASETS_LOCAL.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_if_missing(sub):\n",
        "    src = DATASETS_DRIVE / sub\n",
        "    dst = DATASETS_LOCAL / sub\n",
        "    if not dst.exists():\n",
        "        print(f\"[COPY] {src} → {dst}\")\n",
        "        shutil.copytree(src, dst)\n",
        "    else:\n",
        "        print(f\"[SKIP] already local:\", dst)\n",
        "\n",
        "copy_if_missing(\"train_ds\")\n",
        "copy_if_missing(\"valid_ds\")\n",
        "\n",
        "# Use local path\n",
        "DATASETS = DATASETS_LOCAL\n",
        "print(\"[DATASETS] using\", DATASETS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuWDZODNLJrx",
        "outputId": "bc46993b-4e02-4b40-8917-9623045b077d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COPY] /content/drive/MyDrive/wake2vec/datasets/train_ds → /content/datasets/train_ds\n",
            "[COPY] /content/drive/MyDrive/wake2vec/datasets/valid_ds → /content/datasets/valid_ds\n",
            "[DATASETS] using /content/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# everything on one GPU, no meta/offload\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(RESUME_FROM),\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=None,          # <- critical\n",
        "    low_cpu_mem_usage=False   # <- build real tensors, not meta\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# tie head↔emb\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "try:\n",
        "    model.tie_weights()\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QRT_V7eLQsW",
        "outputId": "e717d0d1-e5b1-473f-bc3d-be54c54c08bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "CFTmui1oLaNU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p1 resume from 750\n",
        "import json, shutil, time, math, torch, numpy as np, os\n",
        "from pathlib import Path\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback)\n",
        "\n",
        "# configs\n",
        "FINAL_TARGET = 1300\n",
        "LAST_STEP = 750\n",
        "TARGET = min(FINAL_TARGET, LAST_STEP + 550)\n",
        "BASE_SAVE_STEPS = 75\n",
        "SWITCH_AT = 1000\n",
        "LATE_SAVE_STEPS = 100\n",
        "\n",
        "print(f\"[PLAN] last={LAST_STEP} → target={TARGET} | base_save_steps={BASE_SAVE_STEPS} | late_save={LATE_SAVE_STEPS} after {SWITCH_AT}\")\n",
        "\n",
        "# helpers\n",
        "def has_weights(ck_dir: Path) -> bool:\n",
        "    if not ck_dir.exists(): return False\n",
        "    if (ck_dir/\"model.safetensors\").exists() or (ck_dir/\"pytorch_model.bin\").exists():\n",
        "        return True\n",
        "    if list(ck_dir.glob(\"model-*-of-*.safetensors\")) or list(ck_dir.glob(\"pytorch_model-*-of-*.bin\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def latest_ckpt_with_weights(root: Path):\n",
        "    cks = [p for p in root.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "    cks = sorted(cks, key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "    cks = [p for p in cks if has_weights(p)]\n",
        "    return cks[-1] if cks else None\n",
        "\n",
        "def ensure_resume_stub(ckpt: Path, last_step: int, target: int, per_dev_bs=1, grad_accum=16, world_size=1):\n",
        "    \"\"\"\n",
        "    Create a minimal trainer_state.json so HF Trainer can resume from a rebuilt checkpoint.\n",
        "    This DOES NOT fabricate optimizer/scheduler state; it only restores the global step for\n",
        "    correct eval/save/snapshot scheduling.\n",
        "    \"\"\"\n",
        "    ts = ckpt/\"trainer_state.json\"\n",
        "    if ts.exists():\n",
        "        print(\"[RESUME] trainer_state.json exists — using it\")\n",
        "        return\n",
        "    stub = {\n",
        "        \"best_metric\": None,\n",
        "        \"best_model_checkpoint\": None,\n",
        "        \"epoch\": None,\n",
        "        \"global_step\": int(last_step),\n",
        "        \"is_hyper_param_search\": False,\n",
        "        \"log_history\": [],\n",
        "        \"max_steps\": int(target),\n",
        "        \"num_input_batches\": None,\n",
        "        \"train_batch_size\": int(per_dev_bs * grad_accum * world_size),\n",
        "        \"total_flos\": 0\n",
        "    }\n",
        "    ts.write_text(json.dumps(stub, indent=2))\n",
        "    print(f\"[RESUME] wrote stub trainer_state.json @ {ckpt} (global_step={last_step})\")\n",
        "\n",
        "# load tokenizer/model from 750\n",
        "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(str(RESUME_FROM), torch_dtype=torch.float32, device_map=\"auto\")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# tie lm_head to input embeddings\n",
        "with torch.no_grad():\n",
        "    if model.get_output_embeddings() is not None and model.get_input_embeddings() is not None:\n",
        "        model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "try:\n",
        "    model.tie_weights()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# datasets\n",
        "train_ds = load_from_disk(str(DATASETS/\"train_ds\"))\n",
        "valid_all = load_from_disk(str(DATASETS/\"valid_ds\"))\n",
        "valid_ds = valid_all.select(range(min(1000, len(valid_all))))\n",
        "dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "\n",
        "# callbacks\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        if s and s % self.n == 0:\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True\n",
        "\n",
        "class SaveCadenceSwitch(TrainerCallback):\n",
        "    \"\"\"Adds extra saves every 100 steps after SWITCH_AT.\"\"\"\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        if s >= SWITCH_AT and s % LATE_SAVE_STEPS == 0:\n",
        "            control.should_save = True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            cks = [p for p in LOCAL_RUN.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "            if not cks: return\n",
        "            ck = max(cks, key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "            if not has_weights(ck):\n",
        "                print(f\"[SENTRY] skip mirror, no weights in {ck.name}\")\n",
        "                return\n",
        "            dst = SENTRY/ck.name\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] mirrored {ck.name}\")\n",
        "            # metrics\n",
        "            msrc = LOCAL_RUN/\"metrics\"\n",
        "            if msrc.exists():\n",
        "                (SENTRY/\"metrics\").mkdir(parents=True, exist_ok=True)\n",
        "                for f in msrc.glob(\"*.json\"): shutil.copy2(f, (SENTRY/\"metrics\"/f.name))\n",
        "        except Exception as e:\n",
        "            print(\"[SENTRY] mirror failed:\", e)\n",
        "\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    def __init__(self, every=50): self.every = every\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        if s and s % self.every == 0:\n",
        "            try:\n",
        "                emb = model.get_input_embeddings().weight.detach().cpu()\n",
        "                out = (DRIVE/\"emb_snaps\"/RUN_ID)/f\"emb_step{int(s):04d}.pt\"\n",
        "                out.parent.mkdir(parents=True, exist_ok=True)\n",
        "                torch.save(emb, out)\n",
        "                (out.parent/\"heartbeat.json\").write_text(json.dumps(\n",
        "                    {\"run_id\": RUN_ID, \"step\": int(s), \"rows\": int(emb.size(0)), \"dim\": int(emb.size(1)), \"ts\": time.time()}, indent=2))\n",
        "                print(f\"[SNAP] embeddings → {out.name}\")\n",
        "            except Exception as e:\n",
        "                print(\"[SNAP] failed:\", e)\n",
        "\n",
        "# args\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=BASE_SAVE_STEPS,\n",
        "    save_total_limit=20,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False, bf16=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        "    dataloader_num_workers=2,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[EvalEveryNSteps(200), SaveCadenceSwitch(), SentryMirror(), EmbeddingSnap(50)],\n",
        ")\n",
        "\n",
        "# resume stub then train\n",
        "ensure_resume_stub(\n",
        "    RESUME_FROM,\n",
        "    last_step=LAST_STEP,\n",
        "    target=TARGET,\n",
        "    per_dev_bs=args.per_device_train_batch_size,\n",
        "    grad_accum=args.gradient_accumulation_steps,\n",
        "    world_size=1\n",
        ")\n",
        "\n",
        "print(\"[GO] training… (true HF resume from checkpoint-750-rebuilt)\")\n",
        "t0 = time.time()\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_FROM))\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "# save + mirror\n",
        "final_dir = LOCAL_RUN/\"checkpoint-final\"\n",
        "trainer.save_model(str(final_dir)); tok.save_pretrained(str(final_dir))\n",
        "\n",
        "dst = SENTRY/\"checkpoint-final\"\n",
        "if dst.exists(): shutil.rmtree(dst)\n",
        "shutil.copytree(final_dir, dst)\n",
        "\n",
        "last_ck = latest_ckpt_with_weights(LOCAL_RUN)\n",
        "print(\"\\n[DONE]\")\n",
        "print(\" final step target      :\", TARGET)\n",
        "print(\" elapsed (s)            :\", round(elapsed, 2))\n",
        "print(\" save cadence (base)    :\", f\"every {BASE_SAVE_STEPS} steps; plus every {LATE_SAVE_STEPS} ≥ {SWITCH_AT}\")\n",
        "print(\" last ckpt with weights :\", str(last_ck) if last_ck else \"N/A\")\n",
        "print(\" sentry mirror          :\", str(dst))\n",
        "print(\" latest emb snapshot    :\", max((DRIVE/'emb_snaps'/RUN_ID).glob('emb_step*.pt'), key=lambda p: p.name) if list((DRIVE/'emb_snaps'/RUN_ID).glob('emb_step*.pt')) else 'none yet')\n",
        "print(\" plots/report folder    :\", str(LOCAL_RUN/'plots'))\n",
        "print(\" tarball (after G)      :\", str(DRIVE/'archives'/f'{RUN_ID}.tar.gz'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "eX91D5Ge5Dq-",
        "outputId": "8560ca92-41ae-4a95-de10-0901d38b076e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PLAN] last=750 → target=1300 | base_save_steps=75 | late_save=100 after 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESUME] trainer_state.json exists — using it\n",
            "[GO] training… (true HF resume from checkpoint-750-rebuilt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tlogging_steps: 50 (from args) != 500 (from trainer_state.json)\n",
            "\tsave_steps: 75 (from args) != 500 (from trainer_state.json)\n",
            "\tper_device_train_batch_size: 1 (from args) != 16 (from trainer_state.json)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='766' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 766/1300 59:03 < 37:32:29, 0.00 it/s, Epoch 191.28/325]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1079813257.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[GO] training… (true HF resume from checkpoint-750-rebuilt)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESUME_FROM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mGeneralTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_blank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2740\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mon_main_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume P1 from best valid ckpt (300) → 750\n",
        "import pathlib, shutil, json, time, os, torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback)\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime).name\n",
        "DRIVE_RUN = DRIVE/\"runs\"/RUN_ID\n",
        "LOCAL_RUN = pathlib.Path(\"/content/runs\")/RUN_ID\n",
        "SENTRY    = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True); SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def list_ckpts(root):\n",
        "    if not root.exists(): return []\n",
        "    return sorted([p for p in root.glob(\"checkpoint-*\") if p.is_dir()],\n",
        "                  key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "\n",
        "def has_weights(ck):\n",
        "    return (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "\n",
        "# pick newest valid (weights) from runs or sentry (your table shows 300 is the last valid)\n",
        "cands = list_ckpts(DRIVE_RUN) + list_ckpts(SENTRY)\n",
        "GOOD = next((p for p in cands if has_weights(p)), None)\n",
        "assert GOOD is not None, \"No valid checkpoint with weights.\"\n",
        "last_step = int(GOOD.name.split(\"-\")[-1])\n",
        "print(f\"[RESUME] {RUN_ID} from {GOOD.name} (last_step={last_step})\")\n",
        "\n",
        "# seed local output_dir from that ckpt (fast, prevents partial Drive saves)\n",
        "if not (LOCAL_RUN/GOOD.name).exists():\n",
        "    shutil.copytree(GOOD, LOCAL_RUN/GOOD.name)\n",
        "    print(\"[LOCAL] seeded:\", (LOCAL_RUN/GOOD.name))\n",
        "\n",
        "# tokenizer + model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tok = AutoTokenizer.from_pretrained(str(GOOD), use_fast=True)\n",
        "if tok.pad_token_id is None: tok.pad_token = tok.eos_token or \"</s>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(str(GOOD), torch_dtype=torch.float32, device_map=\"auto\")\n",
        "model.config.use_cache = False\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight  # tie head\n",
        "\n",
        "# datasets\n",
        "train_ds = load_from_disk(str(DRIVE/\"datasets\"/\"train_ds\"))\n",
        "valid_ds = load_from_disk(str(DRIVE/\"datasets\"/\"valid_ds\")).select(range(min(1000, len(load_from_disk(str(DRIVE/'datasets'/'valid_ds'))))))\n",
        "dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "\n",
        "TARGET = 1300\n",
        "\n",
        "# Callbacks\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.n == 0:\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            ck = max(LOCAL_RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "            dst = SENTRY/ck.name\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] mirrored {ck.name}\")\n",
        "            # metrics\n",
        "            mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "            msrc = LOCAL_RUN/\"metrics\"\n",
        "            if msrc.exists():\n",
        "                for f in msrc.glob(\"*.json\"): shutil.copy2(f, mdst/f.name)\n",
        "            os.sync()\n",
        "        except Exception as e:\n",
        "            print(\"[SENTRY] mirror failed:\", e)\n",
        "\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    def __init__(self, every=50):\n",
        "        self.every = every\n",
        "        (DRIVE/\"emb_snaps\"/RUN_ID).mkdir(parents=True, exist_ok=True)\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.every == 0:\n",
        "            try:\n",
        "                emb = model.get_input_embeddings().weight.detach().cpu()\n",
        "                path = DRIVE/\"emb_snaps\"/RUN_ID/f\"emb_step{int(state.global_step):04d}.pt\"\n",
        "                torch.save(emb, path)\n",
        "                (DRIVE/\"emb_snaps\"/RUN_ID/\"heartbeat.json\").write_text(json.dumps(\n",
        "                    {\"step\": int(state.global_step), \"rows\": int(emb.size(0)), \"dim\": int(emb.size(1)), \"ts\": time.time()}, indent=2))\n",
        "                print(f\"[SNAP] embeddings → {path.name}\")\n",
        "            except Exception as e:\n",
        "                print(\"[SNAP] failed:\", e)\n",
        "\n",
        "# freq at 50 until 700, then 100\n",
        "save_steps = 50 if last_step < 700 else 100\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\", save_steps=save_steps, save_total_limit=20,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False, bf16=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[EvalEveryNSteps(200), SentryMirror(), EmbeddingSnap(50)],\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=str(LOCAL_RUN/GOOD.name))\n",
        "# Finalize\n",
        "trainer.save_model(str(LOCAL_RUN/\"checkpoint-final\")); tok.save_pretrained(str(LOCAL_RUN/\"checkpoint-final\"))\n",
        "# Mirror final\n",
        "dst = SENTRY/\"checkpoint-final\"\n",
        "if dst.exists(): shutil.rmtree(dst)\n",
        "shutil.copytree(LOCAL_RUN/\"checkpoint-final\", dst)\n",
        "print(\"[SENTRY] mirrored checkpoint-final\")\n",
        "print(\"[DONE] Reached\", TARGET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "tLSo5SLZGpLR",
        "outputId": "16aede5c-a9f0-4350-852f-3c8540b57d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESUME] t4_1762376560 from checkpoint-300 (last_step=300)\n",
            "[LOCAL] seeded: /content/runs/t4_1762376560/checkpoint-300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tsave_steps: 50 (from args) != 100 (from trainer_state.json)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='767' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 767/1300 3:03:39 < 3:30:31, 0.04 it/s, Epoch 13.44/23]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>5.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.308200</td>\n",
              "      <td>6.273335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>4.777600</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.089100</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.260400</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.425900</td>\n",
              "      <td>7.170007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.554100</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.798300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.319200</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] embeddings → emb_step0350.pt\n",
            "[SNAP] embeddings → emb_step0400.pt\n",
            "[SNAP] embeddings → emb_step0450.pt\n",
            "[SNAP] embeddings → emb_step0500.pt\n",
            "[SNAP] embeddings → emb_step0550.pt\n",
            "[SNAP] embeddings → emb_step0600.pt\n",
            "[SNAP] embeddings → emb_step0650.pt\n",
            "[SNAP] embeddings → emb_step0700.pt\n",
            "[SNAP] embeddings → emb_step0750.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA:\", torch.cuda.is_available(), \"| device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyEsFTWMF_mQ",
        "outputId": "cfa2420b-08df-43a8-805c-3ddede0decda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True | device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pin + shim\n",
        "import sys, subprocess, importlib, os\n",
        "\n",
        "def pin(pkg, ver):\n",
        "    try:\n",
        "        m = importlib.import_module(pkg)\n",
        "        assert m.__version__ == ver\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "    except Exception:\n",
        "        print(f\"[PIN] {pkg}=={ver}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
        "        m = importlib.import_module(pkg)\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "\n",
        "pin(\"transformers\", \"4.57.1\")\n",
        "pin(\"accelerate\",   \"1.2.1\")\n",
        "pin(\"datasets\",     \"2.21.0\")\n",
        "\n",
        "# unwrap_model shim (ignore keep_torch_compile kw)\n",
        "import accelerate\n",
        "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
        "    _orig = accelerate.Accelerator.unwrap_model\n",
        "    def _shim(self, model, *args, **kw):\n",
        "        kw.pop(\"keep_torch_compile\", None)\n",
        "        return _orig(self, model, *args, **kw)\n",
        "    accelerate.Accelerator.unwrap_model = _shim\n",
        "    accelerate.Accelerator._w2v_patched = True\n",
        "    print(\"[PATCH] unwrap_model shim active\")\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "1q3lzL0pF2X0",
        "outputId": "2991c94e-1854-4bdf-aa5c-bb75914eaa99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] transformers 4.57.1\n",
            "[PIN] accelerate==1.2.1\n",
            "[OK] accelerate 1.11.0\n",
            "[PIN] datasets==2.21.0\n",
            "[OK] datasets 4.0.0\n",
            "[PATCH] unwrap_model shim active\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'expandable_segments:True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force resume from rebuilt 750\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = f\"/content/drive/MyDrive/wake2vec/runs/{RUN_ID}/checkpoint-750-rebuilt\"\n",
        "print(\"[RESUME FROM]\", RESUME_FROM)\n",
        "\n",
        "FINAL_TARGET = 1300\n",
        "last_step = 750\n",
        "TARGET = min(FINAL_TARGET, last_step + 300)\n",
        "save_steps = 75 if last_step < 1000 else 100\n",
        "print(f\"[PLAN] last={last_step} → target={TARGET} | save_steps={save_steps}\")\n",
        "\n",
        "RESUME_FROM = f\"/content/drive/MyDrive/wake2vec/runs/{RUN_ID}/checkpoint-750-rebuilt\"\n",
        "print(\"[RESUME FROM]\", RESUME_FROM)"
      ],
      "metadata": {
        "id": "npp7gTpm8GdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for p1 eval"
      ],
      "metadata": {
        "id": "jRvQ2ZhVQ389"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TRAIN output_dir:\", trainer.args.output_dir)\n",
        "print(\"RUN_ID:\", pathlib.Path(trainer.args.output_dir).name)\n",
        "print(\"global_step:\", trainer.state.global_step)"
      ],
      "metadata": {
        "id": "4Q8WwTrs-zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from transformers import TrainingArguments\n",
        "'evaluation_strategy' in inspect.signature(TrainingArguments.__init__).parameters"
      ],
      "metadata": {
        "id": "eC9-VVNQ2KC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and (state.global_step % self.n == 0):\n",
        "            control.should_save = True         # ensure state gets flushed\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True"
      ],
      "metadata": {
        "id": "yX9eVZvc2SCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# P1 finalise overlap@5, norm stats, and a loss plot\n",
        "import json, numpy as np, pathlib, matplotlib.pyplot as plt\n",
        "\n",
        "DRIVE_ROOT = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS = sorted((DRIVE_ROOT/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "RUN_DIR = pathlib.Path(\"/content/runs\")/RUNS[-1].name\n",
        "METRICS_DIR = RUN_DIR/\"metrics\"\n",
        "PLOTS_DIR = RUN_DIR/\"plots\"; PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load current embeddings\n",
        "from transformers import AutoModelForCausalLM\n",
        "BASE_CKPT = RUN_DIR/\"checkpoint-final\"\n",
        "model = AutoModelForCausalLM.from_pretrained(str(BASE_CKPT), torch_dtype=\"float32\", device_map=None)\n",
        "E_post = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "\n",
        "# Optional composed init + ids\n",
        "E_COMP = DRIVE_ROOT/\"E_comp.npy\"\n",
        "NEW_IDS = DRIVE_ROOT/\"new_ids.npy\"\n",
        "has_comp = E_COMP.exists() and NEW_IDS.exists()\n",
        "\n",
        "def topk_overlap(a, b, k=5):\n",
        "    import numpy as np\n",
        "    from numpy.linalg import norm\n",
        "    a = a / (norm(a, axis=1, keepdims=True)+1e-9)\n",
        "    b = b / (norm(b, axis=1, keepdims=True)+1e-9)\n",
        "    sims = a @ b.T\n",
        "    top_a = np.argsort(-sims, axis=1)[:, :k]\n",
        "    top_b = np.argsort(-sims, axis=1)[:, :k]\n",
        "    inter = np.array([len(set(top_a[i]) & set(top_b[i])) for i in range(a.shape[0])])\n",
        "    return inter.mean()\n",
        "\n",
        "report = {}\n",
        "\n",
        "if has_comp:\n",
        "    E_comp = np.load(E_COMP)\n",
        "    new_ids = np.load(NEW_IDS)\n",
        "    E_post_new = E_post[new_ids]\n",
        "    overlap5 = topk_overlap(E_comp, E_post_new, k=5)\n",
        "    # Norm drift\n",
        "    from numpy.linalg import norm\n",
        "    dn = (norm(E_post_new, axis=1) - norm(E_comp, axis=1)).mean()\n",
        "    report.update({\"overlap_at_5\": float(overlap5), \"mean_delta_norm\": float(dn), \"n_new\": int(len(new_ids))})\n",
        "else:\n",
        "    # Fallback\n",
        "    from numpy.linalg import norm\n",
        "    norms = norm(E_post, axis=1)\n",
        "    report.update({\"post_mean_norm\": float(norms.mean()), \"post_std_norm\": float(norms.std()), \"n_vocab\": int(E_post.shape[0])})\n",
        "\n",
        "# JSON\n",
        "(METRICS_DIR/\"p1_summary.json\").write_text(json.dumps(report, indent=2))\n",
        "print(\"[P1 SUMMARY]\", json.dumps(report, indent=2))\n",
        "\n",
        "# Loss plot\n",
        "import json, glob\n",
        "state_files = [RUN_DIR/\"trainer_state.json\", BASE_CKPT/\"trainer_state.json\"]\n",
        "state_files = [p for p in state_files if p.exists()]\n",
        "logs = []\n",
        "for sf in state_files:\n",
        "    s = json.loads(sf.read_text())\n",
        "    logs.extend([d for d in s.get(\"log_history\", []) if \"loss\" in d])\n",
        "\n",
        "if logs:\n",
        "    steps = [d[\"step\"] for d in logs]\n",
        "    losses = [float(d[\"loss\"]) for d in logs]\n",
        "    ema = []\n",
        "    alpha = 0.1\n",
        "    for i,x in enumerate(losses):\n",
        "        ema.append(x if i==0 else alpha*x + (1-alpha)*ema[-1])\n",
        "    plt.figure(figsize=(7,4.5))\n",
        "    plt.plot(steps, losses, label=\"loss\")\n",
        "    plt.plot(steps, ema, label=\"EMA(0.1)\")\n",
        "    plt.title(f\"P1 Loss — {RUN_DIR.name}\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
        "    outp = PLOTS_DIR/\"p1_loss_curve.png\"\n",
        "    plt.savefig(outp, dpi=140, bbox_inches=\"tight\")\n",
        "    print(\"[PLOT]\", outp)\n",
        "else:\n",
        "    print(\"[WARN] No trainer_state logs found; skip loss plot.\")"
      ],
      "metadata": {
        "id": "5AMneAlxqpmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}