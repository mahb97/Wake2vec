# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gxnSrMNzJRyqTwloQU0Gi6h0FIJPkA9f
"""

# ╔══════════════════════════════════════════════════════════════════╗
# ║  wake2vec — Embedding Animation Pipeline                         ║
# ║  Watch 45K Wake tokens find their place in embedding space       ║
# ║  "riverrun, past Eve and Adam's, from swerve of shore..."        ║
# ╚══════════════════════════════════════════════════════════════════╝
#
# Loads P1 embedding snapshots (every 50 training steps),
# samples 200 random Wake tokens + 18 Ogden's Basic English landmarks,
# projects to 2D via PCA, and outputs:
#   - MP4 animation (matplotlib)
#   - Interactive HTML (plotly with step slider)

# Setup & Config
!pip install -q plotly

import torch
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from sklearn.decomposition import PCA
import glob
import re
import os
import json
from pathlib import Path
from transformers import AutoTokenizer

# paths (adjust if your Drive mount is different) ──
WAKE2VEC_ROOT = Path("/content/drive/MyDrive/wake2vec")
SNAP_DIR      = Path("/content/drive/MyDrive/wake2vec_tiny_p1_fryembeds/emb_snaps")    # P1 snapshots
LEXICON_PATH  = Path("/content/wake_lexicon.txt")
TOKENIZER_DIR = Path("/content/drive/MyDrive/wake2vec_tiny_p1_fryembeds/checkpoint-700")    # any checkpoint with the expanded tokenizer
OGDEN_PATH    = Path("/content/odgen_basic_full.txt")

#config
BASE_VOCAB   = 32000     # TinyLlama base vocab size (Wake tokens start here)
N_SAMPLE     = 200       # how many Wake tokens to animate (keeps it readable)
N_LANDMARKS  = 18        # how many Ogden's words as reference points
SEED         = 42        # for reproducible sampling
TRAIL_LEN    = 5         # how many previous positions to show as trails
OUTPUT_MP4   = WAKE2VEC_ROOT / "wake_embedding_animation.mp4"
OUTPUT_GIF   = WAKE2VEC_ROOT / "wake_embedding_animation.gif"
OUTPUT_HTML  = WAKE2VEC_ROOT / "wake_embedding_animation.html"

print("wake2vec embedding animation pipeline")
print(f"Snapshot dir: {SNAP_DIR}")
print(f"Lexicon: {LEXICON_PATH}")
print(f"Tokenizer: {TOKENIZER_DIR}")
print(f"Ogden's: {OGDEN_PATH}")

# Load Tokenizer & Lexicon
print("\n── Loading tokenizer ──")
tok = AutoTokenizer.from_pretrained(str(TOKENIZER_DIR))
print(f"Vocab size: {len(tok)}")

print("\n── Loading Wake lexicon ──")
with open(LEXICON_PATH, "r", encoding="utf-8") as f:
    wake_words = [line.strip() for line in f if line.strip()]
print(f"Lexicon: {len(wake_words)} words")

# map each wake word to its token ID
# the tokenizer should have these as single tokens (they were added in P1)
wake_word_to_id = {}
failed = []
for word in wake_words:
    ids = tok.encode(word, add_special_tokens=False)
    if len(ids) == 1 and ids[0] >= BASE_VOCAB:
        wake_word_to_id[word] = ids[0]
    else:
        failed.append(word)

# If tokenizer mapping got less than half, fall back to positional mapping
if len(wake_word_to_id) < len(wake_words) // 2:
    print("Tokenizer mapping incomplete — using positional mapping (BASE_VOCAB + index)")
    wake_word_to_id = {}
    for i, word in enumerate(wake_words):
        wake_word_to_id[word] = BASE_VOCAB + i

print(f"Mapped: {len(wake_word_to_id)} / {len(wake_words)} Wake tokens")
if failed:
    print(f"Failed to map (first 10): {failed[:10]}")

# show a few examples
samples = list(wake_word_to_id.items())[:8]
print("Sample mappings:")
for word, tid in samples:
    print(f"  '{word}' -> token ID {tid}")

# Sample Tokens & Load Snapshots
rng = np.random.RandomState(SEED)

# sample 200 Wake tokens (initial selection before bounds check)
all_wake_ids = list(wake_word_to_id.values())
all_wake_words_mapped = list(wake_word_to_id.keys())
num_wake_to_sample = min(N_SAMPLE, len(all_wake_ids))
initial_sample_indices = rng.choice(len(all_wake_ids), size=num_wake_to_sample, replace=False)
initial_sampled_wake_ids = [all_wake_ids[i] for i in initial_sample_indices]
initial_sampled_wake_words = [all_wake_words_mapped[i] for i in initial_sample_indices]

# sample 18 Ogden's landmarks (this part is fine as is)
with open(OGDEN_PATH, "r", encoding="utf-8") as f:
    ogden_raw = f.read()
# ogden file uses \r as delimiter
ogden_words = [w.strip() for w in ogden_raw.replace('\n', '\r').split('\r') if w.strip()]
# filter to words that are single tokens in base vocab
ogden_candidates = []
for word in ogden_words:
    ids = tok.encode(word, add_special_tokens=False)
    if len(ids) == 1 and ids[0] < BASE_VOCAB:
        ogden_candidates.append((word, ids[0]))

print(f"\nOgden's candidates (single base tokens): {len(ogden_candidates)}")
landmark_indices = rng.choice(len(ogden_candidates), size=min(N_LANDMARKS, len(ogden_candidates)), replace=False)
landmark_words = [ogden_candidates[i][0] for i in landmark_indices]
landmark_ids = [ogden_candidates[i][1] for i in landmark_indices]
print(f"Selected landmarks: {landmark_words}")

# load all snapshots (moved this section up to get E.shape)
snap_files = sorted(glob.glob(str(SNAP_DIR / "emb_step*.pt")))
# extract step numbers and sort
def get_step(path):
    m = re.search(r'emb_step(\d+)\.pt', path)
    return int(m.group(1)) if m else 0

snap_files = sorted(snap_files, key=get_step)
step_numbers = [get_step(f) for f in snap_files]
print(f"\nFound {len(snap_files)} snapshots")
print(f"Steps: {step_numbers[0]} to {step_numbers[-1]}")

if not snap_files:
    raise FileNotFoundError(f"No snapshot files found in {SNAP_DIR}. Please check the path and file existence.")

# Determine max valid embedding index from the first snapshot
first_E = torch.load(snap_files[0], map_location="cpu")
max_embedding_idx = first_E.shape[0] - 1
print(f"Embedding matrix size (first snapshot): {first_E.shape[0]} (max index {max_embedding_idx})")

# Filter initially sampled wake tokens to ensure their IDs are within the embedding matrix bounds
sampled_wake_ids = []
sampled_wake_words = []
dropped_wake_tokens_count = 0
for idx, token_id in enumerate(initial_sampled_wake_ids):
    if token_id <= max_embedding_idx:
        sampled_wake_ids.append(token_id)
        sampled_wake_words.append(initial_sampled_wake_words[idx])
    else:
        dropped_wake_tokens_count += 1

if dropped_wake_tokens_count > 0:
    print(f"Warning: Dropped {dropped_wake_tokens_count} sampled wake tokens because their IDs (>= {max_embedding_idx+1}) were out of bounds for the embedding matrix.")
    print("This indicates a mismatch between the tokenizer's assigned token IDs (using positional mapping) and the available embedding snapshots.")

# Update N_SAMPLE globally to reflect the actual number of valid sampled tokens
global N_SAMPLE
N_SAMPLE = len(sampled_wake_ids)

print(f"\nSampled {N_SAMPLE} Wake tokens for animation (after filtering).")
if N_SAMPLE > 0:
    print(f"Examples: {sampled_wake_words[:10]}")
else:
    print("No valid wake tokens available for animation.")


# extract rows for sampled tokens
print("\nLoading snapshots and extracting rows...")
wake_positions = []    # [n_steps, N_SAMPLE, hidden_dim]
landmark_positions = [] # [n_steps, N_LANDMARKS, hidden_dim]

for i, fpath in enumerate(snap_files):
    E = torch.load(fpath, map_location="cpu")
    # extract wake token rows (only if N_SAMPLE > 0)
    if N_SAMPLE > 0:
        wake_rows = E[sampled_wake_ids].numpy()
    else:
        wake_rows = np.array([]).reshape(0, E.shape[1]) # Empty array if no wake tokens

    # extract landmark rows
    land_rows = E[landmark_ids].numpy()

    wake_positions.append(wake_rows)
    landmark_positions.append(land_rows)
    if (i + 1) % 10 == 0 or i == 0:
        print(f"  Loaded {i+1}/{len(snap_files)} (step {step_numbers[i]}, shape {E.shape})")

wake_positions = np.array(wake_positions)      # [n_steps, N_SAMPLE, hidden_dim]
landmark_positions = np.array(landmark_positions) # [n_steps, N_LANDMARKS, hidden_dim]
print(f"\nWake positions shape: {wake_positions.shape}")
print(f"Landmark positions shape: {landmark_positions.shape}")

# PCA Projection
print("\n── PCA projection ──")

# fit PCA on the FINAL snapshot's wake rows (so axes are stable)
pca = PCA(n_components=2, random_state=SEED)
pca.fit(wake_positions[-1])
print(f"PCA explained variance: {pca.explained_variance_ratio_}")
print(f"Total variance explained: {sum(pca.explained_variance_ratio_):.3f}")

# transform all snapshots through the same projection
n_steps = wake_positions.shape[0]
wake_2d = np.zeros((n_steps, N_SAMPLE, 2))
landmark_2d = np.zeros((n_steps, N_LANDMARKS, 2))

for i in range(n_steps):
    wake_2d[i] = pca.transform(wake_positions[i])
    landmark_2d[i] = pca.transform(landmark_positions[i])

print(f"Wake 2D shape: {wake_2d.shape}")
print(f"Landmark 2D shape: {landmark_2d.shape}")

# check how much landmarks move (should be minimal, gradient-masked in P1)
landmark_drift = np.linalg.norm(landmark_2d[-1] - landmark_2d[0], axis=1)
wake_drift = np.linalg.norm(wake_2d[-1] - wake_2d[0], axis=1)
print(f"\nLandmark drift (mean): {landmark_drift.mean():.4f}")
print(f"Wake drift (mean): {wake_drift.mean():.4f}")
print(f"Wake drift (max): {wake_drift.max():.4f}")
print(f"Wake drift (min): {wake_drift.min():.4f}")

# Static Frame Preview
print("\n── Static preview: first vs last frame ──")

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

for ax, frame_idx, title in [(axes[0], 0, f"Step {step_numbers[0]} (initial)"),
                               (axes[1], -1, f"Step {step_numbers[-1]} (final)")]:
    ax.set_facecolor('#0a0a0a')
    fig.patch.set_facecolor('#0a0a0a')

    # wake tokens — bright cyan
    ax.scatter(wake_2d[frame_idx, :, 0], wake_2d[frame_idx, :, 1],
               c='#00ffcc', s=12, alpha=0.7, zorder=3, label='Wake tokens')

    # landmarks — gray with labels
    ax.scatter(landmark_2d[frame_idx, :, 0], landmark_2d[frame_idx, :, 1],
               c='#888888', s=40, alpha=0.9, zorder=4, marker='s', label='Ogden\'s landmarks')
    for j, word in enumerate(landmark_words):
        ax.annotate(word, (landmark_2d[frame_idx, j, 0], landmark_2d[frame_idx, j, 1]),
                    fontsize=7, color='#cccccc', ha='left', va='bottom',
                    xytext=(3, 3), textcoords='offset points')

    ax.set_title(title, color='white', fontsize=14, fontweight='bold')
    ax.tick_params(colors='#666666')
    for spine in ax.spines.values():
        spine.set_color('#333333')
    ax.legend(loc='upper right', fontsize=8, facecolor='#1a1a1a', edgecolor='#333333',
              labelcolor='white')

plt.tight_layout()
plt.savefig(WAKE2VEC_ROOT / "wake_embedding_preview.png", dpi=150, facecolor='#0a0a0a')
plt.show()
print("Saved preview to wake_embedding_preview.png")

# MP4 Animation (matplotlib)
print("\n── Building MP4 animation ──")
print(f"Frames: {n_steps}, Trail length: {TRAIL_LEN}")

matplotlib.use('Agg')  # non-interactive backend for animation

fig, ax = plt.subplots(figsize=(10, 8))
fig.patch.set_facecolor('#0a0a0a')
ax.set_facecolor('#0a0a0a')

# compute global axis limits from all frames (with padding)
all_x = np.concatenate([wake_2d[:, :, 0].flatten(), landmark_2d[:, :, 0].flatten()])
all_y = np.concatenate([wake_2d[:, :, 1].flatten(), landmark_2d[:, :, 1].flatten()])
pad_x = (all_x.max() - all_x.min()) * 0.08
pad_y = (all_y.max() - all_y.min()) * 0.08
xlim = (all_x.min() - pad_x, all_x.max() + pad_x)
ylim = (all_y.min() - pad_y, all_y.max() + pad_y)

# static elements
ax.set_xlim(xlim)
ax.set_ylim(ylim)
ax.tick_params(colors='#444444', labelsize=7)
for spine in ax.spines.values():
    spine.set_color('#333333')

# title
title_text = ax.set_title("", color='white', fontsize=14, fontweight='bold', pad=10)

# landmark scatter (static-ish, they barely move)
land_scatter = ax.scatter([], [], c='#888888', s=40, alpha=0.9, zorder=4, marker='s')
land_labels = []
for j, word in enumerate(landmark_words):
    txt = ax.annotate(word, (0, 0), fontsize=6, color='#999999', ha='left', va='bottom',
                      xytext=(3, 3), textcoords='offset points', zorder=5)
    land_labels.append(txt)

# wake scatter
wake_scatter = ax.scatter([], [], c='#00ffcc', s=10, alpha=0.7, zorder=3)

# trails: one line per token, faint
trail_lines = []
for _ in range(N_SAMPLE):
    line, = ax.plot([], [], color='#00ffcc', alpha=0.15, linewidth=0.5, zorder=2)
    trail_lines.append(line)

# step counter in corner
step_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, color='#666666',
                    fontsize=10, verticalalignment='top', fontfamily='monospace')

# subtitle
ax.text(0.5, -0.02, 'wake2vec P1 — TinyLlama 1.1B — PCA projection',
        transform=ax.transAxes, color='#555555', fontsize=8,
        ha='center', va='top', fontfamily='monospace')

def init():
    wake_scatter.set_offsets(np.empty((0, 2)))
    land_scatter.set_offsets(np.empty((0, 2)))
    for line in trail_lines:
        line.set_data([], [])
    return [wake_scatter, land_scatter, step_text, title_text] + trail_lines + land_labels

def update(frame):
    # wake tokens
    wake_scatter.set_offsets(wake_2d[frame])

    # landmarks
    land_scatter.set_offsets(landmark_2d[frame])
    for j, txt in enumerate(land_labels):
        txt.set_position((landmark_2d[frame, j, 0], landmark_2d[frame, j, 1]))

    # trails: show last TRAIL_LEN positions
    trail_start = max(0, frame - TRAIL_LEN)
    for tok_idx in range(N_SAMPLE):
        if frame > 0:
            xs = wake_2d[trail_start:frame+1, tok_idx, 0]
            ys = wake_2d[trail_start:frame+1, tok_idx, 1]
            trail_lines[tok_idx].set_data(xs, ys)
        else:
            trail_lines[tok_idx].set_data([], [])

    # title & step counter
    step = step_numbers[frame]
    title_text.set_text(f"Wake Tokens in Embedding Space — Step {step}")
    step_text.set_text(f"step {step:>4d} / {step_numbers[-1]}")

    return [wake_scatter, land_scatter, step_text, title_text] + trail_lines + land_labels

anim = FuncAnimation(fig, update, init_func=init, frames=n_steps,
                     interval=120, blit=True)

# try ffmpeg first, fall back to pillow for gif
try:
    from matplotlib.animation import FFMpegWriter
    writer = FFMpegWriter(fps=12, metadata={'title': 'wake2vec embedding animation'})
    anim.save(str(OUTPUT_MP4), writer=writer, dpi=150)
    print(f"Saved MP4: {OUTPUT_MP4}")
except Exception as e:
    print(f"FFmpeg not available ({e}), falling back to GIF via pillow...")
    try:
        anim.save(str(OUTPUT_GIF), writer='pillow', fps=10, dpi=120)
        print(f"Saved GIF: {OUTPUT_GIF}")
    except Exception as e2:
        print(f"Pillow also failed ({e2}). Try: !apt-get install -y ffmpeg")

plt.close(fig)
print("Animation complete!")

# Interactive HTML (plotly)

print("\n── Building interactive HTML ──")

try:
    import plotly.graph_objects as go

    # build frames for plotly animation
    frames = []
    for i in range(n_steps):
        step = step_numbers[i]
        frame_data = [
            # wake tokens
            go.Scatter(
                x=wake_2d[i, :, 0].tolist(),
                y=wake_2d[i, :, 1].tolist(),
                mode='markers',
                marker=dict(size=4, color='#00ffcc', opacity=0.7),
                name='Wake tokens',
                text=sampled_wake_words,
                hovertemplate='%{text}<br>x: %{x:.3f}<br>y: %{y:.3f}<extra></extra>',
                showlegend=(i == 0),
            ),
            # landmarks
            go.Scatter(
                x=landmark_2d[i, :, 0].tolist(),
                y=landmark_2d[i, :, 1].tolist(),
                mode='markers+text',
                marker=dict(size=8, color='#888888', symbol='square', opacity=0.9),
                text=landmark_words,
                textposition='top right',
                textfont=dict(size=8, color='#999999'),
                name='Ogden\'s landmarks',
                hovertemplate='%{text}<br>x: %{x:.3f}<br>y: %{y:.3f}<extra></extra>',
                showlegend=(i == 0),
            ),
        ]
        frames.append(go.Frame(data=frame_data, name=str(step)))

    # initial figure first frame
    fig_plotly = go.Figure(
        data=frames[0].data,
        frames=frames,
        layout=go.Layout(
            title=dict(
                text='wake2vec P1 — Wake Tokens Settling Into Embedding Space',
                font=dict(color='white', size=16),
            ),
            paper_bgcolor='#0a0a0a',
            plot_bgcolor='#0a0a0a',
            xaxis=dict(
                range=[xlim[0], xlim[1]],
                gridcolor='#1a1a1a',
                zerolinecolor='#333333',
                tickfont=dict(color='#555555'),
            ),
            yaxis=dict(
                range=[ylim[0], ylim[1]],
                gridcolor='#1a1a1a',
                zerolinecolor='#333333',
                tickfont=dict(color='#555555'),
            ),
            legend=dict(
                font=dict(color='white'),
                bgcolor='#1a1a1a',
                bordercolor='#333333',
            ),
            updatemenus=[
                dict(
                    type='buttons',
                    showactive=False,
                    y=0,
                    x=0.5,
                    xanchor='center',
                    buttons=[
                        dict(label='▶ Play',
                             method='animate',
                             args=[None, dict(frame=dict(duration=120, redraw=True),
                                              fromcurrent=True,
                                              transition=dict(duration=0))]),
                        dict(label='⏸ Pause',
                             method='animate',
                             args=[[None], dict(frame=dict(duration=0, redraw=False),
                                                mode='immediate',
                                                transition=dict(duration=0))]),
                    ],
                    font=dict(color='white'),
                    bgcolor='#222222',
                    bordercolor='#444444',
                ),
            ],
            sliders=[
                dict(
                    active=0,
                    steps=[
                        dict(args=[[str(step_numbers[i])],
                                   dict(frame=dict(duration=0, redraw=True),
                                        mode='immediate',
                                        transition=dict(duration=0))],
                             label=str(step_numbers[i]),
                             method='animate')
                        for i in range(n_steps)
                    ],
                    x=0.05,
                    len=0.9,
                    xanchor='left',
                    y=-0.05,
                    currentvalue=dict(
                        prefix='Step: ',
                        visible=True,
                        xanchor='center',
                        font=dict(color='white'),
                    ),
                    font=dict(color='#888888'),
                    bgcolor='#222222',
                    activebgcolor='#00ffcc',
                    bordercolor='#444444',
                    tickcolor='#444444',
                ),
            ],
            width=1000,
            height=700,
        )
    )

    fig_plotly.write_html(str(OUTPUT_HTML))
    print(f"Saved interactive HTML: {OUTPUT_HTML}")
    # also display inline if in notebook
    try:
        fig_plotly.show()
    except:
        pass

except ImportError:
    print("plotly not installed. Run: pip install plotly")
except Exception as e:
    print(f"plotly animation failed: {e}")

# Summary Stats
print("\n── Summary Statistics ──")

# total displacement per wake token (L2 norm of final - initial)
displacement = np.linalg.norm(wake_2d[-1] - wake_2d[0], axis=1)

print(f"\nDisplacement (2D PCA space):")
print(f"  Mean: {displacement.mean():.4f}")
print(f"  Std:  {displacement.std():.4f}")
print(f"  Max:  {displacement.max():.4f}")
print(f"  Min:  {displacement.min():.4f}")

# top 10 most mobile tokens
top_idx = np.argsort(displacement)[::-1][:10]
print(f"\nTop 10 most mobile Wake tokens:")
for rank, idx in enumerate(top_idx):
    print(f"  {rank+1}. '{sampled_wake_words[idx]}' — displacement: {displacement[idx]:.4f}")

# top 10 most stationary
bot_idx = np.argsort(displacement)[:10]
print(f"\nTop 10 most stationary Wake tokens:")
for rank, idx in enumerate(bot_idx):
    print(f"  {rank+1}. '{sampled_wake_words[idx]}' — displacement: {displacement[idx]:.4f}")

# mean displacement over time (convergence curve)
mean_disp_over_time = []
for i in range(n_steps):
    d = np.linalg.norm(wake_2d[i] - wake_2d[0], axis=1).mean()
    mean_disp_over_time.append(d)

fig2, ax2 = plt.subplots(figsize=(10, 4))
fig2.patch.set_facecolor('#0a0a0a')
ax2.set_facecolor('#0a0a0a')
ax2.plot(step_numbers, mean_disp_over_time, color='#00ffcc', linewidth=2)
ax2.set_xlabel('Training Step', color='#aaaaaa')
ax2.set_ylabel('Mean Displacement from Init', color='#aaaaaa')
ax2.set_title('Wake Token Migration Over Training', color='white', fontweight='bold')
ax2.tick_params(colors='#666666')
for spine in ax2.spines.values():
    spine.set_color('#333333')
ax2.grid(True, alpha=0.15, color='#444444')
plt.tight_layout()
plt.savefig(WAKE2VEC_ROOT / "wake_displacement_curve.png", dpi=150, facecolor='#0a0a0a')
plt.show()
print("Saved displacement curve to wake_displacement_curve.png")

# displacement in FULL embedding space (not just PCA 2D)
displacement_full = np.linalg.norm(wake_positions[-1] - wake_positions[0], axis=1)
print(f"\nDisplacement (full {wake_positions.shape[2]}D space):")
print(f"  Mean: {displacement_full.mean():.4f}")
print(f"  Std:  {displacement_full.std():.4f}")
print(f"  Max:  {displacement_full.max():.4f}")
print(f"  Min:  {displacement_full.min():.4f}")

# save summary JSON
summary = {
    "n_snapshots": n_steps,
    "steps": step_numbers,
    "n_wake_sampled": len(sampled_wake_ids),
    "n_landmarks": len(landmark_ids),
    "pca_variance_explained": pca.explained_variance_ratio_.tolist(),
    "displacement_2d": {
        "mean": float(displacement.mean()),
        "std": float(displacement.std()),
        "max": float(displacement.max()),
        "min": float(displacement.min()),
    },
    "displacement_full": {
        "mean": float(displacement_full.mean()),
        "std": float(displacement_full.std()),
        "max": float(displacement_full.max()),
        "min": float(displacement_full.min()),
    },
    "top10_mobile": [(sampled_wake_words[i], float(displacement[i])) for i in top_idx],
    "top10_stationary": [(sampled_wake_words[i], float(displacement[i])) for i in bot_idx],
    "landmark_drift_mean": float(landmark_drift.mean()),
}
with open(WAKE2VEC_ROOT / "embedding_animation_summary.json", "w") as f:
    json.dump(summary, f, indent=2)
print("\nSaved summary to embedding_animation_summary.json")

print("\n" + "="*60)
print("done! outputs:")
print(f"  Preview:   wake_embedding_preview.png")
print(f"  Animation: {OUTPUT_MP4.name} (or .gif)")
print(f"  HTML:      {OUTPUT_HTML.name}")
print(f"  Curve:     wake_displacement_curve.png")
print(f"  Summary:   embedding_animation_summary.json")
print("="*60)