# wake2vec devlog 2026-02-23

## Qwen 2.5-14B P1 IT'S TRAINING

the *qween* is training on a free T4. 14 billion parameters. the VRAM nightmares are over and the wake won.

Catnapp [Thunder](https://soundcloud.com/catnapp/thunder?si=4e6618f328534986a9681765864bf0c2&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)

### What it took

See DEVLOG_0220_QWEN for the full saga, but the final recipe:

1. **Triton shim** a fake `triton.ops` in `sys.modules` so bnb 0.45.0 imports on triton 3.x
2. **`mean_resizing=False`** prevents transformers from computing a 152K x 5120 covariance matrix
3. **Padding range fix** because Qwen has 399 padding slots (151,665 real tokens, 152,064 config vocab). Use `TOTAL_VOCAB - BASE_VOCAB` not `num_added`
4. **fp16 embeddings** this was halved from 3.8 GB to 2.0 GB
5. **WakeOverlay** a separate `nn.Embedding(43824, 5120)` for Wake tokens only. Gradient 0.86 GB instead of 1.87 GB. This was the fix
6. **Accelerate monkey-patch** to bypass the "can't train 4-bit with CPU offload" check
7. **Optimizer fix** using `wte.wake_embed.weight` not `wte.weight` in Adafactor param group

### Loss table

| Step | Train | Val | Notes |
|------|-------|-----|-------|
| | | | |

### VRAM during training

Waiting for first step timer output to confirm speed and memory usage.

---

## Status across all runs

| Model | Phase | Status | Notes |
|-------|-------|--------|-------|
| TinyLlama 1.1B | P1 | Complete | Loss 8.46 â†’ 0.079 (1,300 steps) |
| TinyLlama 1.1B | P2 | Complete | Best ckpt step 1400 (val 0.6393, gap 0.001) |
| TinyLlama 1.1B | P3 | Script ready | `wake2vec_phase_3_morpheme.py`, from step 1400 |
| Llama 3.2-1B | P1 | Paused | Step 1800, val ~5.4 plateau |
| Qwen 2.5-14B | P1 | TRAINING | long live the Qween |
