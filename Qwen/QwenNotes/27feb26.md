# wake2vec devlog 2026-02-27

## Qwen 2.5-14B P1 (relaunch with lightweight callbacks)

Runtime previously disconnected at ~step 20 because the callback suite was hanging Colab: `model.save_pretrained()` writing ~8GB to Drive every 50 steps, `shutil.copytree()` doing the same, and `os.sync()` forcing the flush. 16GB of Drive writes per save event on a 14B model = death.

### Callback overhaul

Rewrote the paranoia suite for Qween's size:

| Callback | Before | After |
|----------|--------|-------|
| `FullCheckpoint` | `model.save_pretrained()` ~8GB to Drive + `os.sync()` | **removed entirely** |
| `SentryMirror` | `shutil.copytree()` ~8GB to Drive + `os.sync()` | → `DriveSentry`: saves only new-token embeddings as fp16 .pt (~215MB), no flush |
| `EmbeddingSnapshot` | Full embedding table to Drive + `os.sync()` | New-token rows only to LOCAL_RUN, no flush |

Net effect: ~16GB Drive writes + 2 forced flushes per save vs ~215MB Drive write, no flush. Base model is from HuggingFace anyway so only need the learned Wake embeddings for recovery.

### Recovery after disconnect

If Colab kills the runtime, the sentry `.pt` files on Drive have everything needed:

```python
sentries = sorted(SENTRY.glob("sentry_step_*.pt"))
if sentries:
    ckpt = torch.load(sentries[-1], map_location="cpu")
    with torch.no_grad():
        wte.wake_embed.weight.data[old_vocab:] = ckpt['embeddings'].float()
```

Adafactor is nearly stateless (no momentum), so the only cost of a restart is the cosine LR schedule resetting but negligible on a 3000-step run.

### Config still same 

| Parameter | Value |
|-----------|-------|
| Model | Qwen/Qwen2.5-14B (4-bit NF4, bfloat16 compute) |
| New Wake tokens | 43,824 via WakeOverlay |
| Trainable | Wake embeddings only (fp32 overlay, base frozen) |
| Optimizer | Adafactor (no momentum) |
| LR | 5e-4, cosine, 5% warmup |
| Effective batch | 16 (batch 1, grad accum 16) |
| Seq len | 128 |
| Save steps | 20 |
| Eval steps | 50 |
| Max steps | 3,000 |

### Loss table

| Step | Train | Val | Notes |
|------|-------|-----|-------|
| | | | relaunch with lightweight callbacks |

---

## The Colab Rug Pull of Feb 27

Got 3 steps into Qween's relaunch before Colab disconnected which wasn't a VRAM issue or a callback hang. Google pushed a runtime update mid-session:

```
torch        2.9.0  → 2.10.0
transformers 4.57.6 → 5.0.0   ← MAJOR VERSION BUMP MID-TRAINING
huggingface-hub 0.36.0 → 1.4.1  ← ANOTHER MAJOR VERSION BUMP
```

`transformers 5.0.0`. Five point zero, pushed to production Colab runtimes on a Thursday evening, while shhhhhhhh the models are learning. 

Love the irony here because I spent days debugging callback I/O to keep Qween alive on this platform, and now the platform itself is the threat. 

**Fix:** Colab lets you select runtime version at connect time, so just pinning to **2026.01** (not 02) keeps `torch 2.9.0`, `transformers 4.57.6`, and all existing dependency pins intact. 

**Future approach for un-started Llama scripts (3B, 8B):** update version pins to work with the 2026.02 runtime (torch 2.10, transformers 5.0.0) so it's not permanently stuck on the January image. 

---

Some Nessa Barrett for you: [Aftercare](https://soundcloud.com/nessabarrett/aftercare?si=44225007680c4976ba55d0d513ee4769&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)
