# wake2vec devlog for 2026-02-20 on Qwen 2.5-14B P1

the absolute unit.

---

## First run: Qwen 2.5-14B embedding-only

### Tokenizer results

```
Base vocab: 151,665
Wake tokens in lexicon: 44,990
Already in Qwen vocab: 767
New tokens added: 44,223
Final vocab size: 195,888
Vocab expansion: 29.1%
```

Qwen already knows 767 Wake tokens which are accented forms, some multilingual words, probably some of the less stylistically complex compounds. Compare to TinyLlama which knew zero. The 29% expansion (vs TinyLlama's 140%) means far less disruption to existing attention patterns.

Embedding matrix size: 195,888 x 5,120 x fp32 = **~3.8 GB**. That's a lot of embedding on a 15GB card.

### Config

| Parameter | Value |
|-----------|-------|
| Model | Qwen/Qwen2.5-14B (base, not instruct) |
| Quantisation | 4-bit NF4, double quant, bfloat16 compute |
| Base vocab | 151,665 (152K — Qwen came prepared) |
| New Wake tokens | 44,223 (but see padding range note below) |
| Actual new rows | 43,824 (399 tokens landed in Qwen's padding range) |
| Final vocab | 195,888 |
| Embedding dim | 5,120 |
| Trainable | Wake embeddings only via WakeOverlay (see VRAM wars below) |
| Embedding dtype | fp16 (was fp32 but halved to survive) |
| Init | Spherical (random direction, radius = 1.5 x base std x sqrt(dim)) |
| Optimizer | Adafactor |
| LR | 5e-4, cosine schedule, 5% warmup |
| Batch | 1 (effective 16 with grad accum) |
| Seq len | 128 (was 256 but reduced during OOM debugging) |
| Gradient checkpointing | On (essential for 14B) |
| Max steps | 3,000 |
| Save every | 50 steps |
| Eval every | 200 steps |
| Embedding snapshots | Every 50 steps |
| Model load | `device_map="auto"`, `max_memory={0: "11GB", "cpu": "30GB"}` |
| Accelerate | Monkey-patched `prepare_model` to allow 4-bit + CPU offload training |

### VRAM budget (final, after all fixes)

```
4-bit model body (GPU):        ~7.7 GB  (rest on CPU)
fp16 full embeddings (frozen): ~2.0 GB
fp32 Wake overlay (43,824):    ~0.86 GB
Wake overlay gradient:         ~0.86 GB  (was 1.87 GB for full matrix!)
Adafactor states:              ~0 GB (bless)
activations (seq_len 128):     ~1-2 GB
─────────────────────────────────────────
total:                         ~12-14 GB
T4 has:                         15 GB
margin:                         slim but real
```

### Loss table

| Step | Train | Val | Notes |
|------|-------|-----|-------|
| | | | |

---

## The VRAM wars (a debugging saga)

Getting 14B params to train on a 15 GB card was... an experience.

### Round 1: bitsandbytes won't import

`ModuleNotFoundError: No module named 'triton.ops'`

Colab upgraded to torch 2.9.0 which ships triton 3.x, which removed `triton.ops`. bitsandbytes (even 0.45.0) still imports from `triton.ops.matmul_perf_model`. Tried:

- Pinning bnb 0.43.3 → depends on triton.ops even harder
- Pinning bnb 0.45.0 → still has the codepath
- Unpinned → torch got upgraded to 2.10, broke everything else

**Fix:** Two-part approach:
1. Separate pip cell pinning exact versions: `torch==2.9.0+cu126`, `bitsandbytes==0.45.0`, `peft==0.18.1`, `accelerate==1.12.0`
2. Triton shim in envi cell with a fake `triton.ops` module registered in `sys.modules`:

```python
import sys, types
fake_perf = types.ModuleType('triton.ops.matmul_perf_model')
fake_perf.early_config_prune = lambda *a, **k: []
fake_perf.estimate_matmul_time = lambda *a, **k: 0
sys.modules['triton.ops'] = types.ModuleType('triton.ops')
sys.modules['triton.ops.matmul_perf_model'] = fake_perf
```

Those triton matmul perf functions are only used for int8 kernel autotuning but this is on NF4 so they never get called.

### Round 2: OOM on resize_token_embeddings

New transformers computes a full covariance matrix during `mean_resizing` which OOMed on a 152K x 5120 matrix.

**Fix:** `model.resize_token_embeddings(TOTAL_VOCAB, mean_resizing=False)`

### Round 3: Qwen's padding range

Qwen has 151,665 real tokens but `vocab_size = 152,064` so 399 padding slots. Some Wake tokens landed in this range, so `num_added` from the tokenizer (44,223) didn't match actual new embedding rows (43,824). The old code used `num_added` as the count and `old_vocab` as the start, both wrong.

**Fix:** Use `actual_wake_count = TOTAL_VOCAB - BASE_VOCAB` (195,888 - 152,064 = 43,824) as the true count and `BASE_VOCAB` (152,064) as the start.

### Round 4: OOM on backward pass (the big one)

First attempt at SEQ_LEN=256, fp32 embeddings:
```
OutOfMemoryError: Tried to allocate 1.87 GiB
VRAM before training: 9.72 GB, free: 5.9 GB
```

The 1.87 GB is the **full embedding gradient** is 195,888 × 5,120 × fp32 = 3.8 GB for the matrix, and the gradient is the same size. The gradient mask zeros out base rows, but PyTorch allocates the full tensor first.

Tried:
1. **SEQ_LEN 256→128** but got same error, same 1.87 GB. Proved it wasn't activation memory
2. **fp16 embeddings** which halved embedding from 3.8 to 2.0 GB. Still OOMs: gradient still 1.87 GB (fp16 gradient = 195K × 5120 × 2 = 2 GB, close enough)
3. **`max_memory={0: "7GB"}`** got accelerate refuses: `ValueError: You can't train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload`
4. **`device_map={"": 0}`** (all on GPU) the model itself OOMs during loading at shard 6/8

### Round 5: WakeOverlay (the solution)

The gradient was 1.87 GB because PyTorch computed it for ALL 195,888 rows. But we only train 43,824 Wake rows. The gradient mask was zeroing 152K rows after the full tensor was already allocated.

**Fix:** Don't train the full embedding matrix at all. Instead:
1. Freeze the main embedding layer entirely (`requires_grad=False`)
2. Create a small separate `nn.Embedding(43824, 5120)` in fp32 for just the Wake tokens
3. Override forward pass to look up from the right table based on token ID

The Wake-only gradient: 43,824 × 5,120 × fp32 = **0.86 GB** instead of 1.87 GB. That's the 370 MB we needed and then more.

But then accelerate still blocked 4-bit + CPU offload training. **Fix:** monkey-patch `accelerate.Accelerator.prepare_model` to skip the device-map validation:

```python
import accelerate.accelerator as _acc
_orig_prepare = _acc.Accelerator.prepare_model
def _hacked_prepare(self, model, device_placement=None, evaluation_mode=False):
    self._models.append(model)
    return model
_acc.Accelerator.prepare_model = _hacked_prepare
```
---
smoked a ciggy instead of crying. 

[La Yugular](https://soundcloud.com/rosaliaofficial/la-yugular?in=rosaliaofficial/sets/lux-226901126&si=98d05545ab934aff847fa9073736090a&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)
