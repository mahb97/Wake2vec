{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1YxzvmFZN2PQfgwhi2bHbtrfisacZGGgb",
      "authorship_tag": "ABX9TyP8NS2YC4YEFoq+Hcp3IqnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_morpheme_expansion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec Morpheme Expansion Pipeline\n",
        "\n",
        "This notebook documents a controlled procedure for integrating Joyce-style neologisms into a compact GPT-type language model through morphology-aware token expansion. I curate a small lexicon of prefixes and suffixes and generate synthetic candidates, then extend the tokenizer to admit previously split neologisms as single tokens. New embeddings are initialised by morphemic composition, using the rule \\(E(\\text{word}) = \\alpha\\,E(\\text{prefix}) + (1 - 2\\alpha)\\,E(\\text{root}) + \\alpha\\,E(\\text{suffix}) + \\varepsilon\\), where \\(\\alpha\\) is a fixed weight and \\(\\varepsilon\\) is small Gaussian noise that prevents identical vectors. Training proceeds in two stages: an embedding-only warm-up on a mixture of synthetic lines and Finnegans *Wake* text, followed by a short full-model fine-tune under conservative schedules suitable for a T4 environment.\n",
        "\n",
        " I report top-five neighbor overlap for the newly introduced tokens before and after training, track shifts in embedding norms, provide a t-SNE projection of the new tokens against pre-training neighbor centroids, and save JSON snapshots of neighborhoods at each stage. These diagnostics are intended to show coherent integration of the new forms into the embedding space rather than collapse or runaway drift, and to make the procedure straightforward to reproduce on modest hardware.\n",
        "\n",
        "**Config**\n",
        "\n",
        "Base model: `TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T`. Composition weight \\(\\alpha = 0.25\\). Maximum sequence length set to 1024 to respect T4 memory limits. Batching uses `per_device_train_batch_size = 1` with `gradient_accumulation_steps = 8`, attention implementation set to `eager`, and `use_cache = False`. Phase 1 trains input embeddings and the tied output head only; Phase 2 unfreezes all parameters with a warm-up ratio of 0.10 and light weight decay. All runs write plots and machine-readable artifacts to `runs/<RUN_ID>/` and generate a brief HTML report.\n",
        "\n",
        "---\n",
        "\n",
        "## Run controls\n",
        "- **BASE_MODEL:** `TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T`\n",
        "- **α (composition weight):** `0.25` (can tune)\n",
        "- **Max seq length:** `1024` (T4-safe; raise only if VRAM allows)\n",
        "- **Batching:** `per_device_train_batch_size=1`, `gradient_accumulation_steps=8`\n",
        "- **Attn impl:** `eager` (avoid SDPA spikes on T4)\n",
        "- **Two phases:**\n",
        "  - **Phase 1:** embeddings + lm_head only, Adafactor/8-bit Adam, 1 epoch\n",
        "  - **Phase 2:** full model, short run, warmup 0.10\n",
        "\n",
        "## Inputs\n",
        "- `data/FW_TEXT.txt` — Finnegans Wake plain text (slice for demo)\n",
        "- `data/morpheme_data.json` or `data/morphemes.csv`  \n",
        "  Structure maps:\n",
        "  - `prefixes`: `{ prefix → [example words…] }`\n",
        "  - `suffixes`: `{ suffix → [example words…] }`\n",
        "\n",
        "## Outputs (per run)\n",
        "- `runs/<RUN_ID>/metrics/`\n",
        "  - `pre_morpheme_snapshot.json`\n",
        "  - `morpheme_comparison_p1.json` *(midpoint, after Phase 1)*\n",
        "  - `morpheme_comparison.json` *(final, after Phase 2)*\n",
        "  - `summary_stats_p1.json`, `summary_stats.json`\n",
        "- `runs/<RUN_ID>/plots/`\n",
        "  - `hist_overlap_top5(_p1).png`, `hist_norm_change(_p1).png`\n",
        "  - `scatter_norm_vs_overlap.png`, `tsne_newtokens_vs_precentroids.png`\n",
        "- `reports/Wake2Vec_Report.html`\n",
        "\n",
        "## Quickstart\n",
        "1. **Reset & install** deps (Colab-friendly).  \n",
        "2. **Load data** (prefers JSON).  \n",
        "3. **Generate** synthetic forms (prefix + root + suffix).  \n",
        "4. **Expand tokenizer** (add new tokens); compose embeddings with α-rule; tie head.  \n",
        "5. **Phase 1**: train embeddings only. Saves midpoint snapshot.\n",
        "6. **Phase 2**: unfreeze and short fine-tune.  \n",
        "7. **Diagnostics**: compute overlap@5, norm deltas, t-SNE; write HTML report.  \n",
        "\n",
        "\n",
        "## Diagnostics (what “good” looks like)\n",
        "- **Top-5 neighbor overlap (pre→post):** ~3–4/5 indicates coherent integration (not collapse).\n",
        "- **Norm shift (Δ‖E‖):** small positive mean (slight energy increase from training).\n",
        "- **Qualitative neighbors:** morpheme-aligned (e.g., `presounder` ≈ `resound`, `ensounder`, …).\n",
        "- **Tokenization:** most synthetic forms now **single IDs**.\n",
        "\n",
        "## Repro & env\n",
        "- `RUN_ID = \"t4_<unix>\"` auto-stamped; seeds fixed at 42.\n",
        "- Tested on Colab T4 with: `transformers 4.57.1`, `datasets 2.21.0`, `pyarrow 22.0.0`.\n",
        "- T4 guardrails: `MAX_LEN=1024`, `gradient_checkpointing=True`, attention=`eager`, batch=1 + accum=8.\n",
        "\n",
        "## Troubleshooting (T4)\n",
        "- **CUDA OOM** → lower `MAX_LEN` to 768/512; keep batch=1; accum=8–16; ensure `use_cache=False`; `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`.\n",
        "- **Version noise** → uninstall RAPIDS/TF; pin `transformers 4.57.1`, `datasets 2.21.0`, `pyarrow 22.0.0`.\n",
        "\n",
        "---\n",
        "\n",
        " *Wake2Vec tests morphology-aware token expansion to integrate Joyce-style neologisms into a small language model without destabilising the embedding space. We curate a prefix/suffix lexicon, generate synthetic forms, initialise new vectors by morpheme composition, and train in two phases. Evaluation reports neighbor-overlap@5, embedding-norm shifts, and qualitative neighborhoods, with JSON snapshots for reproducibility.*\n"
      ],
      "metadata": {
        "id": "Ovb6LuW8cVUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y cudf-cu12 pylibcudf-cu12 cuml-cu12 dask-cudf-cu12 cupy-cuda12x tensorflow opencv-python-headless opencv-contrib-python opencv-python >/dev/null\n",
        "\n",
        "!pip -q install --no-cache-dir --upgrade-strategy eager \\\n",
        "  \"transformers==4.57.1\" \"accelerate>=0.33\" \"tokenizers>=0.15\" \"safetensors\" \\\n",
        "  \"datasets==2.21.0\" \"evaluate>=0.4.0\" \"pyarrow==22.0.0\" \\\n",
        "  \"huggingface-hub>=0.34,<1.0\" \"bitsandbytes>=0.43\" \\\n",
        "  \"umap-learn\" \"faiss-cpu\" \"wordfreq\" \"Unidecode\" \"matplotlib\" \"scikit-learn\"\n",
        "\n",
        "import transformers, datasets, pyarrow\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"pyarrow:\", pyarrow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shcc5QFPT7iJ",
        "outputId": "230a11a3-dfbd-472d-d5ca-045f4f7d82c3",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pylibcudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cuml-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dask-cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cupy-cuda12x as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mTransformers: 4.57.1\n",
            "datasets: 2.21.0\n",
            "pyarrow: 22.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports, seeds, run IDs, paths"
      ],
      "metadata": {
        "id": "LJd_oDr2evlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, time, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "RUN_ID = f\"t4_{int(time.time())}\"\n",
        "ROOT = Path(\"/content\")\n",
        "RUN_DIR = ROOT / \"runs\" / RUN_ID\n",
        "PLOTS_DIR = RUN_DIR / \"plots\"\n",
        "METRICS_DIR = RUN_DIR / \"metrics\"\n",
        "REPORTS_DIR = ROOT / \"reports\"\n",
        "for p in (PLOTS_DIR, METRICS_DIR, REPORTS_DIR): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "META = {\n",
        "    \"run_id\": RUN_ID, \"seed\": SEED, \"alpha\": 0.25,\n",
        "    \"phase1\": {\"lr\": 5e-4, \"epochs\": 1, \"ptd_bs\": 8, \"grad_accum\": 2},\n",
        "    \"phase2\": {\"lr\": 2e-5, \"epochs\": 2, \"warmup_ratio\": 0.10, \"ptd_bs\": 8, \"grad_accum\": 2, \"weight_decay\": 0.01}\n",
        "}\n",
        "(METRICS_DIR/\"meta.json\").write_text(json.dumps(META, indent=2))\n",
        "print(\"RUN_ID:\", RUN_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Vjjx2oVFmX",
        "outputId": "7ebd0c33-480f-4350-a8fb-0cb2294cd7fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_ID: t4_1761966609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "txo87v9YgG-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, csv\n",
        "\n",
        "DATA_DIR = ROOT/\"data\"; DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FW_PATH   = DATA_DIR/\"/content/FW_TEXT.txt\"\n",
        "JSON_PATH = DATA_DIR/\"/content/morpheme_data.json\"\n",
        "CSV_PATH  = DATA_DIR/\"/content/morphemes.csv\"\n",
        "\n",
        "def load_morpheme_csv(path):\n",
        "    d = {\"prefixes\": {}, \"suffixes\": {}}\n",
        "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        rdr = csv.reader(f); header = next(rdr, None)\n",
        "        for row in rdr:\n",
        "            if not row: continue\n",
        "            typ, morpheme, *examples = [x.strip() for x in row]\n",
        "            if typ not in (\"prefix\",\"suffix\"): continue\n",
        "            key = \"prefixes\" if typ==\"prefix\" else \"suffixes\"\n",
        "            ex = [w for w in dict.fromkeys(examples) if w]\n",
        "            if ex: d[key][morpheme] = ex\n",
        "    return d\n",
        "\n",
        "if JSON_PATH.exists():\n",
        "    MORPHEME_DATA = json.load(open(JSON_PATH, \"r\", encoding=\"utf-8\"))\n",
        "elif CSV_PATH.exists():\n",
        "    MORPHEME_DATA = load_morpheme_csv(CSV_PATH)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Put morpheme_data.json or morphemes.csv in /content/data\")\n",
        "\n",
        "prefixes = MORPHEME_DATA.get(\"prefixes\", {})\n",
        "suffixes = MORPHEME_DATA.get(\"suffixes\", {})\n",
        "\n",
        "if not FW_PATH.exists():\n",
        "    FW_PATH.write_text(\"Placeholder FW text.\\n\"*5000, encoding=\"utf-8\")\n",
        "FW_TEXT = FW_PATH.read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Prefixes: {len(prefixes)} | Suffixes: {len(suffixes)} | FW chars: {len(FW_TEXT):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTFDpC9dVLzU",
        "outputId": "23b80304-e8a9-4d26-b93e-6b67b7c42f3e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prefixes: 15 | Suffixes: 15 | FW chars: 1,364,712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic generator"
      ],
      "metadata": {
        "id": "pzykhrzCKmy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def synthetic_words(n=1200, roots=(\"river thunder word sound dance queen storm tree night sun rain book\".split())):\n",
        "    out=set()\n",
        "    pfx_pool=[p for p,ex in prefixes.items() for _ in range(max(1,len(ex)//2+1))]\n",
        "    sfx_pool=[s for s,ex in suffixes.items() for _ in range(max(1,len(ex)//2+1))]\n",
        "    for _ in range(max(2*n, 2000)):\n",
        "        if not pfx_pool or not sfx_pool: break\n",
        "        p=random.choice(pfx_pool); s=random.choice(sfx_pool); r=random.choice(roots)\n",
        "        if len(p)+len(r)+len(s)>3: out.add(f\"{p}{r}{s}\")\n",
        "        if len(out)>=n: break\n",
        "    return sorted(out)\n",
        "\n",
        "SYN_WORDS = synthetic_words()\n",
        "SYN_LINES = [f\"The {w} rolled down the river at night.\" for w in random.sample(SYN_WORDS, min(400,len(SYN_WORDS)))]\n",
        "print(\"Synthetic words:\", len(SYN_WORDS), \"| synthetic lines:\", len(SYN_LINES))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4I0lXj5Vd2F",
        "outputId": "4f7729ea-50ac-4215-fe2d-51ac3648e83b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic words: 1200 | synthetic lines: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "base model, expand tokenizer, compose embeddings, tie head\n"
      ],
      "metadata": {
        "id": "Qq5P2pjnKv2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, dtype=\"float32\", device_map=\"auto\")\n",
        "\n",
        "pre_token_splits = {w: tok.encode(w, add_special_tokens=False) for w in SYN_WORDS}\n",
        "new_tokens = [w for w, ids in pre_token_splits.items() if len(ids) > 1]\n",
        "added = tok.add_tokens(new_tokens, special_tokens=False)\n",
        "model.resize_token_embeddings(len(tok), mean_resizing=False)\n",
        "print(f\"Added tokens: {added} | Vocab size: {len(tok)}\")\n",
        "\n",
        "import torch\n",
        "def avg_vec(terms, emb, tok):\n",
        "    vecs=[]\n",
        "    for t in terms:\n",
        "        ids = tok.encode(t, add_special_tokens=False)\n",
        "        if len(ids)==1: vecs.append(emb.weight.data[ids[0]])\n",
        "    return torch.stack(vecs,0).mean(0) if vecs else None\n",
        "\n",
        "with torch.no_grad():\n",
        "    emb = model.get_input_embeddings()\n",
        "    alpha = META[\"alpha\"]; std = emb.weight.data.std().item()\n",
        "    for w in new_tokens:\n",
        "        p = next((p for p in prefixes if w.startswith(p)), None)\n",
        "        s = next((s for s in suffixes if w.endswith(s)), None)\n",
        "        root = w[len(p):len(w)-len(s)] if (p and s and len(w)>len(p)+len(s)) else w\n",
        "        vp = avg_vec(prefixes.get(p, []), emb, tok)\n",
        "        vs = avg_vec(suffixes.get(s, []), emb, tok)\n",
        "        vr_ids = tok.encode(root, add_special_tokens=False)\n",
        "        vr = emb.weight.data[vr_ids[0]] if len(vr_ids)==1 else torch.randn(emb.embedding_dim, device=emb.weight.device)*(std*0.5)\n",
        "        comp = alpha*(vp if vp is not None else vr) + (1-2*alpha)*vr + alpha*(vs if vs is not None else vr)\n",
        "        comp = comp + torch.randn_like(comp)*(std*0.01)\n",
        "        emb.weight.data[tok.convert_tokens_to_ids(w)] = comp\n",
        "    model.lm_head.weight = emb.weight\n",
        "\n",
        "print(\"Composed embeddings + tied head.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5_uilzdVkRl",
        "outputId": "69e34de1-a21e-42f1-f6a6-1e4632508f8d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added tokens: 1200 | Vocab size: 33200\n",
            "Composed embeddings + tied head.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "model.config.pad_token_id = tok.pad_token_id"
      ],
      "metadata": {
        "id": "Wun4Cf35Wj34"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "blocks + PRE snapshot"
      ],
      "metadata": {
        "id": "0yvf0QyoLGdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "\n",
        "MAX_LEN=1024; STRIDE=512\n",
        "def make_blocks(text, max_len=MAX_LEN, stride=STRIDE):\n",
        "    ids = tok.encode(text, add_special_tokens=False)\n",
        "    return [{\"input_ids\": ids[i:i+max_len]} for i in range(0, max(0,len(ids)-max_len), stride) if len(ids[i:i+max_len])==max_len]\n",
        "\n",
        "train_text = \"\\n\".join(SYN_LINES) + \"\\n\" + FW_TEXT[:600_000]\n",
        "valid_text = FW_TEXT[600_000:630_000]\n",
        "train_ds = Dataset.from_list(make_blocks(train_text))\n",
        "valid_ds = Dataset.from_list(make_blocks(valid_text))\n",
        "dc = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "print(\"Train blocks:\", len(train_ds), \"| Valid blocks:\", len(valid_ds))\n",
        "\n",
        "with torch.no_grad():\n",
        "    W_pre = model.get_input_embeddings().weight.detach().clone().to(\"cpu\").numpy()\n",
        "    new_ids = [tok.convert_tokens_to_ids(t) for t in new_tokens]\n",
        "    sim_pre = cosine_similarity(W_pre[new_ids], W_pre)\n",
        "    top5_pre = np.argsort(-sim_pre, axis=1)[:,1:6]\n",
        "\n",
        "json.dump({\"new_tokens\": new_tokens, \"top5_pre\": top5_pre[:50].tolist()}, open(METRICS_DIR/\"pre_morpheme_snapshot.json\",\"w\"), indent=2)\n",
        "print(\"Saved pre snapshot.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXeYQJtIVsob",
        "outputId": "71952791-3209-4335-e1a0-d56ea2093543"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train blocks: 372 | Valid blocks: 16\n",
            "Saved pre snapshot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P1 — embeddings-only warm-up"
      ],
      "metadata": {
        "id": "irYAXRooLSNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, gc\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kntqzcokXxhU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# P1 Embedding-only warm-up\n",
        "import gc, torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Freeze everything except embeddings and lm_head\n",
        "def freeze_all_but_embeddings(m):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in m.get_input_embeddings().parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in m.lm_head.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "freeze_all_but_embeddings(model)\n",
        "\n",
        "# Trainer args — tiny batch, big accum, checkpointing\n",
        "model.config.use_cache = False\n",
        "args1 = TrainingArguments(\n",
        "    output_dir=str(RUN_DIR/\"phase1\"),\n",
        "    per_device_train_batch_size=1,     # tiny batch\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=META[\"phase1\"][\"lr\"],\n",
        "    num_train_epochs=META[\"phase1\"][\"epochs\"],\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    eval_steps=400,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,                        # fp16 fragile on T4\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_bnb_8bit\",            # use 8-bit Adam if bitsandbytes is present\n",
        ")\n",
        "\n",
        "trainer1 = Trainer(\n",
        "    model=model,\n",
        "    args=args1,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        ")\n",
        "\n",
        "trainer1.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "-hpbA0aRZvRg",
        "outputId": "209ec861-3dec-4a10-c81d-3a5412e04241"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='47' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [47/47 17:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=47, training_loss=6.399282252534907, metrics={'train_runtime': 1083.9698, 'train_samples_per_second': 0.343, 'train_steps_per_second': 0.043, 'total_flos': 2214661416026112.0, 'train_loss': 6.399282252534907, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MID SNAPSHOT\n",
        "import json, torch, numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MID_DIR = METRICS_DIR  # reuse same folder\n",
        "\n",
        "with torch.no_grad():\n",
        "    W_mid = model.get_input_embeddings().weight.detach().clone().to(\"cpu\").numpy()\n",
        "    sim_mid = cosine_similarity(W_mid[new_ids], W_mid)\n",
        "    top5_mid = np.argsort(-sim_mid, axis=1)[:,1:6]\n",
        "\n",
        "def overlap_at5(a,b): return len(set(a.tolist()) & set(b.tolist()))\n",
        "\n",
        "overlaps_p1 = np.array([overlap_at5(top5_pre[i], top5_mid[i]) for i in range(len(new_ids))])\n",
        "norms_pre   = np.linalg.norm(W_pre[new_ids], axis=1)\n",
        "norms_mid   = np.linalg.norm(W_mid[new_ids], axis=1)\n",
        "norm_deltas_p1 = norms_mid - norms_pre\n",
        "\n",
        "summary_p1 = {\n",
        "    \"phase\": \"phase1\",\n",
        "    \"compared_tokens\": int(len(new_ids)),\n",
        "    \"mean_top5_overlap\": float(np.mean(overlaps_p1)) if len(overlaps_p1) else None,\n",
        "    \"mean_norm_delta\": float(np.mean(norm_deltas_p1)) if len(norm_deltas_p1) else None,\n",
        "}\n",
        "\n",
        "# save JSONs\n",
        "(Path(MID_DIR)/\"morpheme_comparison_p1.json\").write_text(\n",
        "    json.dumps({\n",
        "        \"top5_pre\": top5_pre.tolist(),\n",
        "        \"top5_mid\": top5_mid.tolist(),\n",
        "        \"overlap@5\": overlaps_p1.tolist(),\n",
        "        \"norm_deltas\": norm_deltas_p1.tolist(),\n",
        "    }, indent=2)\n",
        ")\n",
        "(Path(MID_DIR)/\"summary_stats_p1.json\").write_text(json.dumps(summary_p1, indent=2))\n",
        "print(\"Phase-1 summary:\", summary_p1)\n",
        "\n",
        "# quick plots\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "plt.figure(); plt.hist(overlaps_p1, bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5])\n",
        "plt.title(\"Top-5 overlap (PRE → MID)\"); plt.xlabel(\"Overlap\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_overlap_top5_p1.png\", dpi=180); plt.close()\n",
        "\n",
        "plt.figure(); plt.hist(norm_deltas_p1, bins=30)\n",
        "plt.title(\"Embedding norm change (MID − PRE)\"); plt.xlabel(\"Δ norm\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_norm_change_p1.png\", dpi=180); plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XOKrmuXbAtC",
        "outputId": "b7159370-248c-4f04-9bc3-ce2997eaa056"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase-1 summary: {'phase': 'phase1', 'compared_tokens': 1200, 'mean_top5_overlap': 3.1283333333333334, 'mean_norm_delta': 0.07451333105564117}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2 full-model fine-tune"
      ],
      "metadata": {
        "id": "goWc5R-gbHtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# capture init for new ids\n",
        "with torch.no_grad():\n",
        "    E_init = model.get_input_embeddings().weight.data.clone()\n",
        "\n",
        "# custom loss wrapper\n",
        "LAMBDA = 1e-4\n",
        "def add_anchor_loss(outputs, inputs):\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    emb = model.get_input_embeddings().weight\n",
        "    ids = torch.unique(input_ids)\n",
        "    ids = ids[ids >= 0]\n",
        "    return LAMBDA * (emb[ids] - E_init[ids]).pow(2).mean()"
      ],
      "metadata": {
        "id": "xyW3DEk_fU6a"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 384\n",
        "STRIDE  = 192\n",
        "\n",
        "def make_blocks(text, max_len=MAX_LEN, stride=STRIDE):\n",
        "    ids = tok.encode(text, add_special_tokens=False)\n",
        "    return [{\"input_ids\": ids[i:i+max_len]}\n",
        "            for i in range(0, max(0, len(ids)-max_len), stride)\n",
        "            if len(ids[i:i+max_len]) == max_len]\n",
        "\n",
        "train_text = \"\\n\".join(SYN_LINES) + \"\\n\" + FW_TEXT[:600_000]\n",
        "valid_text = FW_TEXT[600_000:630_000]\n",
        "\n",
        "from datasets import Dataset\n",
        "train_ds = Dataset.from_list(make_blocks(train_text))\n",
        "valid_ds = Dataset.from_list(make_blocks(valid_text))\n",
        "\n",
        "# ensure pad token + fresh collator\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "model.config.pad_token_id = tok.pad_token_id\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "dc = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "print(\"Phase-2 dataset:\",\n",
        "      \"Train blocks =\", len(train_ds),\n",
        "      \"| Valid blocks =\", len(valid_ds),\n",
        "      \"| MAX_LEN =\", MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhc9DJDAiqOw",
        "outputId": "216e021a-4e0d-41e0-ed35-c40f9a4c5f1e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase-2 dataset: Train blocks = 995 | Valid blocks = 46 | MAX_LEN = 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === PHASE 2: LoRA adapters (tiny VRAM), no eval ===\n",
        "import os, gc, torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "!pip -q install peft>=0.11\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# hygiene + allocator defrag\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Keep the base model as-is; add small trainable adapters\n",
        "model.config.use_cache = False\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "# Typical LoRA targets for LLaMA-family blocks\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()  # sanity: should be a tiny % of total\n",
        "\n",
        "# Trainer args — tiny batch + accum; AdamW on small adapter params is fine\n",
        "args2 = TrainingArguments(\n",
        "    output_dir=str(RUN_DIR/\"phase2_lora\"),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,     # effective batch while keeping peak minimal\n",
        "    learning_rate=1.5e-4,               # higher LR for adapters\n",
        "    num_train_epochs=2,\n",
        "    warmup_ratio=0.10,\n",
        "    weight_decay=0.0,                   # usually 0 for LoRA\n",
        "    eval_strategy=\"no\",                 # skip eval pass to save VRAM\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=2000,\n",
        "    logging_strategy=\"steps\", logging_steps=100,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch\",                # adapters are small; AdamW is fine\n",
        ")\n",
        "\n",
        "trainer2 = Trainer(\n",
        "    model=model,\n",
        "    args=args2,\n",
        "    train_dataset=train_ds,\n",
        "    data_collator=dc,\n",
        ")\n",
        "\n",
        "print(f\"Starting Phase-2 LoRA (MAX_LEN={MAX_LEN}) …\")\n",
        "trainer2.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "W5R3LbI0i0I-",
        "outputId": "7f20c40a-a38b-4496-ef62-22da93940b47"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6,307,840 || all params: 1,043,277,824 || trainable%: 0.6046\n",
            "Starting Phase-2 LoRA (MAX_LEN=384) …\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 30:03, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.861400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=126, training_loss=4.824113331143818, metrics={'train_runtime': 1817.0464, 'train_samples_per_second': 1.095, 'train_steps_per_second': 0.069, 'total_flos': 4471639155671040.0, 'train_loss': 4.824113331143818, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL SNAPSHOT\n",
        "import json, torch, numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "with torch.no_grad():\n",
        "    W_post = model.get_input_embeddings().weight.detach().clone().to(\"cpu\").numpy()\n",
        "    sim_post = cosine_similarity(W_post[new_ids], W_post)\n",
        "    top5_post = np.argsort(-sim_post, axis=1)[:,1:6]\n",
        "\n",
        "def overlap_at5(a,b): return len(set(a.tolist()) & set(b.tolist()))\n",
        "overlaps = np.array([overlap_at5(top5_pre[i], top5_post[i]) for i in range(len(new_ids))])\n",
        "norms_pre = np.linalg.norm(W_pre[new_ids], axis=1)\n",
        "norms_post = np.linalg.norm(W_post[new_ids], axis=1)\n",
        "norm_deltas = norms_post - norms_pre\n",
        "\n",
        "summary = {\n",
        "    \"phase\": \"phase2_final\",\n",
        "    \"compared_tokens\": int(len(new_ids)),\n",
        "    \"mean_top5_overlap\": float(np.mean(overlaps)),\n",
        "    \"mean_norm_delta\": float(np.mean(norm_deltas)),\n",
        "}\n",
        "(METRICS_DIR/\"morpheme_comparison.json\").write_text(json.dumps({\n",
        "    \"top5_pre\": top5_pre.tolist(),\n",
        "    \"top5_post\": top5_post.tolist(),\n",
        "    \"overlap@5\": overlaps.tolist(),\n",
        "    \"norm_deltas\": norm_deltas.tolist()\n",
        "}, indent=2))\n",
        "(METRICS_DIR/\"summary_stats.json\").write_text(json.dumps(summary, indent=2))\n",
        "print(\"FINAL:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbQ3oGJ8rN4r",
        "outputId": "a9a85a37-3d3f-4ee7-ad22-f0141f239749"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL: {'phase': 'phase2_final', 'compared_tokens': 1200, 'mean_top5_overlap': 3.1283333333333334, 'mean_norm_delta': 0.07451333105564117}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plt.figure(); plt.hist(overlaps, bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5])\n",
        "plt.title(\"Top-5 neighbor overlap (pre → post)\"); plt.xlabel(\"Overlap\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_overlap_top5.png\", dpi=180); plt.close()\n",
        "\n",
        "plt.figure(); plt.hist(norm_deltas, bins=30)\n",
        "plt.title(\"Embedding norm change (post − pre)\"); plt.xlabel(\"Δ norm\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_norm_change.png\", dpi=180); plt.close()\n",
        "\n",
        "plt.figure(); plt.scatter(norm_deltas, overlaps, alpha=0.6)\n",
        "plt.title(\"Norm change vs Overlap@5\"); plt.xlabel(\"Δ norm\"); plt.ylabel(\"Overlap@5\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"scatter_norm_vs_overlap.png\", dpi=180); plt.close()\n",
        "\n",
        "print(\"Saved plots to:\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "k9CyaCBQrUlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "plt.figure(); plt.hist(overlaps, bins=[-0.5,0.5,1.5,2.5,3.5,4.5,5.5])\n",
        "plt.title(\"Top-5 neighbor overlap (pre → post)\"); plt.xlabel(\"Overlap\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_overlap_top5.png\", dpi=180); plt.close()\n",
        "\n",
        "plt.figure(); plt.hist(norm_deltas, bins=30)\n",
        "plt.title(\"Embedding norm change (post − pre)\"); plt.xlabel(\"Δ norm\"); plt.ylabel(\"Freq\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"hist_norm_change.png\", dpi=180); plt.close()\n",
        "\n",
        "plt.figure(); plt.scatter(norm_deltas, overlaps, alpha=0.6)\n",
        "plt.title(\"Norm change vs Overlap@5\"); plt.xlabel(\"Δ norm\"); plt.ylabel(\"Overlap@5\")\n",
        "plt.tight_layout(); plt.savefig(PLOTS_DIR/\"scatter_norm_vs_overlap.png\", dpi=180); plt.close()\n",
        "\n",
        "print(\"Saved plots to:\", PLOTS_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aVesGP5rewS",
        "outputId": "e729b680-c952-4831-f4e0-188dce1a7bec"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved plots to: /content/runs/t4_1761966609/plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "s_mid = json.loads((METRICS_DIR/\"summary_stats_p1.json\").read_text()) if (METRICS_DIR/\"summary_stats_p1.json\").exists() else {}\n",
        "s_fin = json.loads((METRICS_DIR/\"summary_stats.json\").read_text())\n",
        "html = f\"\"\"<!DOCTYPE html><html><head><meta charset=\"utf-8\"><title>Wake2Vec — Report {RUN_ID}</title>\n",
        "<style>body{{font-family:\"Times New Roman\",serif;line-height:1.35}}.c{{max-width:900px;margin:2rem auto;padding:0 1rem 3rem}}\n",
        "h1{{font-size:1.9rem;border-bottom:2px solid #000;padding-bottom:.4rem}}</style></head><body>\n",
        "<div class=\"c\"><h1>Wake2Vec — Interim Report</h1>\n",
        "<p><b>Run:</b> {RUN_ID}</p>\n",
        "<ul>\n",
        "<li><b>Phase 1</b> (PRE→MID): compared={s_mid.get('compared_tokens','—')}, overlap@5={s_mid.get('mean_top5_overlap','—')}, Δ‖E‖={s_mid.get('mean_norm_delta','—')}</li>\n",
        "<li><b>Phase 2</b> (PRE→POST): compared={s_fin['compared_tokens']}, overlap@5={s_fin['mean_top5_overlap']:.3f}, Δ‖E‖={s_fin['mean_norm_delta']:.5f}</li>\n",
        "</ul>\n",
        "<img src=\"../runs/{RUN_ID}/plots/hist_overlap_top5.png\" style=\"width:100%\"><br>\n",
        "<img src=\"../runs/{RUN_ID}/plots/hist_norm_change.png\" style=\"width:100%\"><br>\n",
        "<img src=\"../runs/{RUN_ID}/plots/scatter_norm_vs_overlap.png\" style=\"width:100%\">\n",
        "<ul><li>Metrics: runs/{RUN_ID}/metrics/*.json</li><li>Plots: runs/{RUN_ID}/plots/*.png</li></ul>\n",
        "</div></body></html>\"\"\"\n",
        "(REPORTS_DIR/\"Wake2Vec_Report.html\").write_text(html, encoding=\"utf-8\")\n",
        "print(\"Report:\", REPORTS_DIR/\"Wake2Vec_Report.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81bxlppFrm9t",
        "outputId": "9bc6d9e2-e199-4ab8-a636-88fbce4b458c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report: /content/reports/Wake2Vec_Report.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = RUN_DIR/\"phase2_lora\"/\"final_adapters\"\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(str(SAVE_DIR), safe_serialization=True)\n",
        "tok.save_pretrained(str(SAVE_DIR))\n",
        "print(\"Saved adapters+tokenizer to:\", SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmyO4dulsQzB",
        "outputId": "6e8d0d1a-3b28-4655-b05d-0578f98c8c55"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved adapters+tokenizer to: /content/runs/t4_1761966609/phase2_lora/final_adapters\n"
          ]
        }
      ]
    }
  ]
}