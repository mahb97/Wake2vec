{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/heartbeat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a2ct7YLfVT5",
        "outputId": "faf6a7b1-8f2c-49b4-d772-34bf3e602514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t4_1762376560 step 550 loss 3.2604\n"
          ]
        }
      ],
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "log = json.loads((run/\"metrics\"/\"phase1_loss_log.json\").read_text())\n",
        "print(run.name, \"step\", log[-1][\"step\"], \"loss\", round(float(log[-1][\"loss\"]),4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "ck = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)[0]\n",
        "state = json.loads((ck/\"trainer_state.json\").read_text())\n",
        "ev = [d for d in state.get(\"log_history\", []) if \"eval_loss\" in d]\n",
        "print(ev[-1] if ev else \"no eval yet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWBAGHBE05lD",
        "outputId": "c5db6825-ef90-4537-e8b4-5e2d799b4fac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 10.527472527472527, 'eval_loss': 7.096441268920898, 'eval_runtime': 13.6439, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'step': 600}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "snapshot"
      ],
      "metadata": {
        "id": "llry5ube8_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual snapshot\n",
        "import pathlib, shutil, time\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "src = latest_ckpt_with_weights(RUN)\n",
        "assert src is not None, \"No valid checkpoint with weights found yet.\"\n",
        "SNAPS = RUN/\"snapshots\"; SNAPS.mkdir(exist_ok=True, parents=True)\n",
        "dst = SNAPS/f\"snap_{int(time.time())}_{src.name}\"\n",
        "if not dst.exists():\n",
        "    shutil.copytree(src, dst)\n",
        "print(\"[SNAP] Saved\", dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqEsKdkn8_R8",
        "outputId": "6653c150-b5f6-4d7d-c969-76b50ed2daa5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] Saved /content/drive/MyDrive/wake2vec/runs/t4_1762376560/snapshots/snap_1762565503_checkpoint-300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentry mirror"
      ],
      "metadata": {
        "id": "61EtZyYu9GZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newest full checkpoint + metrics\n",
        "import pathlib, shutil\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name; SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "ck = latest_ckpt_with_weights(RUN)\n",
        "if ck is None:\n",
        "    print(\"[SENTRY] No full checkpoint yet.\")\n",
        "else:\n",
        "    dst = SENTRY/ck.name\n",
        "    if not dst.exists():\n",
        "        shutil.copytree(ck, dst)\n",
        "        print(f\"[SENTRY] Mirrored {ck.name} → {dst}\")\n",
        "    else:\n",
        "        print(\"[SENTRY] Already mirrored:\", dst)\n",
        "\n",
        "# mirror metrics JSONs\n",
        "mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "for f in (RUN/\"metrics\").glob(\"*.json\"):\n",
        "    shutil.copy2(f, mdst/f.name)\n",
        "print(\"[SENTRY] Metrics mirrored →\", mdst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikqpbn0P9IZI",
        "outputId": "134a49e7-7430-468b-82b6-248f58c83e7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirrored checkpoint-300 → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss and checkpoint"
      ],
      "metadata": {
        "id": "CzGNXVVp9jbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heartbeat\n",
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "# loss\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    print(f\"[LOSS] step={logs[-1]['step']}  loss={float(logs[-1]['loss']):.4f}\")\n",
        "else:\n",
        "    print(\"[LOSS] no metrics yet\")\n",
        "\n",
        "# checkpoints\n",
        "def scan(base):\n",
        "    rows=[]\n",
        "    for ck in sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        s=int(ck.name.split(\"-\")[-1])\n",
        "        rows.append((\n",
        "            s,\n",
        "            (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(),\n",
        "            (ck/\"trainer_state.json\").exists(),\n",
        "            (ck/\"optimizer.pt\").exists(),\n",
        "        ))\n",
        "    return rows\n",
        "\n",
        "print(\"\\n[RUNS]\")\n",
        "for s,w,t,o in scan(RUN):\n",
        "    print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "if SENTRY.exists():\n",
        "    print(\"\\n[SENTRY]\")\n",
        "    for s,w,t,o in scan(SENTRY):\n",
        "        print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "else:\n",
        "    print(\"\\n[SENTRY] none\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjpiKxqN9mFs",
        "outputId": "5c78aab1-bdc2-43c9-a0e1-ba56cadd382a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS] step=550  loss=3.2604\n",
            "\n",
            "[RUNS]\n",
            "   100  weights=True   state=True   opt=True \n",
            "   200  weights=True   state=True   opt=True \n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=True   opt=True \n",
            "   700  weights=False  state=True   opt=True \n",
            "\n",
            "[SENTRY]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean drive"
      ],
      "metadata": {
        "id": "CqniUpma9tAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Light flush to help sync small files\n",
        "import os, time, pathlib\n",
        "RUN = max((pathlib.Path(\"/content/drive/MyDrive/wake2vec\")/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "(open(RUN/\"_touch.sync\",\"w\")).write(str(time.time()))\n",
        "os.sync()\n",
        "print(\"[SYNC] touched + sync hinted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CW2IWyO9uUH",
        "outputId": "eab2fc1d-3869-4f53-d45f-912aae37715b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] touched + sync hinted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save if trainer is in scope"
      ],
      "metadata": {
        "id": "f-RvQNxm92Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trainer.save_model()\n",
        "    if hasattr(trainer, \"_save_checkpoint\"):\n",
        "        trainer._save_checkpoint(model=trainer.model, trial=None)\n",
        "    print(\"[TRAINER] save requested\")\n",
        "except NameError:\n",
        "    print(\"[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwlnYfrg-MEZ",
        "outputId": "b060273e-f977-4641-ebaf-2750fe52d87e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RUN = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "\n",
        "# Loss heartbeat\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    print(f\"[LOSS] last step={logs[-1]['step']}  loss={float(logs[-1]['loss']):.4f}\")\n",
        "else:\n",
        "    print(\"[LOSS] no metrics yet\")\n",
        "\n",
        "# Checkpoint table\n",
        "def scan(base):\n",
        "    rows=[]\n",
        "    for ck in sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        s=int(ck.name.split(\"-\")[-1])\n",
        "        rows.append((s,\n",
        "                     (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(),\n",
        "                     (ck/\"trainer_state.json\").exists(),\n",
        "                     (ck/\"optimizer.pt\").exists()))\n",
        "    return rows\n",
        "\n",
        "print(\"\\n[RUNS]\")\n",
        "for s,w,t,o in scan(RUN):\n",
        "    print(f\"  {s:>4}  weights={w!s:5}  state={t!s:5}  opt={o!s:5}\")\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "if SENTRY.exists():\n",
        "    print(\"\\n[SENTRY]\")\n",
        "    for s,w,t,o in scan(SENTRY):\n",
        "        print(f\"  {s:>4}  weights={w!s:5}  state={t!s:5}  opt={o!s:5}\")\n",
        "else:\n",
        "    print(\"\\n[SENTRY] none\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hEMASYx_ZIr",
        "outputId": "e620cf55-77bd-4710-dce9-6d1a1021f072"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS] last step=550  loss=3.2604\n",
            "\n",
            "[RUNS]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n",
            "\n",
            "[SENTRY]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted.\")"
      ],
      "metadata": {
        "id": "VuVTCDxICx2A",
        "outputId": "43839b01-93c6-42fa-d07b-44d68a1bb5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERBOSE mirror of latest full checkpoint (+ metrics) to sentry_backups\n",
        "import pathlib, shutil, time, os\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "CANDIDATES = [pathlib.Path(\"/content/runs\"), DRIVE/\"runs\"]\n",
        "\n",
        "def latest_run():\n",
        "    runs = []\n",
        "    for root in CANDIDATES:\n",
        "        if root.exists():\n",
        "            for p in root.glob(\"t4_*\"):\n",
        "                try:\n",
        "                    runs.append((p.stat().st_mtime, p))\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "    if not runs: return None\n",
        "    runs.sort(reverse=True)\n",
        "    return runs[0][1]\n",
        "\n",
        "def latest_ckpt_with_weights(run):\n",
        "    if not run: return None\n",
        "    cks = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cks:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "RUN = latest_run()\n",
        "print(\"[INFO] Active run:\", RUN if RUN else \"none\")\n",
        "CK  = latest_ckpt_with_weights(RUN)\n",
        "print(\"[INFO] Latest full ckpt:\", CK if CK else \"none\")\n",
        "\n",
        "if CK is None:\n",
        "    print(\"[SENTRY] No checkpoint with weights yet — will retry after next save.\")\n",
        "else:\n",
        "    SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "    SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "    DST = SENTRY/CK.name\n",
        "\n",
        "    src_mtime = time.ctime(CK.stat().st_mtime)\n",
        "    print(f\"[SENTRY] Source: {CK} (mtime {src_mtime})\")\n",
        "    if DST.exists():\n",
        "        # check if dest is older/stale by file count or mtime\n",
        "        src_files = sum(1 for _ in CK.rglob(\"*\"))\n",
        "        dst_files = sum(1 for _ in DST.rglob(\"*\"))\n",
        "        print(f\"[SENTRY] Already exists: {DST} (files src={src_files} dst={dst_files})\")\n",
        "        if dst_files < src_files:\n",
        "            print(\"[SENTRY] Detected partial mirror; refreshing…\")\n",
        "            shutil.rmtree(DST)\n",
        "            shutil.copytree(CK, DST)\n",
        "            print(\"[SENTRY] Re-mirrored:\", DST)\n",
        "        else:\n",
        "            print(\"[SENTRY] Mirror up-to-date.\")\n",
        "    else:\n",
        "        shutil.copytree(CK, DST)\n",
        "        print(\"[SENTRY] Mirrored:\", DST)\n",
        "\n",
        "    # Mirror metrics verbosely\n",
        "    msrc = RUN/\"metrics\"\n",
        "    mdst = SENTRY/\"metrics\"\n",
        "    mdst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    if msrc.exists():\n",
        "        for f in msrc.glob(\"*.json\"):\n",
        "            shutil.copy2(f, mdst/f.name)\n",
        "            copied += 1\n",
        "    print(f\"[SENTRY] Metrics mirrored → {mdst} ({copied} files)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJdM-MCcJoVP",
        "outputId": "8122a071-095e-4bb2-96ed-4f51e0f3c907"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Active run: /content/drive/MyDrive/wake2vec/runs/t4_1762376560\n",
            "[INFO] Latest full ckpt: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Source: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 (mtime Wed Nov  5 22:45:46 2025)\n",
            "[SENTRY] Already exists: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300 (files src=12 dst=12)\n",
            "[SENTRY] Mirror up-to-date.\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "BASE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN = BASE/\"runs\"/\"t4_1762376560\"  # adjust if different\n",
        "SENTRY = BASE/\"sentry_backups\"/\"t4_1762376560\"\n",
        "\n",
        "def audit(root):\n",
        "    print(f\"\\n[{root}]\")\n",
        "    for ck in sorted(root.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        step = int(ck.name.split(\"-\")[-1])\n",
        "        w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "        t = (ck/\"trainer_state.json\").exists()\n",
        "        o = (ck/\"optimizer.pt\").exists()\n",
        "        print(f\"{step:>5}  weights={w:<5}  state={t:<5}  opt={o:<5}  → {ck.name}\")\n",
        "\n",
        "if RUN.exists():   audit(RUN)\n",
        "if SENTRY.exists(): audit(SENTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLwaBc-GKRK8",
        "outputId": "9c33c77a-c5bf-49ba-f91c-130eb5454270"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[/content/drive/MyDrive/wake2vec/runs/t4_1762376560]\n",
            "  100  weights=1      state=1      opt=1      → checkpoint-100\n",
            "  200  weights=1      state=1      opt=1      → checkpoint-200\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=1      opt=1      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n",
            "\n",
            "[/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560]\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=0      opt=0      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P1 resume\n",
        "import pathlib, json, shutil, torch, time, os\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback)\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime).name\n",
        "LOCAL_RUN = pathlib.Path(\"/content/runs\")/RUN_ID\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "DRIVE_RUN = DRIVE/\"runs\"/RUN_ID\n",
        "SENTRY    = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "\n",
        "def list_ckpts(base):\n",
        "    if not base.exists(): return []\n",
        "    return sorted([p for p in base.glob(\"checkpoint-*\") if p.is_dir()], key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "\n",
        "def has_weights(ck):\n",
        "    return (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "\n",
        "# newest w/ weights\n",
        "cands = list_ckpts(DRIVE_RUN) + list_ckpts(SENTRY)\n",
        "GOOD = next((p for p in cands if has_weights(p)), None)\n",
        "assert GOOD is not None, \"No valid checkpoint with weights found.\"\n",
        "last_step = int(GOOD.name.split(\"-\")[-1])\n",
        "print(f\"[RESUME] {RUN_ID} from {GOOD.name} (last_step={last_step})\")\n",
        "\n",
        "# LOCAL_RUN with checkpoint\n",
        "if not (LOCAL_RUN/GOOD.name).exists():\n",
        "    shutil.copytree(GOOD, LOCAL_RUN/GOOD.name)\n",
        "    print(\"[LOCAL] seeded\", (LOCAL_RUN/GOOD.name))\n",
        "\n",
        "# tokenizer/model\n",
        "tok = AutoTokenizer.from_pretrained(str(GOOD), use_fast=True)\n",
        "if tok.pad_token_id is None: tok.pad_token = tok.eos_token or \"</s>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(str(GOOD), torch_dtype=torch.float32, device_map=\"auto\")\n",
        "model.config.use_cache = False\n",
        "with torch.no_grad():\n",
        "    # (re)tie head\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "\n",
        "# data\n",
        "train_ds = load_from_disk(str(DRIVE/\"datasets\"/\"train_ds\"))\n",
        "valid_ds = load_from_disk(str(DRIVE/\"datasets\"/\"valid_ds\")).select(range(min(1000, len(load_from_disk(str(DRIVE/'datasets'/'valid_ds'))))))\n",
        "dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "\n",
        "TARGET = 1300\n",
        "\n",
        "# callbacks\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.n == 0:\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            ck = max(LOCAL_RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "            dst = SENTRY/ck.name\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] mirrored {ck.name}\")\n",
        "            # metrics\n",
        "            msrc = LOCAL_RUN/\"metrics\"\n",
        "            mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "            if msrc.exists():\n",
        "                for f in msrc.glob(\"*.json\"): shutil.copy2(f, mdst/f.name)\n",
        "            os.sync()\n",
        "        except Exception as e:\n",
        "            print(\"[SENTRY] mirror failed:\", e)\n",
        "\n",
        "# Embedding-only snapshot\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    def __init__(self, every=50):\n",
        "        self.every = every\n",
        "        (DRIVE/\"emb_snaps\"/RUN_ID).mkdir(parents=True, exist_ok=True)\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.every == 0:\n",
        "            try:\n",
        "                emb = model.get_input_embeddings().weight.detach().cpu()\n",
        "                path = DRIVE/\"emb_snaps\"/RUN_ID/f\"emb_step{int(state.global_step):04d}.pt\"\n",
        "                torch.save(emb, path)\n",
        "                # tiny heartbeat json\n",
        "                hb = {\"step\": int(state.global_step), \"ts\": time.time(), \"rows\": int(emb.size(0)), \"dim\": int(emb.size(1))}\n",
        "                (DRIVE/\"emb_snaps\"/RUN_ID/\"heartbeat.json\").write_text(json.dumps(hb, indent=2))\n",
        "                print(f\"[SNAP] embeddings → {path.name}\")\n",
        "            except Exception as e:\n",
        "                print(\"[SNAP] failed:\", e)\n",
        "\n",
        "# dynamic save_steps\n",
        "save_steps = 50 if last_step < 700 else 100\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=15,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False, bf16=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[EvalEveryNSteps(200), SentryMirror(), EmbeddingSnap(50)],\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=str(LOCAL_RUN/GOOD.name))\n",
        "# final save\n",
        "trainer.save_model(str(LOCAL_RUN/\"checkpoint-final\")); tok.save_pretrained(str(LOCAL_RUN/\"checkpoint-final\"))\n",
        "# mirror the final\n",
        "try:\n",
        "    dst = SENTRY/\"checkpoint-final\"\n",
        "    if dst.exists(): shutil.rmtree(dst)\n",
        "    shutil.copytree(LOCAL_RUN/\"checkpoint-final\", dst)\n",
        "    print(\"[SENTRY] mirrored checkpoint-final\")\n",
        "except Exception as e:\n",
        "    print(\"[SENTRY] final mirror failed:\", e)\n",
        "\n",
        "print(\"[DONE] Reached\", TARGET)"
      ],
      "metadata": {
        "id": "TXlVItHZLx5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
