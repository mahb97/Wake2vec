# Core - pinned for Nov 2025 Colab compatibility
torch==2.5.1+cu121
transformers==4.45.2
accelerate==0.34.2
datasets
tokenizers>=0.15
safetensors
scikit-learn

# Quantization (optional, for Llama)
bitsandbytes==0.43.3
triton==3.1.0

# PEFT (optional, for P2 LoRA)
peft==0.13.2

# Finetuning / PEFT (optional but used in repo)
peft>=0.11
bitsandbytes>=0.43; platform_system!="Windows"

# Metrics / analysis / viz
scikit-learn
umap-learn
faiss-cpu
wordfreq
Unidecode
matplotlib
numpy
pandas

# Optional quality-of-life
tqdm
evaluate
