{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2vec_heartbeat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec Training Monitor\n",
        "\n",
        "Remote monitoring and checkpoint verification system for long-running embedding training experiments.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides read-only monitoring utilities for tracking the progress of Wake2Vec training runs executing on separate Colab instances. It implements a non-invasive inspection layer that queries checkpoint directories and metrics logs without interfering with active training processes. The monitoring system supports both local ephemeral storage and persistent Google Drive locations.\n",
        "\n",
        "## Functionality\n",
        "\n",
        "The notebook performs the following diagnostic operations:\n",
        "\n",
        "Training progress monitoring via loss history and evaluation metrics\n",
        "\n",
        "Checkpoint inventory and validation across multiple storage locations\n",
        "\n",
        "Automatic mirroring of validated checkpoints to secondary backup directories\n",
        "\n",
        "Temporal analysis of file modifications to detect stale or incomplete saves\n",
        "\n",
        "Embedding snapshot tracking at configurable step intervals\n",
        "\n",
        "Resume point identification for interrupted training sessions\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "Storage hierarchy: The system monitors two primary locations:\n",
        "\n",
        "- Local ephemeral: `/content/runs/t4_*` (active training directory)\n",
        "- Drive persistent: `/content/drive/MyDrive/wake2vec/runs/t4_*` (synchronized copy)\n",
        "- Sentry backup: `/content/drive/MyDrive/wake2vec/sentry_backups/t4_*` (safety mirror)\n",
        "\n",
        "Checkpoint validation: Weights are verified by checking for the presence of `model.safetensors`, `pytorch_model.bin`, or sharded weight files. Checkpoints without valid weights are excluded from resume candidates.\n",
        "\n",
        "Loss monitoring: The system preferentially reads from structured JSON logs at `metrics/phase1_loss_log.json`, falling back to `trainer_state.json` within checkpoint directories when streaming logs are unavailable.\n",
        "\n",
        "Evaluation tracking: Evaluation metrics are extracted from the most recent checkpoint's trainer state, displaying the tail of recorded validation losses with corresponding step numbers and runtime statistics.\n",
        "\n",
        "\n",
        "## Monitoring Schedule\n",
        "\n",
        "The notebook is designed for manual execution at user-defined intervals during training. Typical usage patterns include:\n",
        "\n",
        "- Hourly checks during active training phases\n",
        "- Post-checkpoint verification after save events\n",
        "- Pre-resume validation before launching continuation runs\n",
        "- Post-mortem analysis for failed or interrupted sessions\n",
        "\n"
      ],
      "metadata": {
        "id": "ynOqEGeqU5u5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQKnp813U5EQ",
        "outputId": "41e41d6e-b306-4272-abce-8496576515f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolve active RUN"
      ],
      "metadata": {
        "id": "LuVQgnjyVrU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, time\n",
        "\n",
        "RUN_ID = None\n",
        "\n",
        "LOCAL_ROOT = pathlib.Path(\"/content/runs\")\n",
        "DRIVE_ROOT = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "def latest_run(root):\n",
        "    if not root.exists(): return None\n",
        "    runs = []\n",
        "    for p in root.glob(\"t4_*\"):\n",
        "        try: runs.append((p.stat().st_mtime, p))\n",
        "        except FileNotFoundError: pass\n",
        "    return max(runs, key=lambda x: x[0])[1] if runs else None\n",
        "\n",
        "LOCAL_RUN  = (LOCAL_ROOT/RUN_ID) if RUN_ID else latest_run(LOCAL_ROOT)\n",
        "DRIVE_RUN  = (DRIVE_ROOT/\"runs\"/RUN_ID) if RUN_ID else latest_run(DRIVE_ROOT/\"runs\")\n",
        "RUN = LOCAL_RUN or DRIVE_RUN\n",
        "assert RUN is not None, \"No local or Drive t4_* run found.\"\n",
        "\n",
        "print(\"Watching:\", RUN, \"| mtime:\", time.ctime(RUN.stat().st_mtime))\n",
        "SENTRY = DRIVE_ROOT/\"sentry_backups\"/RUN.name\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "sSwAWS8BVK-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3365c13-945b-4050-af9b-e5b91aad06fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Watching: /content/drive/MyDrive/wake2vec/runs/t4_1762376560 | mtime: Wed Nov  5 21:02:40 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss tail"
      ],
      "metadata": {
        "id": "XYZ2FZoFVdKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    tail = logs[-5:]\n",
        "    print(\"[LOSS stream] last:\", [(d[\"step\"], round(float(d[\"loss\"]),4)) for d in tail])\n",
        "else:\n",
        "    # Fallback\n",
        "    cks = sorted(RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "    if cks and (cks[-1]/\"trainer_state.json\").exists():\n",
        "        state = json.loads((cks[-1]/\"trainer_state.json\").read_text())\n",
        "        tail = [(d[\"step\"], d[\"loss\"]) for d in state.get(\"log_history\", []) if \"loss\" in d][-5:]\n",
        "        print(\"[LOSS state ] last:\", tail if tail else \"—\")\n",
        "    else:\n",
        "        print(\"[LOSS] no logs yet\")"
      ],
      "metadata": {
        "id": "j8ZlK6S3VOYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a1d1e0-c046-49cf-f707-609467716aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS stream] last: [(350, 5.4883), (400, 5.3082), (450, 4.7776), (500, 4.0891), (550, 3.2604)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval tail"
      ],
      "metadata": {
        "id": "MtiVwK5aV4L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "cks = sorted(RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "if cks:\n",
        "    p = cks[-1]/\"trainer_state.json\"\n",
        "    if p.exists():\n",
        "        state = json.loads(p.read_text())\n",
        "        evals = [d for d in state.get(\"log_history\", []) if \"eval_loss\" in d][-3:]\n",
        "        print(\"[EVAL] tail:\", evals if evals else \"—\")\n",
        "    else:\n",
        "        print(\"[EVAL] none yet (next at 400/600/800...)\")\n",
        "else:\n",
        "    print(\"[EVAL] no checkpoints yet\")"
      ],
      "metadata": {
        "id": "nKxTbFa5V8-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e5e157-284a-4a03-99d1-9f8708410723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] tail: [{'epoch': 3.50989010989011, 'eval_loss': 6.237724304199219, 'eval_runtime': 13.5689, 'eval_samples_per_second': 3.538, 'eval_steps_per_second': 0.442, 'step': 200}, {'epoch': 7.0175824175824175, 'eval_loss': 6.416950225830078, 'eval_runtime': 13.5887, 'eval_samples_per_second': 3.532, 'eval_steps_per_second': 0.442, 'step': 400}, {'epoch': 10.527472527472527, 'eval_loss': 7.096441268920898, 'eval_runtime': 13.6439, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'step': 600}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checkpoint audit"
      ],
      "metadata": {
        "id": "FlUMH5iyV-As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, time\n",
        "\n",
        "def latest_full_ckpt(root):\n",
        "    cks = sorted(root.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cks:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "src = latest_full_ckpt(RUN)\n",
        "if src is None:\n",
        "    print(\"[SENTRY] No full checkpoint yet; wait for next save.\")\n",
        "else:\n",
        "    dst = SENTRY/src.name\n",
        "    if not dst.exists():\n",
        "        shutil.copytree(src, dst)\n",
        "        print(f\"[SENTRY] mirrored {src.name} (mtime {time.ctime(src.stat().st_mtime)})\")\n",
        "    else:\n",
        "        print(\"[SENTRY] already has\", src.name)\n",
        "    # Mirror metrics\n",
        "    msrc = RUN/\"metrics\"; mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    if msrc.exists():\n",
        "        for f in msrc.glob(\"*.json\"):\n",
        "            shutil.copy2(f, mdst/f.name); copied += 1\n",
        "    print(f\"[SENTRY] metrics mirrored ({copied} files)\")"
      ],
      "metadata": {
        "id": "68OG0U5TV_-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acd3ee4-0076-42af-8596-ea1eb018295a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] already has checkpoint-300\n",
            "[SENTRY] metrics mirrored (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding snapshot quick view"
      ],
      "metadata": {
        "id": "Eg4OE_TVWJt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SNAPS_DIR = DRIVE_ROOT/\"emb_snaps\"/RUN.name\n",
        "if SNAPS_DIR.exists():\n",
        "    snaps = sorted(SNAPS_DIR.glob(\"emb_step*.pt\"))\n",
        "    print(f\"[SNAPS] count={len(snaps)}  latest=\", snaps[-1].name if snaps else \"—\")\n",
        "    hb = SNAPS_DIR/\"heartbeat.json\"\n",
        "    if hb.exists(): print(\"[SNAPS] heartbeat:\", hb.read_text())\n",
        "else:\n",
        "    print(\"[SNAPS] none yet (first at step 350 if every 50)\")"
      ],
      "metadata": {
        "id": "XEvsO-XAWOm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5acb2f-1057-46b7-f4cd-ef33e2306ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAPS] count=9  latest= emb_step0750.pt\n",
            "[SNAPS] heartbeat: {\n",
            "  \"step\": 750,\n",
            "  \"rows\": 32000,\n",
            "  \"dim\": 2048,\n",
            "  \"ts\": 1762729811.8068688\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0HV11oD6Xuj",
        "outputId": "8cd21d09-9ef6-4064-b4ee-f4a55ade6a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "opt light sync"
      ],
      "metadata": {
        "id": "Mltz83NCWR-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, pathlib\n",
        "touch = RUN/\"_heartbeat.touch\"\n",
        "touch.write_text(str(time.time()))\n",
        "os.sync()\n",
        "print(\"[SYNC] touched + sync hinted →\", touch)"
      ],
      "metadata": {
        "id": "H9oPq1LhWVol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4b7968-0939-4aac-f4f4-f11b0160f565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] touched + sync hinted → /content/drive/MyDrive/wake2vec/runs/t4_1762376560/_heartbeat.touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, pathlib\n",
        "RUN = pathlib.Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560\")\n",
        "SENTRY = pathlib.Path(\"/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\")\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "src = RUN/\"checkpoint-750\"; dst = SENTRY/\"checkpoint-750\"\n",
        "if src.exists() and not dst.exists():\n",
        "    shutil.copytree(src, dst); print(\"[SENTRY] mirrored checkpoint-750\")"
      ],
      "metadata": {
        "id": "5rhzlcL373m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "RUN = pathlib.Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560\")\n",
        "ck = RUN/\"checkpoint-750\"\n",
        "print(\"750 exists:\", ck.exists(),\n",
        "      \"| weights:\", (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoKLm8Op7yez",
        "outputId": "a2be1183-6d34-48c8-ea1c-8c98d30e89a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750 exists: False | weights: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unlink drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.flush_and_unmount()\n",
        "    print(\"[OK] flushed & unmounted\")\n",
        "except Exception as e:\n",
        "    print(\"[INFO] flush/unmount skipped:\", e)\n",
        "\n",
        "import shutil, os\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "    shutil.rmtree(\"/content/drive\", ignore_errors=True)\n",
        "    print(\"[OK] removed /content/drive\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# anity check\n",
        "import pathlib\n",
        "BASE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "print(\"[BASE exists]\", BASE.exists())\n",
        "print(\"[BASE contents]\", [p.name for p in BASE.iterdir()] if BASE.exists() else \"—\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv5y4SBQAFvu",
        "outputId": "0792c47e-58be-4721-e5b4-efe1723c2f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "[OK] flushed & unmounted\n",
            "[OK] removed /content/drive\n",
            "Mounted at /content/drive\n",
            "[BASE exists] True\n",
            "[BASE contents] ['runs', 'adapters', 'reports', 'archives', 'notebooks', 'datasets', 'sentry_backups', 'emb_snaps']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "BASE   = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS   = BASE/\"runs\"\n",
        "SENTRY = BASE/\"sentry_backups\"\n",
        "SNAPS  = BASE/\"emb_snaps\"\n",
        "\n",
        "def audit(run_root):\n",
        "    print(f\"\\n[{run_root}] exists:\", run_root.exists())\n",
        "    if not run_root.exists(): return\n",
        "    for run in sorted(run_root.glob(\"t4_*\")):\n",
        "        print(\" \", run.name)\n",
        "        for ck in sorted(run.glob(\"checkpoint-*\"), key=lambda p:int(p.name.split(\"-\")[-1])):\n",
        "            step = int(ck.name.split(\"-\")[-1]) if ck.name.count(\"-\") else -1\n",
        "            has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "            print(f\"   {ck.name:>20}  weights={int(has_w)}\")\n",
        "\n",
        "audit(RUNS)\n",
        "audit(SENTRY)\n",
        "\n",
        "print(\"\\n[SNAPS] exists:\", SNAPS.exists())\n",
        "if SNAPS.exists():\n",
        "    for r in sorted(SNAPS.glob(\"t4_*\")):\n",
        "        snaps = sorted(r.glob(\"emb_step*.pt\"))\n",
        "        print(\" \", r.name, \"| snaps:\", len(snaps), \"| latest:\", snaps[-1].name if snaps else \"—\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GicjBh8rBFOL",
        "outputId": "d86a924f-611b-4e37-c920-e8f61315f89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[/content/drive/MyDrive/wake2vec/runs] exists: True\n",
            "  t4_1762100254\n",
            "  t4_1762104879\n",
            "  t4_1762105026\n",
            "  t4_1762113417\n",
            "  t4_1762375997\n",
            "  t4_1762376307\n",
            "  t4_1762376560\n",
            "         checkpoint-100  weights=1\n",
            "         checkpoint-200  weights=1\n",
            "         checkpoint-300  weights=1\n",
            "         checkpoint-400  weights=0\n",
            "         checkpoint-500  weights=0\n",
            "         checkpoint-600  weights=0\n",
            "         checkpoint-700  weights=0\n",
            "\n",
            "[/content/drive/MyDrive/wake2vec/sentry_backups] exists: True\n",
            "  t4_1762376560\n",
            "         checkpoint-300  weights=0\n",
            "         checkpoint-400  weights=0\n",
            "         checkpoint-500  weights=0\n",
            "         checkpoint-600  weights=0\n",
            "         checkpoint-700  weights=0\n",
            "\n",
            "[SNAPS] exists: True\n",
            "  t4_1762376560 | snaps: 9 | latest: emb_step0750.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_ID = \"t4_1762376560\"\n",
        "STEP = 700\n",
        "ck = RUNS/RUN_ID/f\"checkpoint-{STEP}\"\n",
        "print(ck)\n",
        "print(\"\\nFiles:\")\n",
        "for p in sorted(ck.glob(\"*\")):\n",
        "    print(\" -\", p.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt_B_VGTChLa",
        "outputId": "967dfdbc-d221-4aad-e62e-fca038bc4418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-700\n",
            "\n",
            "Files:\n",
            " - config.json\n",
            " - generation_config.json\n",
            " - optimizer.pt\n",
            " - rng_state.pth\n",
            " - scheduler.pt\n",
            " - special_tokens_map.json\n",
            " - tokenizer.json\n",
            " - tokenizer.model\n",
            " - tokenizer_config.json\n",
            " - trainer_state.json\n",
            " - training_args.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild a loadable checkpoint-750 from the embedding snapshot\n",
        "import pathlib, torch, shutil, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "BASE   = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS   = BASE/\"runs\"\n",
        "SENTRY = BASE/\"sentry_backups\"\n",
        "SNAPS  = BASE/\"emb_snaps\"\n",
        "\n",
        "# Pick RUN_ID from snapshots\n",
        "RUN_IDS = sorted([p.name for p in SNAPS.glob(\"t4_*\")])\n",
        "assert RUN_IDS, \"No t4_* in emb_snaps — check Drive mount/account.\"\n",
        "RUN_ID = RUN_IDS[-1]\n",
        "print(\"[RUN_ID]\", RUN_ID)\n",
        "\n",
        "# Find base checkpoint ≤ 750\n",
        "def full_ckpts(root):\n",
        "    out = []\n",
        "    d = root/RUN_ID\n",
        "    if not d.exists(): return out\n",
        "    for ck in d.glob(\"checkpoint-*\"):\n",
        "        step = int(ck.name.split(\"-\")[-1])\n",
        "        has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists() \\\n",
        "                or list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\"))\n",
        "        if has_w: out.append((step, ck))\n",
        "    return sorted(out, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "bases = full_ckpts(SENTRY) + full_ckpts(RUNS)\n",
        "assert bases, \"No base checkpoints with weights found in sentry_backups/ or runs/.\"\n",
        "base_step, BASE_CK = next(((s, p) for s, p in bases if s <= 750), bases[-1])\n",
        "print(f\"[BASE] Using {BASE_CK} (step {base_step})\")\n",
        "\n",
        "# Load embedding snapshot @ 750\n",
        "EMB = SNAPS/RUN_ID/\"emb_step0750.pt\"\n",
        "assert EMB.exists(), \"emb_step0750.pt not found.\"\n",
        "emb = torch.load(EMB, map_location=\"cpu\")\n",
        "\n",
        "# Load base and inject embeddings; re-tie head\n",
        "tok = AutoTokenizer.from_pretrained(str(BASE_CK), use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(str(BASE_CK), torch_dtype=torch.float32, device_map=\"cpu\")\n",
        "with torch.no_grad():\n",
        "    model.get_input_embeddings().weight[:emb.size(0), :].copy_(emb)\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "print(\"[REBUILD] Injected emb_step0750 into base\")\n",
        "\n",
        "# Save as checkpoint-750-rebuilt\n",
        "OUT_RUN    = RUNS/RUN_ID/\"checkpoint-750-rebuilt\"\n",
        "OUT_SENTRY = SENTRY/RUN_ID/\"checkpoint-750-rebuilt\"\n",
        "for d in (OUT_RUN, OUT_SENTRY):\n",
        "    if d.exists(): shutil.rmtree(d)\n",
        "model.save_pretrained(str(OUT_RUN), safe_serialization=True)\n",
        "tok.save_pretrained(str(OUT_RUN))\n",
        "shutil.copytree(OUT_RUN, OUT_SENTRY)\n",
        "print(\"[SAVED] →\", OUT_RUN)\n",
        "print(\"[MIRRORED] →\", OUT_SENTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhZdpEosENvp",
        "outputId": "68ca0380-f8fd-4dfe-affb-7e9d0482834b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN_ID] t4_1762376560\n",
            "[BASE] Using /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 (step 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REBUILD] Injected emb_step0750 into base\n",
            "[SAVED] → /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\n",
            "[MIRRORED] → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-750-rebuilt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "ck = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists() \\\n",
        "        or list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\"))\n",
        "print(\"750-rebuilt loadable:\", bool(has_w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkre3dB1Jysb",
        "outputId": "608a822e-8cdf-4544-b160-5779a458a781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750-rebuilt loadable: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HB-1\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_LOCAL = Path(f\"/content/runs/{RUN_ID}\")\n",
        "RUN_DRIVE = DRIVE / \"runs\" / RUN_ID\n",
        "RESUME_750 = RUN_DRIVE / \"checkpoint-750-rebuilt\"\n",
        "\n",
        "def has_weights(ck: Path) -> bool:\n",
        "    if not ck.exists(): return False\n",
        "    if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "        return True\n",
        "    if list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def ckpt_step(p: Path) -> int:\n",
        "    \"\"\"\n",
        "    Extract numeric step safely.\n",
        "    - 'checkpoint-1234' -> 1234\n",
        "    - 'checkpoint-750-rebuilt' -> 750\n",
        "    - 'checkpoint-final' -> very large sentinel so it's treated as latest\n",
        "    - anything else -> -1 (ignored)\n",
        "    \"\"\"\n",
        "    m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
        "    if m:\n",
        "        return int(m.group(1))\n",
        "    if p.name == \"checkpoint-final\":\n",
        "        return 10**9\n",
        "    return -1\n",
        "\n",
        "candidates = []\n",
        "for root in (RUN_LOCAL, RUN_DRIVE):\n",
        "    if root.exists():\n",
        "        for p in root.glob(\"checkpoint-*\"):\n",
        "            if p.is_dir() and has_weights(p):\n",
        "                candidates.append(p)\n",
        "\n",
        "# Sort for best\n",
        "catalog = sorted([(ckpt_step(p), p) for p in candidates], key=lambda x: x[0])\n",
        "print(\"[CANDIDATES]\", [f\"{s}:{p.name}\" for s, p in catalog])\n",
        "\n",
        "latest = max(candidates, key=ckpt_step) if candidates else None\n",
        "if latest is not None and ckpt_step(latest) < 0:\n",
        "    latest = None\n",
        "\n",
        "# fallback to 750-rebuilt\n",
        "if latest is None and has_weights(RESUME_750):\n",
        "    latest = RESUME_750\n",
        "\n",
        "print(\"[RUN]\", RUN_ID)\n",
        "print(\"[BASE]\", RESUME_750, \"| has_weights:\", has_weights(RESUME_750))\n",
        "print(\"[LATEST]\", latest if latest else \"none yet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5bigjZhA6UF",
        "outputId": "4e918965-9309-4bd7-9842-12f45aaf04d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[CANDIDATES] ['100:checkpoint-100', '200:checkpoint-200', '300:checkpoint-300', '750:checkpoint-750-rebuilt']\n",
            "[RUN] t4_1762376560\n",
            "[BASE] /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt | has_weights: True\n",
            "[LATEST] /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hb watch"
      ],
      "metadata": {
        "id": "z_a0YMyADwYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HB quick status (with ages)\n",
        "from pathlib import Path\n",
        "import re, time, os\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "ROOT   = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "EMB    = ROOT / \"emb_snaps\" / RUN_ID\n",
        "SENTRY = ROOT / \"sentry_backups\" / RUN_ID\n",
        "\n",
        "def has_weights(ck: Path) -> bool:\n",
        "    if not ck.exists(): return False\n",
        "    if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(): return True\n",
        "    if list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\")): return True\n",
        "    return False\n",
        "\n",
        "def step_of(name: str) -> int:\n",
        "    if name == \"checkpoint-final\": return 10**9\n",
        "    m = re.search(r\"checkpoint-(\\d+)\", name); return int(m.group(1)) if m else -1\n",
        "\n",
        "def age(p: Path):\n",
        "    if not p.exists(): return \"n/a\"\n",
        "    secs = time.time() - p.stat().st_mtime\n",
        "    if secs < 90:   return f\"{int(secs)}s\"\n",
        "    if secs < 5400: return f\"{int(secs//60)}m\"\n",
        "    return f\"{secs/3600:.1f}h\"\n",
        "\n",
        "# snapshots\n",
        "snaps = sorted(EMB.glob(\"emb_step*.pt\"))\n",
        "snap_name = snaps[-1].name if snaps else \"none\"\n",
        "snap_age  = age(snaps[-1]) if snaps else \"n/a\"\n",
        "print(f\"[SNAP] {snap_name}  (age {snap_age})\")\n",
        "\n",
        "hb = EMB / \"heartbeat.json\"\n",
        "print(f\"[HB]   {('present, age '+age(hb)) if hb.exists() else 'none'}\")\n",
        "\n",
        "# checkpoints\n",
        "cands = [p for p in SENTRY.glob(\"checkpoint-*\") if p.is_dir() and has_weights(p)]\n",
        "cands = sorted(cands, key=lambda p: step_of(p.name))\n",
        "if cands:\n",
        "    latest = cands[-1]\n",
        "    print(f\"[CKPT] {latest.name}  (age {age(latest)})\")\n",
        "else:\n",
        "    print(\"[CKPT] none yet (first expected: checkpoint-0825)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27FZGfR3GOtT",
        "outputId": "07e610e0-9fbf-4d59-e785-70a89ba51097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] emb_step0800.pt  (age 21m)\n",
            "[HB]   present, age 20m\n",
            "[CKPT] checkpoint-750-rebuilt  (age 142.2h)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Heartbeat\n",
        "from pathlib import Path\n",
        "import re, time\n",
        "\n",
        "ROOT   = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "EMB    = ROOT / \"emb_snaps\" / RUN_ID\n",
        "SENTRY = ROOT / \"sentry_backups\" / RUN_ID\n",
        "\n",
        "def has_weights(ck: Path) -> bool:\n",
        "    if not ck.exists(): return False\n",
        "    if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(): return True\n",
        "    if list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\")): return True\n",
        "    return False\n",
        "\n",
        "def age_str(p: Path) -> str:\n",
        "    if not p.exists(): return \"n/a\"\n",
        "    secs = time.time() - p.stat().st_mtime\n",
        "    return (f\"{int(secs)}s\" if secs < 90 else f\"{int(secs//60)}m\" if secs < 5400 else f\"{secs/3600:.1f}h\")\n",
        "\n",
        "# Expected embedding snapshots every 50 from 800..1300\n",
        "snap_targets = [EMB / f\"emb_step{n:04d}.pt\" for n in range(800, 1301, 50)]\n",
        "snap_missing = [p.name for p in snap_targets if not p.exists()]\n",
        "snap_existing = [p.name for p in snap_targets if p.exists()]\n",
        "\n",
        "# Expected checkpoints:\n",
        "# - pre-1000: every 75 (825, 900, 975)\n",
        "# - ≥1000: extra every 100 (1000, 1100, 1200, 1300); you may ALSO see 1075/1175/1275 from base 75 saves\n",
        "ck_steps = sorted(set([825, 900, 975, 1000, 1075, 1100, 1175, 1200, 1275, 1300]))\n",
        "ck_targets = [SENTRY / f\"checkpoint-{s:04d}\" for s in ck_steps]\n",
        "ck_missing = [f.name for f in ck_targets if not has_weights(f)]\n",
        "ck_existing = [f.name for f in ck_targets if has_weights(f)]\n",
        "\n",
        "print(\"[WATCHING]\")\n",
        "print(\"  SNAPS :\", EMB)\n",
        "print(\"  SENTRY:\", SENTRY)\n",
        "\n",
        "# latest snap & checkpoint\n",
        "snaps = sorted(EMB.glob(\"emb_step*.pt\"))\n",
        "print(\"\\n[LATEST SNAP ]\", snaps[-1].name if snaps else \"none\", \"| age:\", age_str(snaps[-1]) if snaps else \"n/a\")\n",
        "\n",
        "cands = [p for p in SENTRY.glob(\"checkpoint-*\") if p.is_dir() and has_weights(p)]\n",
        "def step_of(name: str) -> int:\n",
        "    if name == \"checkpoint-final\": return 10**9\n",
        "    m = re.search(r\"checkpoint-(\\d+)\", name); return int(m.group(1)) if m else -1\n",
        "latest_ck = max(cands, key=lambda p: step_of(p.name)) if cands else None\n",
        "print(\"[LATEST CKPT]\", latest_ck.name if latest_ck else \"none\", \"| age:\", age_str(latest_ck) if latest_ck else \"n/a\")\n",
        "\n",
        "print(\"\\n[NEXT SNAP CANDIDATES]\", (snap_missing[:5] or [\"(all later)\"]))\n",
        "print(\"[HAVE SNAPS        ]\", (snap_existing[-5:] or [\"none\"]))\n",
        "print(\"\\n[NEXT CKPT CANDS  ]\", (ck_missing[:5] or [\"(all later)\"]))\n",
        "print(\"[HAVE CKPTS       ]\", (ck_existing[-5:] or [\"none\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3IDjlX8KA5x",
        "outputId": "3565bf27-61ab-4b1a-c976-e3b106a991cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WATCHING]\n",
            "  SNAPS : /content/drive/MyDrive/wake2vec/emb_snaps/t4_1762376560\n",
            "  SENTRY: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\n",
            "\n",
            "[LATEST SNAP ] emb_step0800.pt | age: 21m\n",
            "[LATEST CKPT] checkpoint-750-rebuilt | age: 142.2h\n",
            "\n",
            "[NEXT SNAP CANDIDATES] ['emb_step0850.pt', 'emb_step0900.pt', 'emb_step0950.pt', 'emb_step1000.pt', 'emb_step1050.pt']\n",
            "[HAVE SNAPS        ] ['emb_step0800.pt']\n",
            "\n",
            "[NEXT CKPT CANDS  ] ['checkpoint-0825', 'checkpoint-0900', 'checkpoint-0975', 'checkpoint-1000', 'checkpoint-1075']\n",
            "[HAVE CKPTS       ] ['none']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HB StepSpeed\n",
        "from pathlib import Path\n",
        "import re, time, math, json\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "ROOT   = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "SNAPS  = ROOT / \"emb_snaps\" / RUN_ID\n",
        "SENTRY = ROOT / \"sentry_backups\" / RUN_ID\n",
        "\n",
        "MILESTONES = [800, 825, 850, 900, 975, 1000, 1050, 1100, 1200, 1300]\n",
        "\n",
        "def last_snapshot():\n",
        "    snaps = sorted(SNAPS.glob(\"emb_step*.pt\"))\n",
        "    if not snaps: return None\n",
        "    p = snaps[-1]\n",
        "    m = re.search(r\"emb_step(\\d+)\\.pt\", p.name)\n",
        "    step = int(m.group(1)) if m else None\n",
        "    ts = p.stat().st_mtime\n",
        "    return step, ts, p\n",
        "\n",
        "def last_ckpt():\n",
        "    cands = [p for p in SENTRY.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "    if not cands: return None\n",
        "    def step_of(p):\n",
        "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
        "        return int(m.group(1)) if m else (-1 if p.name!=\"checkpoint-final\" else 10**9)\n",
        "    p = max(cands, key=step_of)\n",
        "    return step_of(p), p.stat().st_mtime, p\n",
        "\n",
        "def fmt_eta(sec):\n",
        "    if sec is None or sec == float(\"inf\"): return \"—\"\n",
        "    if sec < 90: return f\"{int(sec)}s\"\n",
        "    if sec < 3600: return f\"{int(sec//60)}m\"\n",
        "    return f\"{sec/3600:.1f}h\"\n",
        "\n",
        "print(\"[HB] watching:\")\n",
        "print(\"  snaps :\", SNAPS)\n",
        "print(\"  sentry:\", SENTRY)\n",
        "\n",
        "ema = None\n",
        "alpha = 0.3  # EMA smoothing\n",
        "\n",
        "prev = last_snapshot()\n",
        "if prev:\n",
        "    print(f\"[INIT] last snapshot: step {prev[0]} at {time.ctime(prev[1])}\")\n",
        "else:\n",
        "    print(\"[INIT] no snapshots yet; first expected at step 0800\")\n",
        "\n",
        "while True:\n",
        "    cur = last_snapshot()\n",
        "    ck  = last_ckpt()\n",
        "    now = time.time()\n",
        "\n",
        "    if cur and prev and cur[0] > prev[0]:\n",
        "        d_steps = cur[0] - prev[0]\n",
        "        d_time  = cur[1] - prev[1]\n",
        "        sps = d_time / max(d_steps, 1)\n",
        "        ema = sps if ema is None else (alpha*sps + (1-alpha)*ema)\n",
        "\n",
        "        # ETA to upcoming milestones\n",
        "        nxt = [m for m in MILESTONES if m > cur[0]]\n",
        "        etas = {m: fmt_eta(ema*(m - cur[0])) for m in nxt[:5]}\n",
        "\n",
        "        print(f\"[{cur[0]:4d}] Δ{d_steps} in {d_time:.1f}s → {sps:.2f}s/step | EMA {ema:.2f}s/step\")\n",
        "        if ck and ck[0] >= 800:\n",
        "            print(f\"      last ckpt: {ck[2].name} ({int(now-ck[1])}s ago)\")\n",
        "        print(\"      next:\", \", \".join([f\"{m}:{etas[m]}\" for m in nxt[:5]]) if nxt else \"done\")\n",
        "\n",
        "        prev = cur\n",
        "\n",
        "    elif cur and not prev:\n",
        "        prev = cur\n",
        "\n",
        "    else:\n",
        "        # still waiting for first new artifact\n",
        "        if cur:\n",
        "            print(f\"[{cur[0]:4d}] waiting for next snapshot… ({int(now-cur[1])}s since last)\")\n",
        "        else:\n",
        "            print(\"[WAIT] no snapshots yet (pre-0800)\")\n",
        "\n",
        "    time.sleep(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "HdS3rkczTlfU",
        "outputId": "0f2cbba1-f6a0-45c4-bcdf-91a323e16e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HB] watching:\n",
            "  snaps : /content/drive/MyDrive/wake2vec/emb_snaps/t4_1762376560\n",
            "  sentry: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\n",
            "[INIT] last snapshot: step 800 at Sat Nov 15 21:55:50 2025\n",
            "[ 800] waiting for next snapshot… (1276s since last)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1277858590.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[WAIT] no snapshots yet (pre-0800)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HB one-shot\n",
        "from pathlib import Path\n",
        "import re, time, json\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "ROOT   = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "SNAPS  = ROOT / \"emb_snaps\" / RUN_ID\n",
        "SENTRY = ROOT / \"sentry_backups\" / RUN_ID\n",
        "MILESTONES = [800, 825, 850, 900, 975, 1000, 1050, 1100, 1200, 1300]\n",
        "\n",
        "def last_snapshot():\n",
        "    snaps = sorted(SNAPS.glob(\"emb_step*.pt\"))\n",
        "    if not snaps: return None\n",
        "    p = snaps[-1]\n",
        "    m = re.search(r\"emb_step(\\d+)\\.pt\", p.name)\n",
        "    step = int(m.group(1)) if m else None\n",
        "    ts = p.stat().st_mtime\n",
        "    return step, ts, p\n",
        "\n",
        "def last_ckpt():\n",
        "    cands = [p for p in SENTRY.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "    if not cands: return None\n",
        "    def step_of(p):\n",
        "        m = re.search(r\"checkpoint-(\\d+)\", p.name)\n",
        "        return int(m.group(1)) if m else (-1 if p.name!=\"checkpoint-final\" else 10**9)\n",
        "    p = max(cands, key=step_of)\n",
        "    return step_of(p), p.stat().st_mtime, p\n",
        "\n",
        "cur = last_snapshot()\n",
        "ck  = last_ckpt()\n",
        "\n",
        "print(\"[SNAP]\", cur[2].name if cur else \"none\", \"| age:\", f\"{int(time.time()-cur[1])}s\" if cur else \"n/a\")\n",
        "print(\"[CKPT]\", ck[2].name if ck else \"none\",  \"| age:\", f\"{int(time.time()-ck[1])}s\" if ck else \"n/a\")\n",
        "\n",
        "EMA_FILE = SNAPS.parent / \"speed_ema.json\"\n",
        "def load_ema():\n",
        "    try:\n",
        "        return json.loads(EMA_FILE.read_text())[\"ema_s_per_step\"]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "ema = load_ema()\n",
        "if cur and ema:\n",
        "    nxt = [m for m in MILESTONES if m > cur[0]][:2]\n",
        "    for m in nxt:\n",
        "        eta = ema*(m - cur[0])\n",
        "        print(f\"[ETA] → {m}: ~{int(eta//60)}m {int(eta%60)}s\")\n",
        "else:\n",
        "    print(\"[ETA] need at least two snapshots (e.g., 750→800) to estimate speed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmD20Zm8Ujpm",
        "outputId": "9dd3d4bf-1417-43a7-b287-3c0ee73eafc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] emb_step0800.pt | age: 1172s\n",
            "[CKPT] checkpoint-750-rebuilt | age: 511827s\n",
            "[ETA] need at least two snapshots (e.g., 750→800) to estimate speed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for <750"
      ],
      "metadata": {
        "id": "QtsDnDqReEuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "SENTRY = Path(\"/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\")\n",
        "\n",
        "print(\"[HEARTBEAT MONITOR]\")\n",
        "print(f\"Watching: {SENTRY}\\n\")\n",
        "\n",
        "def has_weights(ck):\n",
        "    return (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "\n",
        "last_seen = -1\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        checkpoints = sorted([p for p in SENTRY.glob(\"checkpoint-*\") if p.is_dir() and has_weights(p)],\n",
        "                           key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "\n",
        "        if checkpoints:\n",
        "            latest = checkpoints[-1]\n",
        "            step = int(latest.name.split(\"-\")[-1])\n",
        "\n",
        "            if step > last_seen:\n",
        "                mtime = time.ctime(latest.stat().st_mtime)\n",
        "                print(f\"[{time.strftime('%H:%M:%S')}] {latest.name} (saved: {mtime})\")\n",
        "                last_seen = step\n",
        "\n",
        "                remaining = 1300 - step\n",
        "                print(f\"  Progress: {step}/1300 ({remaining} steps remaining)\\n\")\n",
        "        else:\n",
        "            print(f\"[{time.strftime('%H:%M:%S')}] No checkpoints in sentry yet...\")\n",
        "\n",
        "        time.sleep(600)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(f\"\\n[STOPPED] Last checkpoint: {last_seen}\")"
      ],
      "metadata": {
        "id": "GI5Cbagz7JdE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
