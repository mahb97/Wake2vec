# Wake2Vec

## TL;DR

I extend a tokenizer with a curated *Finnegans Wake* lexicon, initialise new vectors by morpheme composition, and fine-tune with a two-phase protocol that protects stability. I report geometry shifts (top-k neighbour overlap, embedding norm deltas, isotropy and PIP loss if requested), language behaviour (validation loss and perplexity on held-out Wake slices), and qualitative intrusion. The full pipeline reproduces on a Colab T4.

## Why This Project

Style control is often attempted through prompts or full fine-tuning. Wake2Vec explores a third path: an embedding-first intervention that inserts Joyce-specific forms and trains the input layer in a controlled way. The goal is local, interpretable changes to semantic neighbourhoods under tight compute, with results that can be verified and challenged.

## Method

### Lexicon and Morphology

A hand-curated prefix and suffix lexicon is parsed into maps of morpheme to examples. Synthetic Joyce-style forms are generated by sampling prefix, root, and suffix with frequency weighting.

### Tokenizer Augmentation

New forms are added to the tokenizer. I disable mean-resizing when expanding the embedding matrix so that custom initialisation is preserved, and I tie the output head to the input embeddings so the new vectors participate in prediction.

### Compositional Initialisation

Each new vector is composed as a weighted sum of prefix, root, and suffix embeddings with small Gaussian noise for diversity. If a morpheme is not a single token, I average embeddings of its high-quality examples.

### Two-Phase Training

**Phase 1** warms the new embeddings on synthetic sentences while all other weights are frozen. **Phase 2** fine-tunes the full model on *Finnegans Wake* with conservative schedules, early stopping on validation loss, and pinned software versions. No fp16 is used on T4 to avoid instability.

## Data and Setup

- **Base text**: *Finnegans Wake* plain text
- **Synthetic sentences**: Short prompts that contain the injected forms
- **Token additions**: Recent run added 447 new tokens after filtering duplicates
- **Tokenizer vocabulary size after expansion**: 33,098
- **Maximum sequence length**: 2,048
- **Datasets**: Blockified Wake text with a held-out set
  - Train blocks: 1,566
  - Valid blocks: 174

## Training Configuration

**Environment**: `transformers==4.57.1` with Adafactor or 8-bit Adam if available, Python 3.12 on Colab T4.

**Phase 1 (embedding-only)**:
- Epochs: 1
- Learning rate: 5e-4
- Batch size: 8
- Gradient accumulation: 2
- Warmup ratio: 0.0
- Save steps: 200

**Phase 2 (full model)**:
- Epochs: 2
- Learning rate: 2e-5
- Warmup ratio: 0.10
- Batch size: 8
- Gradient accumulation: 2
- Weight decay: 0.01
- Save steps: 200
- Early stopping with patience: 2

**Memory and stability**: `use_cache=False`, gradient checkpointing on in Phase 2, no half precision on T4.

## Results from the Latest Run

This run evaluates a subset of 49 synthetic morphemic tokens with pre and post snapshots.

### Geometry and Stability

- **Mean neighbour overlap (top-5)**: 3.7 of 5
- **Mean embedding norm change**: 0.0051

**Interpretation**: Neighbourhoods remain coherent; the vectors move slightly toward Wake-like regions without collapse or uncontrolled drift.

### Representative Examples

- **conmanes** — overlap 4 of 5, neighbours: manes, conmaning, enmanes, comanes
- **presounder** — overlap 4 of 5, neighbours: presounded, ensounder, resound, soundy
- **soundity** — overlap 3 of 5, shows modest drift yet remains in the sound cluster

### Visual Diagnostics

- t-SNE shows the new tokens clustered near centroids of their pre-training neighbour sets
- Histograms for neighbour overlap and norm change show a stable centre with light positive shift
- Scatter of norm change versus overlap highlights a small tail of tokens to inspect manually


## Evaluation

- **Geometry**: Top-k neighbour overlap before and after, embedding norm deltas, optional isotropy and PIP loss
- **Language**: Validation loss and optional perplexity on a held-out Wake slice
- **Behaviour**: Short generation probes that seed with high-drift and low-drift tokens, and nearest-neighbour maps saved to JSON for audit

## Quickstart on T4 or CPU
```bash
pip install -r requirements.txt
```

1. **Lexicon**: Parse or regenerate the morpheme maps and write `wake_lexicon.txt`
2. **Token injection**: Expand the tokenizer, compose embeddings, tie the head
3. **Training**: Run Phase 1 embedding warm-up, then Phase 2 full fine-tune
4. **Metrics and report**: Write snapshots, compute overlaps, and build `results/wake2vec_report.html`

### Model Choice

- **On GPU**: `BASE_MODEL="TinyLlama/TinyLlama-1.1B-Chat-v1.0"` is a good default
- **On CPU**: `BASE_MODEL="distilgpt2"` to smoke-test the pipeline

## Artifacts Saved Automatically

- `results/summary_stats.json`, `results/morpheme_comparison.json`
- `results/pre_morpheme_snapshot.json`, `results/post_morpheme_snapshot.json`
- `results/wake2vec_report.html` with t-SNE, histograms, and tables
- `checkpoints/*` and a `run_meta.json` that records hyperparameters and paths

## Practical Notes

- If `load_best_model_at_end=True`, match `eval_strategy` and `save_strategy` to `"steps"`
- Prefer Adafactor or 8-bit Adam on T4. Reduce per-device batch size and increase gradient accumulation if memory is tight
- Keep fp16 off on T4 for this pipeline. Set `use_cache=False` during training to reduce memory

## Citation and Credit

- **Text**: James Joyce, *Finnegans Wake*
- **Base model**: [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- Conceptual inspiration from work on embedding surgery, retrofitting, and lightweight adapter methods

---

**cite**: (https://github.com/mahb97/Wake2vec/blob/21469d75c26d40988ec5af8a4358d1796a36fdf0/data/CITATION.cff)

