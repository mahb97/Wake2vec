{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMioJWb4NVRNqNxkn/qwsII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_LLaMA_2_13B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec: LLaMA-2-13B Embeddings fun (4-bit, NC-17)\n",
        "\n",
        "**What this notebook does**\n",
        "- Injects a custom fw lexicon into LLaMA-2-13B (4-bit, T4-safe)\n",
        "- Trains only the input embedding matrix (lm_head hard-tied)\n",
        "- Uses an aggressive spherical init for new tokens\n",
        "- Masks gradients to update only new rows, keeping the base space fixed.\n",
        "- Adds a repulsion regularizer so Wake tokens don’t collapse into a single cluster.\n",
        "- Optional norm clamp to keep things wild but stable.\n",
        "- Saves compact embedding-only artifacts and writes geometry receipts:\n",
        "  - PIP loss (global geometry shift)\n",
        "  - Isotropy (spectral health)\n",
        "  - Top-k neighbor overlap (neighborhood reshuffle)\n",
        "\n",
        "aim is to push maximum local semantic drift with minimal compute. TinyLlama is cute; this is not.\n",
        "\n",
        "\n",
        "**Outputs**\n",
        "- `embedding_only/embed_tokens.pt` — updated embedding weights\n",
        "- `added_tokens.json` — which tokens were injected\n",
        "- `geometry_report.json` — PIP / isotropy / overlap metrics\n"
      ],
      "metadata": {
        "id": "Jbq2Ra4vimNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Ii-33Ge8gN"
      },
      "outputs": [],
      "source": [
        "%capture\n",
        "# Wake2Vec: fuck the embeddings edition\n",
        "# lets see how far that t4 will go on this embedding rampage (PG 18+) — LLaMA ONLY, 4-bit, Embedding-Only\n",
        "\n",
        "!pip -q install torch==2.4.0 bitsandbytes==0.43.3 transformers==4.45.2 accelerate==0.34.2 datasets==2.20.0 peft==0.13.2 --progress-bar off\n",
        "\n",
        "import os, math, json, random, gc, torch, torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, set_seed)\n",
        "\n",
        "# CONFIG LLaMA\n",
        "SEED = 42\n",
        "MODEL_NAME = \"meta-llama/Llama-2-13b-hf\"\n",
        "WAKE_LEX_PATH = \"/mnt/data/wake_lexicon.txt\"\n",
        "CORPUS_TXT    = \"/content/finnegans_wake.txt\"\n",
        "RUN_DIR = \"/content/wake_llama_embs\"\n",
        "SEQ_LEN = 1024\n",
        "MAX_STEPS = 1100\n",
        "LOG_STEPS = 20\n",
        "SAVE_STEPS = 200\n",
        "LR = 8e-4\n",
        "GRAD_ACCUM = 8\n",
        "REPULSION_W = 0.05\n",
        "TARGET_NORM = None            # e.g. 1.8 * base_radius\n",
        "MAX_ROW_NORM = None           # e.g. 2.0 * base_radius\n",
        "REPORT_SAMPLE = 1500\n",
        "\n",
        "os.makedirs(RUN_DIR, exist_ok=True); set_seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# helpers\n",
        "def read_lines(p, fb=None):\n",
        "    if not os.path.exists(p): return fb or []\n",
        "    return [x.strip() for x in open(p, encoding=\"utf-8\") if x.strip()]\n",
        "\n",
        "def read_corpus(p):\n",
        "    if not os.path.exists(p):\n",
        "        s = (\"riverrun, past Eve and Adam’s, from swerve of shore to bend of bay, \"\n",
        "             \"brings us by a commodius vicus of recirculation to Howth Castle and Environs.\")\n",
        "        return Dataset.from_dict({\"text\":[s]*1000})\n",
        "    txt = open(p, \"r\", encoding=\"utf-8\").read()\n",
        "    paras = [t.strip() for t in txt.split(\"\\n\") if t.strip()]\n",
        "    return Dataset.from_dict({\"text\": paras})\n",
        "\n",
        "def pack_causal(ex, tok, block):\n",
        "    ids = tok(\"\\n\\n\".join(ex[\"text\"]), add_special_tokens=False)[\"input_ids\"]\n",
        "    chunks = [ids[i:i+block] for i in range(0, len(ids)-block, block)]\n",
        "    return {\"input_ids\": chunks, \"labels\": chunks.copy()}\n",
        "\n",
        "def isotropy(M):\n",
        "    u, s, v = torch.pca_lowrank(M - M.mean(0, keepdim=True), q=min(128, M.shape[1]-1))\n",
        "    return float((s.max() / s.min().clamp_min(1e-8)))\n",
        "\n",
        "def pip_loss(A, B):\n",
        "    return float(torch.norm((A@A.T)-(B@B.T), p='fro')/(A.shape[0]**2))\n",
        "\n",
        "def topk_overlap(M1, M2, k=10, sample=1000):\n",
        "    W1 = M1/(M1.norm(dim=1, keepdim=True)+1e-8); W2 = M2/(M2.norm(dim=1, keepdim=True)+1e-8)\n",
        "    vocab = W1.shape[0]; idxs = random.sample(range(vocab), min(sample, vocab))\n",
        "    acc = 0.0\n",
        "    for i in idxs:\n",
        "        c1 = torch.topk(W1 @ W1[i], k+1).indices.tolist(); c1=[j for j in c1 if j!=i][:k]\n",
        "        c2 = torch.topk(W2 @ W2[i], k+1).indices.tolist(); c2=[j for j in c2 if j!=i][:k]\n",
        "        acc += len(set(c1)&set(c2))/k\n",
        "    return float(acc/len(idxs))\n",
        "\n",
        "# LLaMA in 4-bit\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                         bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb,\n",
        "                                             torch_dtype=torch.float16, device_map=\"auto\")\n",
        "if hasattr(model, \"tie_weights\"): model.tie_weights()\n",
        "model.config.tie_word_embeddings = True\n",
        "\n",
        "# Wake lex\n",
        "wake = read_lines(WAKE_LEX_PATH)\n",
        "missing = [t for t in wake if tok.convert_tokens_to_ids(t)==tok.unk_token_id]\n",
        "num_added = tok.add_tokens(missing, special_tokens=False)\n",
        "old_vocab = model.get_input_embeddings().weight.shape[0]\n",
        "model.resize_token_embeddings(len(tok))\n",
        "wte = model.get_input_embeddings()\n",
        "if hasattr(model, \"lm_head\"): model.lm_head.weight = wte.weight  # ensure tie\n",
        "\n",
        "# spherical kick init\n",
        "with torch.no_grad():\n",
        "    base = wte.weight[:old_vocab]; dim = base.shape[1]\n",
        "    std = base.std().item(); base_radius = std * math.sqrt(dim)\n",
        "    target_radius = TARGET_NORM or (1.5 * base_radius)\n",
        "    if num_added>0:\n",
        "        new = torch.randn((num_added, dim), device=wte.weight.device)\n",
        "        new = new/(new.norm(dim=1, keepdim=True)+1e-8)*target_radius\n",
        "        wte.weight.data[old_vocab:old_vocab+num_added] = new\n",
        "\n",
        "# trainables: ONLY new rows\n",
        "for n,p in model.named_parameters(): p.requires_grad=False\n",
        "wte.weight.requires_grad=True\n",
        "new_rows = torch.arange(old_vocab, old_vocab+num_added, device=wte.weight.device) if num_added>0 else None\n",
        "base_rows = torch.arange(0, old_vocab, device=wte.weight.device)\n",
        "\n",
        "def mask_grad(grad):\n",
        "    if grad is None or new_rows is None: return grad\n",
        "    grad[base_rows]=0; return grad\n",
        "wte.weight.register_hook(mask_grad)\n",
        "\n",
        "def clamp_rows_(emb, max_norm):\n",
        "    if max_norm is None or new_rows is None: return\n",
        "    rows = emb.weight.data[old_vocab:old_vocab+num_added]\n",
        "    norms = rows.norm(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "    scale = (max_norm/norms).clamp_max(1.0)\n",
        "    emb.weight.data[old_vocab:old_vocab+num_added] = rows*scale\n",
        "\n",
        "# data\n",
        "ds = read_corpus(CORPUS_TXT)\n",
        "split = ds.train_test_split(test_size=0.05, seed=SEED)\n",
        "tok_tr = split[\"train\"].map(lambda e: pack_causal(e, tok, SEQ_LEN), batched=True, remove_columns=[\"text\"])\n",
        "tok_ev = split[\"test\"].map(lambda e: pack_causal(e, tok, SEQ_LEN), batched=True, remove_columns=[\"text\"])\n",
        "coll = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# receipts (pre)\n",
        "with torch.no_grad():\n",
        "    pre_full = wte.weight.detach().clone().cpu()\n",
        "    pre_new  = pre_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, pre_full.shape[1])\n",
        "\n",
        "# trainer with repulsion\n",
        "class EmbOnlyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        out = model(**inputs); loss = out.loss\n",
        "        if num_added and num_added>1 and REPULSION_W>0:\n",
        "            E = model.get_input_embeddings().weight[old_vocab:old_vocab+num_added]\n",
        "            E = E - E.mean(0, keepdim=True); E = E/(E.norm(dim=1, keepdim=True)+1e-8)\n",
        "            sims = (E @ E.t()); repul = (sims - torch.eye(E.shape[0], device=E.device)).pow(2).mean()\n",
        "            loss = loss + REPULSION_W*repul\n",
        "        return (loss, out) if return_outputs else loss\n",
        "    def training_step(self, *args, **kwargs):\n",
        "        out = super().training_step(*args, **kwargs)\n",
        "        clamp_rows_(model.get_input_embeddings(), MAX_ROW_NORM)\n",
        "        return out\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=RUN_DIR, per_device_train_batch_size=1, gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR, max_steps=MAX_STEPS, warmup_steps=max(20, MAX_STEPS//20),\n",
        "    lr_scheduler_type=\"cosine\", weight_decay=0.0, fp16=True, bf16=False,\n",
        "    logging_steps=LOG_STEPS, save_steps=SAVE_STEPS, save_total_limit=3,\n",
        "    evaluation_strategy=\"no\", report_to=\"none\", dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "trainer = EmbOnlyTrainer(model=model, args=args, data_collator=coll, train_dataset=tok_tr)\n",
        "\n",
        "print(f\"[Run] LLaMA emb-only | steps={MAX_STEPS} | seq_len={SEQ_LEN} | accum={GRAD_ACCUM} | LR={LR}\")\n",
        "trainer.train()\n",
        "\n",
        "# save deltas\n",
        "save_dir = os.path.join(RUN_DIR, \"embedding_only\"); os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(wte.weight.detach().cpu(), os.path.join(save_dir, \"embed_tokens.pt\"))\n",
        "with open(os.path.join(save_dir, \"added_tokens.json\"), \"w\") as f:\n",
        "    json.dump({\"added_tokens\": missing, \"old_vocab\": old_vocab, \"num_added\": num_added}, f, indent=2)\n",
        "tok.save_pretrained(RUN_DIR)\n",
        "\n",
        "# receipts\n",
        "with torch.no_grad():\n",
        "    post_full = wte.weight.detach().clone().cpu()\n",
        "    post_new  = post_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, post_full.shape[1])\n",
        "\n",
        "report = {\n",
        "  \"model\": MODEL_NAME, \"added_tokens\": int(num_added), \"old_vocab\": int(old_vocab),\n",
        "  \"pip_loss_full\": pip_loss(pre_full, post_full),\n",
        "  \"topk_overlap_all\": topk_overlap(pre_full, post_full, k=10, sample=min(REPORT_SAMPLE, pre_full.shape[0]-1)),\n",
        "  \"isotropy_pre\": isotropy(pre_full), \"isotropy_post\": isotropy(post_full),\n",
        "  \"pip_loss_new_rows\": (pip_loss(pre_new, post_new) if num_added>1 else None),\n",
        "  \"isotropy_new_rows\": (isotropy(post_new) if num_added>1 else None),\n",
        "}\n",
        "json.dump(report, open(os.path.join(RUN_DIR, \"geometry_report.json\"), \"w\"), indent=2)\n",
        "print(\"\\n=== GEOMETRY REPORT ===\")\n",
        "for k,v in report.items(): print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nothing makes me smile more than \"gosh honey you are still working with the Wake are you okay babes?\"\n",
        "\n",
        "and maybe \"oh we’re doing violence to vectors today? say less.\""
      ],
      "metadata": {
        "id": "XR9htRlAe9AK"
      }
    }
  ]
}
