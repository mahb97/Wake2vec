# -*- coding: utf-8 -*-
"""Wake2vec_Llama-3-1.8b

Wake2Vec "F* the Embeddings" (T4 Edition)

This is a colab-friendly, embedding-only finetune pipeline for large decoder LMs 
(Mistral-7B / Llama-2-13B / Llama-3.1-8B) using a Wake lexicon injection. It adds 
Joyce-specific tokens, initializes them on a sphere, and trains only the input 
embedding rows (optionally with a minimal LoRA r=1 on q_proj to satisfy 
quantized-training rules).

Phase 2 Configuration:
- max_steps: 1500-2500
- lr: 5e-4 → 1e-4 (cosine decay)
- batch_size: 1
- grad_accum: 16
- Custom loss = LM_loss + λ₁·attraction + λ₂·repulsion + λ₃·morphological + λ₄·adversarial
"""

import os
import math
import json
import random
import shutil
import gc
from pathlib import Path
from getpass import getpass

import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from numpy.linalg import norm

# Transformers and PEFT
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    set_seed
)
from peft import LoraConfig, get_peft_model
from huggingface_hub import login


# configs

# Stop TorchAO
os.environ["TRANSFORMERS_NO_TORCHAO"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

SEED = 42
set_seed(SEED)

MODEL_NAME = "meta-llama/Llama-3.2-1B"
WAKE_LEX_PATH = "/content/wake_lexicon.txt"
CORPUS_TXT = "/content/FW_TEXT.txt"

# Paths - CRITICAL: Save to Drive, not /content
RUN_DIR = Path("/content/drive/MyDrive/wake_llama_P1")
LOCAL_RUN = Path("/content/runs/wake_llama_P1")
SENTRY = RUN_DIR / "sentry_backups"

# Training hyperparameters
SEQ_LEN = 512
STRIDE = 512
MAX_STEPS = 1100
LOG_STEPS = 20
SAVE_STEPS = 200
LR = 5e-5
GRAD_ACCUM = 8
CHECKPOINT_NUM = 600  # For resuming


# helper functions

def read_lines(path):
    """Read lines from a text file."""
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]


class BlockDataset(Dataset):
    """Sliding window dataset for causal LM training."""
    
    def __init__(self, txt_path, tokenizer, seq_len=512, stride=512):
        if not os.path.exists(txt_path):
            raise FileNotFoundError(f"Text file not found: {txt_path}")
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        self.tokens = tokenizer(text, add_special_tokens=False)['input_ids']
        self.seq_len = seq_len
        self.stride = stride
        self.starts = list(range(0, len(self.tokens) - seq_len + 1, stride))
    
    def __len__(self):
        return len(self.starts)
    
    def __getitem__(self, idx):
        start = self.starts[idx]
        chunk = self.tokens[start:start + self.seq_len]
        ids = torch.tensor(chunk, dtype=torch.long)
        return {
            'input_ids': ids,
            'labels': ids.clone(),
            'attention_mask': torch.ones_like(ids)
        }


def has_weights(ck):
    """Check if checkpoint has model weights."""
    return (ck / "adapter_model.safetensors").exists() or (ck / "pytorch_model.bin").exists()


class SentryMirror(TrainerCallback):
    """Callback to mirror checkpoints to Google Drive."""
    
    def on_save(self, args, state, control, **kw):
        try:
            cks = sorted(
                LOCAL_RUN.glob("checkpoint-*"),
                key=lambda p: int(p.name.split("-")[-1]),
                reverse=True
            )
            if not cks:
                return
            
            ck = cks[0]
            if not has_weights(ck):
                print(f"[SENTRY] {ck.name} no weights, skip")
                return
            
            dst = SENTRY / ck.name
            if not dst.exists():
                print(f"[SENTRY] Mirroring {ck.name}...")
                shutil.copytree(ck, dst)
                print(f"[SENTRY] {ck.name} backed up to Drive")
            os.sync()
        except Exception as e:
            print(f"[SENTRY] ERROR: {e}")


class EmbOnlyTrainer(Trainer):
    """Custom trainer that only optimizes embeddings."""
    
    def create_optimizer(self):
        from torch.optim import AdamW
        if not hasattr(self, "optimizer") or self.optimizer is None:
            # Ensure embeddings are trainable
            wte.weight.requires_grad = True
            self.optimizer = AdamW(
                [{"params": [wte.weight], "lr": LR, "weight_decay": 0.0}],
                betas=(0.9, 0.999), eps=1e-8
            )
        return self.optimizer
    
    def compute_loss(self, model, inputs, return_outputs=False):
        out = model(**inputs, use_cache=False)
        loss = out.loss
        if torch.isnan(loss) or torch.isinf(loss):
            raise RuntimeError("NaN/Inf loss detected")
        return (loss, out) if return_outputs else loss

# main exec

if __name__ == "__main__":
    # Note: This file is designed to be run in Google Colab
    # The following sections would need to be uncommented and run in sequence
    
    print("Wake2Vec Llama Training Script")
    print("=" * 80)
    print("This script is designed for Google Colab execution.")
    print("Please run the cells in sequence in a Colab notebook.")
    print("=" * 80)
