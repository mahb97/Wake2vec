{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "mount_file_id": "19NmxnOQ7c82bZJaM5yckEiwfqjIwQMZn",
      "authorship_tag": "ABX9TyPiSvpSEV6YeaUT6TLA1b1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2vec_Llama_3_1_8b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "guardrail"
      ],
      "metadata": {
        "id": "Hxmoqws8wF_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop TorchAO\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHAO\"] = \"1\""
      ],
      "metadata": {
        "id": "vv0Oy84mIC99"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to match Colab 2025.10 (Torch 2.8.0+cu126)\n",
        "!pip -q install -U triton==3.2.0 bitsandbytes==0.44.1 \\\n",
        "    transformers==4.45.2 accelerate==0.34.2 peft==0.13.2 --progress-bar off\n",
        "\n",
        "import torch, bitsandbytes as bnb, triton\n",
        "print(\"torch:\", torch.__version__, \"| cuda:\", torch.version.cuda)\n",
        "print(\"bnb:\", bnb.__version__, \"| triton:\", triton.__version__)"
      ],
      "metadata": {
        "id": "dBSraP8yLrJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = getpass(\"Paste your HF token (hidden): \")\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "UcbqrQFzIwtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B\"  # or \"...-8B-Instruct\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, token=HF_TOKEN)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "0XwMuvDiI1Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "HF_TOKEN = getpass(\"HF token (hidden): \")\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=HF_TOKEN)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Colab/T4 sanity\n",
        "model.config.attn_implementation = \"eager\"\n",
        "model.config.tie_word_embeddings = True\n",
        "if hasattr(model, \"tie_weights\"):\n",
        "    model.tie_weights()\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME, \"| dtype:\", model.dtype, \"| pad_token_id:\", tok.pad_token_id)"
      ],
      "metadata": {
        "id": "g4pjNK8xJc_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, json, random, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, set_seed)\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# CONFIG\n",
        "SEED = 42\n",
        "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "WAKE_LEX_PATH = \"/content/wake_lexicon.txt\"\n",
        "CORPUS_TXT    = \"/content/FW_TEXT.txt\"\n",
        "RUN_DIR = \"/content/wake_llama_embs\"\n",
        "SEQ_LEN = 1024\n",
        "STRIDE  = 1024\n",
        "MAX_STEPS = 1100\n",
        "LOG_STEPS = 20\n",
        "SAVE_STEPS = 200\n",
        "LR = 8e-4\n",
        "GRAD_ACCUM = 8\n",
        "REPULSION_W = 0.05\n",
        "TARGET_NORM = None\n",
        "MAX_ROW_NORM = None\n",
        "REPORT_SAMPLE = 1500\n",
        "\n",
        "os.makedirs(RUN_DIR, exist_ok=True); set_seed(SEED)\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "def read_lines(p, fb=None):\n",
        "    if not os.path.exists(p): return fb or []\n",
        "    return [x.strip() for x in open(p, encoding=\"utf-8\") if x.strip()]\n",
        "\n",
        "# Tiny torch-only dataset\n",
        "class BlockDataset(Dataset):\n",
        "    def __init__(self, path, tok, seq_len=1024, stride=1024):\n",
        "        if not os.path.exists(path):\n",
        "            stub = (\"riverrun, past Eve and Adam’s, from swerve of shore to bend of bay, \"\n",
        "                    \"brings us by a commodius vicus of recirculation to Howth Castle and Environs. \")\n",
        "            text = stub * 2000\n",
        "        else:\n",
        "            text = open(path, \"r\", encoding=\"utf-8\").read()\n",
        "        ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
        "        blocks=[]\n",
        "        for i in range(0, max(1, len(ids)-seq_len), stride):\n",
        "            chunk = ids[i:i+seq_len]\n",
        "            if len(chunk) >= seq_len//2:\n",
        "                blocks.append(chunk[:seq_len])\n",
        "        self.blocks = blocks\n",
        "    def __len__(self): return len(self.blocks)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.tensor(self.blocks[idx], dtype=torch.long)\n",
        "        return {\"input_ids\": ids, \"labels\": ids.clone()}\n",
        "\n",
        "# 4-bit load\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                         bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb,\n",
        "                                             torch_dtype=torch.float16, device_map=\"auto\")\n",
        "model.config.attn_implementation = \"eager\"  # T4 stable\n",
        "if hasattr(model, \"tie_weights\"): model.tie_weights()\n",
        "model.config.tie_word_embeddings = True\n",
        "\n",
        "# Minimal PEFT adapter to satisfy “no pure-quant training”\n",
        "peft_cfg = LoraConfig(r=1, lora_alpha=1, lora_dropout=0.0,\n",
        "                      target_modules=[\"q_proj\"], bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "# Wake lex\n",
        "wake = read_lines(WAKE_LEX_PATH)\n",
        "missing = [t for t in wake if tok.convert_tokens_to_ids(t)==tok.unk_token_id]\n",
        "num_added = tok.add_tokens(missing, special_tokens=False)\n",
        "old_vocab = model.get_input_embeddings().weight.shape[0]\n",
        "model.resize_token_embeddings(len(tok))\n",
        "wte = model.get_input_embeddings()\n",
        "if hasattr(model, \"lm_head\"): model.lm_head.weight = wte.weight  # re-tie\n",
        "\n",
        "# Spherical kick\n",
        "with torch.no_grad():\n",
        "    base = wte.weight[:old_vocab]; dim = base.shape[1]\n",
        "    std = base.std().item(); base_radius = std * math.sqrt(dim)\n",
        "    target_radius = TARGET_NORM or (1.5 * base_radius)\n",
        "    if num_added>0:\n",
        "        new = torch.randn((num_added, dim), device=wte.weight.device)\n",
        "        new = new/(new.norm(dim=1, keepdim=True)+1e-8)*target_radius\n",
        "        wte.weight.data[old_vocab:old_vocab+num_added] = new\n",
        "print(f\"[Init] new rows: {num_added} | target L2 ≈ {target_radius:.3f}\")\n",
        "\n",
        "# Trainables: only embeddings\n",
        "for n,p in model.named_parameters(): p.requires_grad=False\n",
        "wte.weight.requires_grad=True\n",
        "new_rows = torch.arange(old_vocab, old_vocab+num_added, device=wte.weight.device) if num_added>0 else None\n",
        "base_rows = torch.arange(0, old_vocab, device=wte.weight.device)\n",
        "def mask_grad(grad):\n",
        "    if grad is None or new_rows is None: return grad\n",
        "    grad[base_rows]=0; return grad\n",
        "wte.weight.register_hook(mask_grad)\n",
        "\n",
        "def clamp_rows_(emb, max_norm):\n",
        "    if max_norm is None or new_rows is None: return\n",
        "    rows = emb.weight.data[old_vocab:old_vocab+num_added]\n",
        "    norms = rows.norm(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "    scale = (max_norm/norms).clamp_max(1.0)\n",
        "    emb.weight.data[old_vocab:old_vocab+num_added] = rows*scale\n",
        "\n",
        "# Data\n",
        "train_ds = BlockDataset(CORPUS_TXT, tok, seq_len=SEQ_LEN, stride=STRIDE)\n",
        "coll = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Geometry (pre)\n",
        "@torch.no_grad()\n",
        "def isotropy(M):\n",
        "    u, s, v = torch.pca_lowrank(M - M.mean(0, keepdim=True), q=min(128, M.shape[1]-1))\n",
        "    return float((s.max() / s.min().clamp_min(1e-8)))\n",
        "@torch.no_grad()\n",
        "def pip_loss(A, B):\n",
        "    return float(torch.norm((A@A.T)-(B@B.T), p='fro')/(A.shape[0]**2))\n",
        "@torch.no_grad()\n",
        "def topk_overlap(M1, M2, k=10, sample=1000):\n",
        "    W1 = M1/(M1.norm(dim=1, keepdim=True)+1e-8); W2 = M2/(M2.norm(dim=1, keepdim=True)+1e-8)\n",
        "    vocab = W1.shape[0]; idxs = random.sample(range(vocab), min(sample, vocab))\n",
        "    acc = 0.0\n",
        "    for i in idxs:\n",
        "        c1 = torch.topk(W1 @ W1[i], k+1).indices.tolist(); c1=[j for j in c1 if j!=i][:k]\n",
        "        c2 = torch.topk(W2 @ W2[i], k+1).indices.tolist(); c2=[j for j in c2 if j!=i][:k]\n",
        "        acc += len(set(c1)&set(c2))/k\n",
        "    return float(acc/len(idxs))\n",
        "\n",
        "with torch.no_grad():\n",
        "    pre_full = wte.weight.detach().clone().cpu()\n",
        "    pre_new  = pre_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, pre_full.shape[1])\n",
        "\n",
        "# Trainer\n",
        "class EmbOnlyTrainer(Trainer):\n",
        "    def create_optimizer(self):\n",
        "        from torch.optim import AdamW\n",
        "        if not hasattr(self, \"optimizer\") or self.optimizer is None:\n",
        "            self.optimizer = AdamW([{\"params\": [wte.weight], \"lr\": LR, \"weight_decay\": 0.0}],\n",
        "                                   betas=(0.9, 0.999), eps=1e-8)\n",
        "        return self.optimizer\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        out = model(**inputs); loss = out.loss\n",
        "        if num_added and num_added>1 and REPULSION_W>0:\n",
        "            E = model.get_input_embeddings().weight[old_vocab:old_vocab+num_added]\n",
        "            E = E - E.mean(0, keepdim=True); E = E/(E.norm(dim=1, keepdim=True)+1e-8)\n",
        "            sims = (E @ E.t()); repul = (sims - torch.eye(E.shape[0], device=E.device)).pow(2).mean()\n",
        "            loss = loss + REPULSION_W*repul\n",
        "        return (loss, out) if return_outputs else loss\n",
        "    def training_step(self, *args, **kwargs):\n",
        "        out = super().training_step(*args, **kwargs)\n",
        "        clamp_rows_(model.get_input_embeddings(), MAX_ROW_NORM)\n",
        "        return out\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=RUN_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_steps=max(20, MAX_STEPS//20),\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.0,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    ataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "trainer = EmbOnlyTrainer(model=model, args=args, data_collator=coll, train_dataset=train_ds)\n",
        "print(f\"[Run] emb-only | base={MODEL_NAME} | steps={MAX_STEPS} | seq_len={SEQ_LEN} | accum={GRAD_ACCUM} | LR={LR}\")\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "save_dir = os.path.join(RUN_DIR, \"embedding_only\"); os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(wte.weight.detach().cpu(), os.path.join(save_dir, \"embed_tokens.pt\"))\n",
        "with open(os.path.join(save_dir, \"added_tokens.json\"), \"w\") as f:\n",
        "    json.dump({\"added_tokens\": missing, \"old_vocab\": old_vocab, \"num_added\": num_added}, f, indent=2)\n",
        "tok.save_pretrained(RUN_DIR)\n",
        "\n",
        "# Geometry (post)\n",
        "with torch.no_grad():\n",
        "    post_full = wte.weight.detach().clone().cpu()\n",
        "    post_new  = post_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, pre_full.shape[1])\n",
        "\n",
        "report = {\n",
        "  \"model\": MODEL_NAME, \"added_tokens\": int(num_added), \"old_vocab\": int(old_vocab),\n",
        "  \"pip_loss_full\": pip_loss(pre_full, post_full),\n",
        "  \"topk_overlap_all\": topk_overlap(pre_full, post_full, k=10, sample=min(REPORT_SAMPLE, pre_full.shape[0]-1)),\n",
        "  \"isotropy_pre\": isotropy(pre_full), \"isotropy_post\": isotropy(post_full),\n",
        "  \"pip_loss_new_rows\": (pip_loss(pre_new, post_new) if num_added>1 else None),\n",
        "  \"isotropy_new_rows\": (isotropy(post_new) if num_added>1 else None),\n",
        "}\n",
        "json.dump(report, open(os.path.join(RUN_DIR, \"geometry_report.json\"), \"w\"), indent=2)\n",
        "print(\"\\n=== GEOMETRY REPORT ===\")\n",
        "for k,v in report.items(): print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ii-pB2BazrGH",
        "outputId": "60ba46f0-99b8-4433-ac1b-76db34666385"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2823043511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n\u001b[1;32m      4\u001b[0m                           DataCollatorForLanguageModeling, TrainingArguments, Trainer, set_seed)\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# If all above-mentioned conditions are met, preload nvrtc and nvjitlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Please note that order are important for CUDA-11.8 , as nvjitlink does not exist there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0m_preload_cuda_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda_nvrtc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libnvrtc.so.*[0-9]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0m_preload_cuda_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nvjitlink\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"libnvJitLink.so.*[0-9]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_preload_cuda_deps\u001b[0;34m(lib_folder, lib_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{lib_name} not found in the system path {sys.path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
