# wake2vec devlog 2026-02-22

## Llama 3.2-1B P1 resuming from step 1200

Back on account 2.

Come smoke a ciggy with me and listen to Lux: [La Rumba Del Perdón](https://soundcloud.com/rosaliaofficial/la-rumba-del-perdon?in=rosaliaofficial/sets/lux-226901126&si=5ee916dab5a94ead927f83b8e32c7242&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)

### Loss table (continued from DEVLOG_0219)

| Step | Train | Val | Notes |
|------|-------|-----|-------|
| 800 | 70.8957 | 7.7487 | (Feb 19) |
| 1000 | 65.7121 | 7.8129 | (Feb 19) |
| 1200 | 61.0022 | 7.8385 | (Feb 19, session end) |
| 1400 | 78.6625 | 5.3643 | train spike but val dropped hard |
| 1600 | 69.8318 | 5.4101 | val flat, train coming down | 
| 1800 | 64.9033 | 5.4270 | train still dropping, val plateau ~5.4 |

### Notes

- Val loss went from 7.84 → 5.36 between steps 1200 and 1400. 
- Train loss spiked to 78.66 (was 61.0 at 1200) and is noisy on this run because of the lexicon-heavy blocks.

---

## Status across all runs

| Model | Phase | Status | Notes |
|-------|-------|--------|-------|
| TinyLlama 1.1B | P1 | Complete | Loss 8.46 → 0.079 (3000 steps) |
| TinyLlama 1.1B | P2 | Complete | Best ckpt step 1400 (val 0.6393, gap 0.001) |
| TinyLlama 1.1B | P3 | Script ready | `wake2vec_phase_3_morpheme.py`, from step 1400 |
| Llama 3.2-1B | P1 | Running | Step 1400, val 5.36 and dropping |
| Qwen 2.5-14B | P1 | Debugging | WakeOverlay solution in progress (see DEVLOG_0220_QWEN) |
