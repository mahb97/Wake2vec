# wake2vec devlog

## 2026-02-14 Phase 2 LoRA continuing & multi-model plans

### Status: very much WIP

This project is like totally not done lol. Everytime i get closer to completeing, i think of some more bs. 

---

### Current run: TinyLlama 1.1B Phase 2 (LoRA)

Resuming from step 1000, targeting 3000 total.

Setup recap:
- Base: TinyLlama-1.1B-Chat, 4-bit NF4 quantized
- P1 embeddings (44,989 Wake tokens) loaded and **frozen**
- LoRA r=8, alpha=16, dropout=0.1
- Targets: q/k/v projections + gate/up/down MLP
- LR: 2e-5, 10% warmup, cosine schedule
- Effective batch: 16 (8 x 2 grad accum), seq len 256
- Hardware: Colab T4 (free tier)

Phase 1 got training loss from 8.46 down to 0.079 on embedding-only training but of course that also overfit on purpose. 

### Next: multi-model comparison

Work in progress: aim is to compare outputs across architectures on free Colab GPU (T4, 15GB VRAM). The Q here is which model gives the best Wakeish generations. Candidates:

| Model | Params | VRAM fit on T4? | Notes |
|---|---|---|---|
| TinyLlama 1.1B | 1.1B | Yes (current run) | Baseline, already training |
| Llama 3.1-8B | 8B | Tight with 4-bit quant + LoRA | Have embedding scripts already (`wake2vec_on_llama_3_1_8b-7.py`). ~5GB quantized + LoRA overhead. Fits but also slow|
| Mistral 7B | 7B | Similar to Llama 8B | Gonna try with different tokenizer, different attention (GQA + sliding window). Comparable size to Llama 8B so good direct comparison |
| 14B (Qwen 2.5-14B) | 14B | largest to try | Not yet drafted |

Realistic plan: TinyLlama (running now), Llama 3.1-8B (have partial scripts), Mistral 7B (need to set up). *Note to self: run the Llama 3.1-70B one at TU.*

### What's done

- [x] Phase 1: embedding-only fine-tuning on TinyLlama 
- [x] Wake lexicon extraction (44,989 tokens)
- [x] Morpheme analysis (prefix/suffix decomposition)
- [x] Phase 2 training infra (custom callbacks, Drive mirroring, embedding analysis)
- [x] Grassmann flows vs attention experiment (framework ready, not fully run)
- [x] Llama 3.1-8B embedding scripts (partial)


This is maybe 30% done, generously. 
