## 2026-02-15 — Llama-3.2-1B P1 embedding-only, resuming from step 200

i bet you only come here for the music recommendations: [superrich](https://soundcloud.com/livealok/superrich?si=2a808a026c5740388ef2f3739254ec18&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)

Switched to running the Llama today because my other google account won't give me any free GPU access today (henceforth "gaslight GPU"). Running P1 embedding-only on Llama-3.2-1B, picking up from step 200.

Only change today: save frequency 100 → 50 steps. This thing is slow enough that losing 100 steps to a Colab disconnect is genuinely painful.

VRAM: 6.2 / 15.0 GB. System RAM: 7.9 / 12.7 GB.

| step | training loss | validation loss |
| --- | --- | --- |
| 400 | 92.114100	| 7.576191 |

The 92 is almost certainly a logging artifact: batch=1 with no averaging means one lexicon-heavy block can spike the reported loss. The wake_lexicon.txt is concatenated into training data and those bare-token-per-line blocks are basically unpredictable Wake style at this stage. Watching val loss, not panicking just yet.

# New scripts

Next runs after this are:

- **`wake2vec_on_llama_3_2_3b.py`** Llama 3.2-3B. ~5-6GB VRAM, comfortable fit. Same vocab as 1B (128K).
- **`wake2vec_on_llama_3_1_8b.py`** Llama 3.1-8B. Biggest Llama on free T4. ~9-10GB VRAM, tight but doable. `max_memory` capped at 12GB, gradient checkpointing essential. Same vocab (128K).

Both follow the same pattern: 4-bit quant body, spherical Wake init, gradient masking, 50-step saves, sentry mirror, full analysis suite.
