# Wake2Vec Devlog: 2026-02-10

## P1 TinyLlama Training: Steps 1900 to 2600

Training continued from the checkpoint at step 1800 (ran out of storage last time, google is a b****), resuming on a free-tier Colab T4 instance. 
- The model is TinyLlama-1.1B-Chat-v1.0 with 44,990 Wake tokens injected into the embedding layer (total vocabulary: 76,500).
- All transformer parameters remain frozen; only the unified input/output embedding matrix is trainable (76.5M parameters).
- Gradient masking is disabled for this run, meaning both base and Wake token rows receive gradient updates.

### Loss Progression (Steps 1900 to 2600)

| Step | Training Loss | Validation Loss |
|:--|:--|:--|
| 1900 | 0.0985 | 8.2165 |
| 2000 | 0.0650 | 8.2818 |
| 2100 | 0.0435 | 8.3253 |
| 2200 | 0.0377 | 8.3697 |
| 2300 | 0.0339 | 8.4014 |
| 2400 | 0.0293 | 8.4341 |
| 2500 | 0.0264 | 8.4552 |
| 2600 | 0.0245 | 8.4735 |

Training loss continues to decrease monotonically, falling from 0.0985 at step 1900 to 0.0245 at step 2600. Validation loss diverges in the opposite direction, rising steadily from 8.22 to 8.47 over the same interval. Divergence is expected and consistent with the design intent of Phase 1: the embedding layer is being driven toward full memorization of FW lexicon (with the goal being the absorption of Joycean morphology into the embedding space). The model is, by construction, overfitting to the Wake, and treats the resulting embeddings as a learned lexicon rather than a generalizable language representation.

## Embedding Analysis: Revisions for Review Readiness

The post-training embedding analysis pipeline was substantially revised to provide deeper and more detailed insights. The following additions were made.

### 1. Statistical Testing on Norm Distributions

The original analysis reported mean and standard deviation of L2 norms for base and Wake token groups but did not test whether the observed difference was statistically significant. The revised pipeline includes:

- **Welch's t-test** (unequal variance t-test) for comparing group means.
- **Mann-Whitney U test** (non-parametric) as a distribution-free alternative, since embedding norms are not guaranteed to be normally distributed.
- **Cohen's d** as a standardized effect size measure, allowing the magnitude of the difference to be interpreted independently of sample size.

### 2. Eigenvalue-Based Isotropy Score

The original isotropy measure (mean pairwise cosine similarity over a random sample) is a rough proxy at best. Following Mu et al. (2018), the revised analysis computes a partition function isotropy score. For each embedding vector, a partition function Z_i is computed as the sum of exponentiated cosine similarities to all other sampled vectors. The isotropy score is then defined as the ratio min(Z)/max(Z). A score of 1.0 indicates perfect isotropy (uniform angular distribution), while values approaching 0 indicate anisotropy or clustering. This metric is computed separately for the full vocabulary, the base token subset, and the Wake token subset.

### 3. Pre-to-Post Embedding Drift

Because gradient masking is disabled in this run, base token embeddings are free to drift during training. Quantifying this drift is necessary to evaluate whether the base model's representations have been disrupted. The revised pipeline computes per-token cosine similarity between pre-training and post-training embedding vectors for all base tokens. The distribution of these similarities is reported (mean, standard deviation, min, max), and the 20 most-drifted tokens are listed by name (this requires a pre-training embedding snapshot to be saved before training begins).

### 4. Nearest-Neighbor Validation

To provide qualitative evidence that Wake tokens have been positioned meaningfully within the embedding space, the revised pipeline retrieves the five nearest base-vocabulary neighbors (by cosine similarity) for a sample of Wake tokens. This serves as a sanity check: morphologically transparent Wake tokens (e.g., "misunderstord") should land near their standard English counterparts (e.g., "misunderstood"). Tokens drawn from the beginning, middle, and end of the Wake lexicon are sampled to avoid selection bias.

### 5. Intrinsic Dimensionality via PCA

A concern with embedding-only fine-tuning is that new tokens may collapse into a low-dimensional subspace rather than distributing across the full representational capacity of the model. To test for this, PCA is fitted separately to the base and Wake token subsets. The number of principal components required to explain 90% and 95% of the variance is reported for each group. If the Wake tokens require significantly fewer components, this would indicate subspace collapse and poor distributional coverage.

### 6. Pairwise Cosine Similarity Distributions

The revised analysis samples pairwise cosine similarities for three group pairings: (base, base), (Wake, Wake), and (base, Wake). The full distributions are plotted and summarized. A two-sample Kolmogorov-Smirnov test is applied to the (base, base) and (Wake, Wake) distributions to determine whether the angular structure of the Wake tokens differs significantly from that of the base vocabulary.

### 7. Visualization

All six analyses are presented in a single six-panel figure:

1. Norm distributions (base vs. Wake), plotted as overlapping density histograms.
2. Pairwise cosine similarity distributions for all three group pairings.
3. Cumulative PCA explained variance curves for base and Wake tokens.
4. Base token drift histogram (cosine similarity, pre to post).
5. Norm by token index (scatter), showing positional structure.
6. Top-20 eigenvalue spectrum (bar chart) for base and Wake subsets.

All metrics are collected into a structured JSON summary saved alongside the model artifacts.
