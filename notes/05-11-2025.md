# DEVLOG — Wake2Vec
**Date:** 2025-11-05  
**Env:** Colab T4 · TinyLlama-1.1B · transformers 4.57.1 · accelerate 1.2.1

## Summary
Reran morpheme-aware token expansion and restarted Phase-1 (embeddings-only) training on GPU. Datasets were rebuilt with the augmented tokenizer to prevent ID drift. Training is running cleanly with evaluation every 200 steps and frequent checkpoints.

## Run state (live)
- **RUN_ID:** `t4_<epoch>` (see `runs/<RUN_ID>/run_manifest.json`)
- **Progress:** step ~150 (of 1100–1400 target)
- **GPU mem:** ~9.1 / 15.0 GB (comfortable)
- **Loss:** descending steadily; no anomalies

## Configuration (P1)
- `max_steps=1100` (may extend if time permits), `per_device_train_batch_size=1`, `gradient_accumulation_steps=16`
- `learning_rate=5e-4`, `warmup_ratio=0.02`, `optim="adafactor"`
- `MAX_LEN=448`, `gradient_checkpointing=True`, `use_cache=False`, `fp16/bf16=False`
- **Eval:** every 200 steps on small valid shard  
- **Checkpoints:** `save_steps=100`, `save_total_limit=12`
- **Placement:** `device_map="auto"`; lm_head tied to input embeddings post-resize
- **Snapshots:** new-row embedding snapshots every 200 steps (if `new_ids.npy` present)

## Guardrails applied
- Rebuilt tokenized datasets **after** tokenizer augmentation (prevents out-of-range IDs)
- Version pins: `transformers==4.57.1`, `accelerate==1.2.1`, `datasets==2.20.0`
- Accelerate unwrap kwarg-compat shim in place

## Artifacts (paths)
- `runs/<RUN_ID>/run_manifest.json`
- `runs/<RUN_ID>/metrics/phase1_loss_log.json`
- (after finish) `runs/<RUN_ID>/plots/p1_loss_curve.png`, `runs/<RUN_ID>/metrics/p1_summary.json`
- Drive mirrors under `/content/drive/MyDrive/wake2vec/...`
