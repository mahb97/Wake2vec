# Wake2Vec Devlog: 12th of Feb 26

## Phase 2 TinyLlama LoRA: Steps 0 to 1000

Phase 2 training is underway. This run loads the frozen P1 embeddings (from the P1 run, step 3000, no gradient masking) into TinyLlama-1.1B and trains LoRA adapters on all attention projections (q, k, v) and MLP layers (gate, up, down). The embedding matrix is entirely frozen. Only LoRA adapter weights are trainable (5.6M parameters).

### Training Configuration

| Parameter | Value |
|:--|:--|
| Base model | TinyLlama/TinyLlama-1.1B-Chat-v1.0 |
| Quantization | 4-bit NF4 (bitsandbytes) |
| Trainable parameters | 5,586,944 (LoRA only) |
| LoRA rank | 8, alpha 16, dropout 0.1 |
| LoRA targets | q_proj, k_proj, v_proj, gate_proj, up_proj, down_proj |
| Embeddings | Frozen (loaded from P1 step 3000) |
| Optimizer | AdamW (via Trainer) |
| Learning rate | 2e-5 (cosine schedule, 10% warmup) |
| Batch size | 8 (effective 16 via gradient accumulation) |
| Sequence length | 256 |
| Max steps | 3000 |

### Loss Progression (Steps 200 to 800)

| Step | Training Loss | Validation Loss |
|:--|:--|:--|
| 200 | 1.0448 | 0.8386 |
| 400 | 0.7472 | 0.6829 |
| 600 | 0.6585 | 0.6303 |
| 800 | 0.6175 | 0.6172 |
| 1000| 0.551700 | 0.617701 |

### Observations

The loss trajectory is qualitatively different from the P1 embedding-only run. In P1, validation loss diverged from training loss almost immediately and climbed steadily throughout training, consistent with deliberate corpus memorization. In P2, both training and validation losses are decreasing in tandem and have nearly converged by step 800 (train: 0.6175, val: 0.6172). This suggests that the LoRA adapters are learning generalizable patterns from the Wake corpus rather than memorizing it.

The rate of improvement is decelerating and the training loss dropped by 0.30 between steps 200 and 400, by 0.09 between steps 400 and 600, and by 0.04 between steps 600 and 800. This is typical of cosine-scheduled training approaching a plateau. The remaining 2000 steps will likely yield diminishing but nonzero gains.

A notable architectural change from the original P2 pipeline is the addition of k_proj to the LoRA target modules. The original configuration trained only q_proj and v_proj in attention, creating an asymmetry in query-key interaction. Adding k_proj allows the model to reshape attention patterns symmetrically, which is relevant for Wake text where syntactic dependencies differ substantially from standard English.

### Comparison: P1 vs P2 at Step 800

| Metric | P1 (Fry Embeds) | P2 (LoRA) |
|:--|:--|:--|
| Train loss | ~0.55 | 0.6175 |
| Val loss | ~7.40 | 0.6172 |
| Train/val gap | ~6.85 | 0.0003 |
| Trainable params | 156.7M (all embeds) | 5.6M (LoRA only) |
| Approach | Memorization | Generalization |

The train/val gap at step 800 is 0.0003 for P2 compared to approximately 6.85 for P1. This reflects the fundamental difference in training strategy. P1 was designed to overfit the embedding space to the corpus, on the other hand, P2 aims to adapt the model's processing of those embeddings through low-rank attention and MLP modifications.

*this ran for like 3 hours and 10 minutes on free colab GPU before it cut, happy valentines lol*
