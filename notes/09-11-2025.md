# 2025-11-09 (P1 resume from 300)

## recap 
- Reacquired T4 and resumed Phase-1 from `checkpoint-300`.
- Kept saves local (`/content/runs/<RUN_ID>`) and mirrored to Drive via `SentryMirror`.
- Enabled embedding snapshots every 50 steps.
- Used EvalEveryNSteps(200); evals at 400/600, etc.
- Accepted env drift; applied unwrap_model shim (HF accelerate compat). Skipped hard repin to save time.

## Versions (observed)
- `transformers==4.57.1`
- `accelerate==1.11.0` (shim applied to ignore `keep_torch_compile`)
- `datasets==4.0.0`

## Loss / Eval snapshots (today)
| Step | Training Loss | Validation Loss |
|-----:|--------------:|-----------------|
| 350  | 5.488300      | No Log          |
| 400  | 5.308200      | 6.273335        |
| 450  | 4.777600      | No Log          |
| 500  | 4.089100      | No Log          |
| 550  | 3.260400      | No Log          |
| 600  | 2.425900      | 7.170007        |

> Note: T4 cut after 600 on one run; folders 400–700 were created but weights didn’t finish writing (partial). Logs still useful.

## Reliability upgrades (today)
- **Local output_dir** + Drive mirror only on completed saves.
- **Dynamic `save_steps`**: 50 until step 700, then 75→100.
- **Embedding snapshots** every 50 steps (`emb_snaps/<RUN_ID>/emb_step####.pt`) so P1 is recoverable even if a full ckpt is partial.

## Plan 
- **Resume from best valid** → rolling target +300 steps (never exceeding final 1300).
- Keep: `warmup_ratio=0.0`, `optim=adafactor`, `GA=16`, `MAX_LEN=448`, eval every 200.
- Upon reaching 1300: run **P1-finalize** (overlap@5, mean Δ‖E‖, loss plot), archive, then Phase-2 (LoRA).

## Quick notes
- Heartbeat notebook monitors:
  - newest local ckpt’s `trainer_state.json` tail (loss/eval),
  - checkpoint audit (weights/state/opt flags),
  - mirror of latest full checkpoint to `sentry_backups/<RUN_ID>`,
  - embedding snapshot heartbeat.
- If cut mid-save: resume picks highest ckpt with weights; otherwise rebuild from latest embedding snapshot.
