# wake2vec devlog 2026-02-17

"[one day more](https://www.youtube.com/watch?v=Sv-BxH3SVS8&list=RDSv-BxH3SVS8&start_radio=1), another day, another destiny"... (bet you never thought you'd find a les mis quote in a devlog)

## TinyLlama P2 LoRA, really this is the light at the end of the tunnel (not a train, just an embed analysis) 

Gaslight GPU is giving 1hr 50min today. That's enough to finish the last 400 steps (2600 -> 3000) and run the full eval. Let's goooooo.

### Loss table (continued)

| Step | Train | Val | Gap |
|---|---|---|---|
| 1200 | 0.5797 | 0.6255 | 0.046 |
| 1400 | 0.6388 | 0.6393 | 0.001 |
| 1600 | 0.5722 | 0.6460 | 0.074 |
| 1800 | 0.6104 | 0.6594 | 0.049 |
| 2000 | 0.4943 | 0.6679 | 0.174 |
| 2200 | 0.4660 | 0.6736 | 0.208 |
| 2400 | 0.5128 | 0.6791 | 0.166 |
| 2600 | 0.4756 | 0.6817 | 0.206 |
| 2800 | 0.4673 | 0.6823 | 0.215 |
| 3000 | 0.5278 | 0.6827 | 0.155 |

[P2 Loss Curve](assets/p2_loss_curve.png)

Best checkpoint: step 1400 (gap 0.001). Val loss has been plateauing since ~2000 (0.668 -> 0.682 over 600 steps). Overfitting is real but not catastrophic. The eval suite will tell us the full story.

---

## What's in the works 

- **TinyLlama P3 morpheme:** script ready (`wake2vec_phase_3_morpheme.py`), runs after P2 eval from step 1400 checkpoint
