{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpN4e3MxG8jdBEFaCvetrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/letsbuildthisthing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "did some digging and guess what i found, absolute gold, so here comes everybody"
      ],
      "metadata": {
        "id": "twvOaI8RUnBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MORPHEME-AWARE WAKE2VEC**\n",
        "\n",
        "**Teaching TinyLlama Joyce's Generative Grammar**\n",
        "\n",
        "Based on hand-compiled morphological analysis of Finnegans Wake\n",
        "\n",
        "This notebook teaches compositional word formation via embedding arithmetic"
      ],
      "metadata": {
        "id": "lthZp2xlU6BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"1337\"\n",
        "\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.allow_tf32 = False\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "import random, json, math, re\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"Morpheme chaos mode activated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpvYNSFoU5k7",
        "outputId": "82f601a8-b6f6-4daf-8b9d-b463da030ac3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morpheme chaos mode activated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "CTO4KqfqWTfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj0xrjyMUi7Z"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "CORPUS_PATH = \"/content/drive/MyDrive/Wake2vec_runs/fw.txt\"\n",
        "MORPHEME_DATA_PATH = \"/content/drive/MyDrive/Wake2vec_runs/morpheme_data.txt\"\n",
        "\n",
        "# Training params\n",
        "BATCH_SIZE = 2\n",
        "BLOCK_SIZE = 256\n",
        "EPOCHS = 2\n",
        "LR = 2e-5\n",
        "WARMUP_RATIO = 0.05\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_ACCUM = 4\n",
        "SAVE_STEPS = 200\n",
        "\n",
        "# Morpheme chaos params\n",
        "SYNTHETIC_PER_MORPHEME = 10  # Generate N examples per morpheme combo\n",
        "COMPOSITION_ALPHA = 0.33     # Weight for prefix:root:suffix (0.33:0.34:0.33)\n",
        "MORPHEME_NOISE = 0.05        # Add chaos to composed embeddings\n",
        "\n",
        "# Output\n",
        "RUN_ID = datetime.now().strftime(\"morpheme_wake_%Y%m%d_%H%M\")\n",
        "OUTDIR = Path(f\"./runs/{RUN_ID}\")\n",
        "(OUTDIR / \"results\").mkdir(parents=True, exist_ok=True)\n",
        "(OUTDIR / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Teaching TinyLlama Joyce's morphological grammar...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus(path):\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Corpus not found: {p}\")\n",
        "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    print(f\"✓ Loaded corpus: {len(text)} chars\")\n",
        "    return text\n",
        "\n",
        "FW_TEXT = load_corpus(CORPUS_PATH)"
      ],
      "metadata": {
        "id": "MxULlIp5Xl6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parse morpheme data"
      ],
      "metadata": {
        "id": "ynxUJIYnXyVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_morpheme_document(text):\n",
        "    \"\"\"\n",
        "    Parse your hand-compiled morpheme analysis into structured data.\n",
        "    Returns: {prefixes: {}, suffixes: {}, examples: {}}\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'prefixes': defaultdict(list),\n",
        "        'suffixes': defaultdict(list),\n",
        "        'infixes': defaultdict(list),\n",
        "        'prefix_counts': Counter(),\n",
        "        'suffix_counts': Counter(),\n",
        "    }\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    current_type = None\n",
        "    current_morph = None\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Detect section headers\n",
        "        if line.startswith('Prefix') or line.startswith('prefix'):\n",
        "            current_type = 'prefix'\n",
        "            # Extract morpheme: \"Prefix ab- 13\" -> \"ab-\"\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                current_morph = parts[1].lower()\n",
        "                count = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 1\n",
        "                data['prefix_counts'][current_morph] = count\n",
        "\n",
        "        elif line.startswith('Suffix') or line.startswith('suffix'):\n",
        "            current_type = 'suffix'\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                current_morph = parts[1].lower()\n",
        "                count = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 1\n",
        "                data['suffix_counts'][current_morph] = count\n",
        "\n",
        "        elif line.startswith('Infix') or line.startswith('infix'):\n",
        "            current_type = 'infix'\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                current_morph = parts[1].lower()\n",
        "\n",
        "        # Collect examples (lines that aren't headers)\n",
        "        elif current_type and current_morph:\n",
        "            # Skip lines with numbers only or special headers\n",
        "            if line[0].isupper() and current_type in ['prefix', 'suffix']:\n",
        "                continue\n",
        "\n",
        "            # Clean example words\n",
        "            word = line.split()[0] if line.split() else line\n",
        "            word = word.strip('.,;:()[]{}\\\"\\'')\n",
        "\n",
        "            if word and len(word) > 1:\n",
        "                if current_type == 'prefix':\n",
        "                    data['prefixes'][current_morph].append(word)\n",
        "                elif current_type == 'suffix':\n",
        "                    data['suffixes'][current_morph].append(word)\n",
        "                elif current_type == 'infix':\n",
        "                    data['infixes'][current_morph].append(word)\n",
        "\n",
        "    # Convert defaultdicts to regular dicts\n",
        "    data['prefixes'] = dict(data['prefixes'])\n",
        "    data['suffixes'] = dict(data['suffixes'])\n",
        "    data['infixes'] = dict(data['infixes'])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load and parse your morpheme data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PARSING HAND-COMPILED MORPHEME DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "morpheme_doc = Path(MORPHEME_DATA_PATH)\n",
        "if morpheme_doc.exists():\n",
        "    morpheme_text = morpheme_doc.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    MORPHEME_DATA = parse_morpheme_document(morpheme_text)\n",
        "else:\n",
        "    # Fallback: extract from the text you pasted inline\n",
        "    print(\"⚠ Morpheme data file not found, using inline extraction...\")\n",
        "    # You can paste your dataset here as a string if needed\n",
        "    MORPHEME_DATA = {\n",
        "        'prefixes': {\n",
        "            'ab-': ['above', 'abaft', 'abject', 'abler'],\n",
        "            'anti-': ['anticipation', 'antipathies'],\n",
        "            'circum-': ['circumvallator'],\n",
        "            'hyper-': ['hyperchemical'],\n",
        "            'sub-': ['subject', 'substrate', 'subordinating'],\n",
        "        },\n",
        "        'suffixes': {\n",
        "            '-ation': ['acclammitation', 'anticipation', 'paupulation'],\n",
        "            '-ous': ['delicious', 'precious', 'gracious'],\n",
        "            '-ness': ['darkness', 'sweetness', 'softness'],\n",
        "            '-ing': ['going', 'coming', 'being'],\n",
        "        },\n",
        "        'prefix_counts': Counter({'ab-': 13, 'anti-': 2, 'circum-': 1, 'hyper-': 1, 'sub-': 7}),\n",
        "        'suffix_counts': Counter({'-ation': 38, '-ous': 49, '-ness': 28, '-ing': 257}),\n",
        "    }\n",
        "\n",
        "print(f\"✓ Parsed {len(MORPHEME_DATA['prefixes'])} prefixes\")\n",
        "print(f\"✓ Parsed {len(MORPHEME_DATA['suffixes'])} suffixes\")\n",
        "print(f\"\\nTop prefixes by frequency:\")\n",
        "for morph, count in MORPHEME_DATA['prefix_counts'].most_common(10):\n",
        "    print(f\"  {morph}: {count}\")\n",
        "print(f\"\\nTop suffixes by frequency:\")\n",
        "for morph, count in MORPHEME_DATA['suffix_counts'].most_common(10):\n",
        "    print(f\"  {morph}: {count}\")"
      ],
      "metadata": {
        "id": "o5rPEZ76Xqt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " LOAD MODEL & TOKENIZER"
      ],
      "metadata": {
        "id": "-2Vr8nAKYDqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel: {MODEL_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Initial vocab: {len(tok)}\")"
      ],
      "metadata": {
        "id": "SKlnV6e-YErr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, csv\n",
        "from pathlib import Path\n",
        "\n",
        "AFFIX_TXT = Path(\"/content/affixes_terms.txt\")\n",
        "\n",
        "EXAMPLES_PER_MORPHEME = 120\n",
        "\n",
        "def _clean_line(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\b\\d+\\b$\", \"\", s).strip()                 # drop trailing counts\n",
        "    s = re.sub(r\"\\((sic|decap)\\)\", \"\", s, flags=re.I).strip()\n",
        "    return s\n",
        "\n",
        "def _normalise_affix(kind: str, aff: str) -> str:\n",
        "    aff = (aff or \"\").replace(\"–\",\"-\").strip()\n",
        "    if not aff: return \"\"\n",
        "    if kind == \"prefix\":\n",
        "        return aff if aff.endswith(\"-\") else (aff + \"-\")\n",
        "    if kind == \"suffix\":\n",
        "        return aff if aff.startswith(\"-\") else (\"-\" + aff)\n",
        "    return aff\n",
        "\n",
        "def _parse_docx(path: Path):\n",
        "    from docx import Document\n",
        "    doc = Document(str(path))\n",
        "    cat = None\n",
        "    rows = []\n",
        "    cat_re = re.compile(r\"^(prefix|suffix)\\s*[-–]?\\s*([A-Za-z()'\\.]+)\", re.I)\n",
        "\n",
        "    for p in doc.paragraphs:\n",
        "        line = (p.text or \"\").strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        m = cat_re.match(line)\n",
        "        if m:\n",
        "            kind = m.group(1).lower()\n",
        "            aff  = _normalise_affix(kind, m.group(2))\n",
        "            cat = (kind, aff)\n",
        "            continue\n",
        "        # skip alpha dividers like \"A\", \"B\"\n",
        "        if len(line) <= 2 and line.isalpha():\n",
        "            continue\n",
        "\n",
        "        line = _clean_line(line)\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # split very conservatively on commas / tabs / big whitespace\n",
        "        for chunk in re.split(r\"[,\\t]+|\\s{2,}\", line):\n",
        "            w = _clean_line(chunk)\n",
        "            if not w or not re.search(r\"[A-Za-z]\", w):\n",
        "                continue\n",
        "            rec = {\"term\": w, \"prefix\": \"\", \"suffix\": \"\"}\n",
        "            if cat:\n",
        "                kind, aff = cat\n",
        "                rec[kind] = aff\n",
        "            rows.append(rec)\n",
        "\n",
        "    # merge duplicates, prefer rows that carry a prefix/suffix label\n",
        "    merged = {}\n",
        "    for r in rows:\n",
        "        t = r[\"term\"]\n",
        "        if t not in merged:\n",
        "            merged[t] = r\n",
        "        else:\n",
        "            if r.get(\"prefix\") and not merged[t].get(\"prefix\"):\n",
        "                merged[t][\"prefix\"] = r[\"prefix\"]\n",
        "            if r.get(\"suffix\") and not merged[t].get(\"suffix\"):\n",
        "                merged[t][\"suffix\"] = r[\"suffix\"]\n",
        "    return list(merged.values())\n",
        "\n",
        "def _parse_csv(path: Path):\n",
        "    rows = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        rdr = csv.DictReader(f)\n",
        "        for r in rdr:\n",
        "            t = _clean_line(r.get(\"term\",\"\"))\n",
        "            if not t: continue\n",
        "            pref = _normalise_affix(\"prefix\", r.get(\"prefix\",\"\"))\n",
        "            suff = _normalise_affix(\"suffix\", r.get(\"suffix\",\"\"))\n",
        "            rows.append({\"term\": t, \"prefix\": pref, \"suffix\": suff})\n",
        "    # de-dupe similar to docx\n",
        "    merged = {}\n",
        "    for r in rows:\n",
        "        t = r[\"term\"]\n",
        "        if t not in merged:\n",
        "            merged[t] = r\n",
        "        else:\n",
        "            if r.get(\"prefix\") and not merged[t].get(\"prefix\"):\n",
        "                merged[t][\"prefix\"] = r[\"prefix\"]\n",
        "            if r.get(\"suffix\") and not merged[t].get(\"suffix\"):\n",
        "                merged[t][\"suffix\"] = r[\"suffix\"]\n",
        "    return list(merged.values())\n",
        "\n",
        "# 1) Load annotations\n",
        "if AFFIX_CSV.exists():\n",
        "    ann = _parse_csv(AFFIX_CSV)\n",
        "elif AFFIX_DOCX.exists():\n",
        "    try:\n",
        "        ann = _parse_docx(AFFIX_DOCX)\n",
        "    except Exception as e:\n",
        "        # fallback if python-docx isn't installed\n",
        "        raise RuntimeError(\"python-docx is required to read the DOCX. Install it or provide CSV.\") from e\n",
        "else:\n",
        "    raise FileNotFoundError(\"Provide either AFFIX_CSV or AFFIX_DOCX\")\n",
        "\n",
        "# 2) Build maps: morpheme -> list[word]\n",
        "from collections import defaultdict\n",
        "\n",
        "prefix_map = defaultdict(list)\n",
        "suffix_map = defaultdict(list)\n",
        "\n",
        "for r in ann:\n",
        "    w = r[\"term\"].strip()\n",
        "    p = r.get(\"prefix\",\"\").strip()\n",
        "    s = r.get(\"suffix\",\"\").strip()\n",
        "    if p:\n",
        "        prefix_map[p].append(w)\n",
        "    if s:\n",
        "        suffix_map[s].append(w)\n",
        "\n",
        "# 3) Deduplicate & cap examples\n",
        "def _dedupe_cap(d):\n",
        "    out = {}\n",
        "    for k, lst in d.items():\n",
        "        seen = set()\n",
        "        uniq = []\n",
        "        for w in lst:\n",
        "            wl = w.lower()\n",
        "            if wl not in seen:\n",
        "                seen.add(wl)\n",
        "                uniq.append(w)\n",
        "            if len(uniq) >= EXAMPLES_PER_MORPHEME:\n",
        "                break\n",
        "        out[k] = uniq\n",
        "    return out\n",
        "\n",
        "prefix_map = _dedupe_cap(prefix_map)\n",
        "suffix_map = _dedupe_cap(suffix_map)\n",
        "\n",
        "# 4) Final structure for your functions\n",
        "MORPHEME_DATA = {\n",
        "    \"prefixes\": dict(prefix_map),\n",
        "    \"suffixes\": dict(suffix_map),\n",
        "}\n",
        "\n",
        "# 5) Nice preview\n",
        "def _peek(d, n=5):\n",
        "    return {k: d[k][:min(len(d[k]),3)] for k in list(d.keys())[:n]}\n",
        "\n",
        "print(f\"[MORPHEME_DATA] prefixes={len(MORPHEME_DATA['prefixes'])} | suffixes={len(MORPHEME_DATA['suffixes'])}\")\n",
        "print(\"  sample prefixes:\", _peek(MORPHEME_DATA[\"prefixes\"]))\n",
        "print(\"  sample suffixes:\", _peek(MORPHEME_DATA[\"suffixes\"]))"
      ],
      "metadata": {
        "id": "n-RJHYHFb5ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_morpheme_embedding(morpheme, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Find or approximate embedding for a morpheme.\n",
        "    Strategy:\n",
        "    1. Check if morpheme exists as single token\n",
        "    2. Average embeddings of example words containing it\n",
        "    3. Fall back to subtoken average\n",
        "    \"\"\"\n",
        "    emb_matrix = model.get_input_embeddings().weight.data\n",
        "\n",
        "    # Strategy 1: Direct lookup\n",
        "    morph_clean = morpheme.strip('-')\n",
        "    tid = tokenizer.convert_tokens_to_ids(morph_clean)\n",
        "    if tid != tokenizer.unk_token_id:\n",
        "        return emb_matrix[tid].clone()\n",
        "\n",
        "    # Strategy 2: Average from example words\n",
        "    if morpheme in MORPHEME_DATA['prefixes']:\n",
        "        examples = MORPHEME_DATA['prefixes'][morpheme][:5]\n",
        "    elif morpheme in MORPHEME_DATA['suffixes']:\n",
        "        examples = MORPHEME_DATA['suffixes'][morpheme][:5]\n",
        "    else:\n",
        "        examples = []\n",
        "\n",
        "    if examples:\n",
        "        valid_embs = []\n",
        "        for word in examples:\n",
        "            wid = tokenizer.convert_tokens_to_ids(word.lower())\n",
        "            if wid != tokenizer.unk_token_id:\n",
        "                valid_embs.append(emb_matrix[wid])\n",
        "\n",
        "        if valid_embs:\n",
        "            return torch.stack(valid_embs).mean(dim=0)\n",
        "\n",
        "    # Strategy 3: Random init based on similar morphemes\n",
        "    return torch.randn(emb_matrix.shape[1], device=emb_matrix.device) * emb_matrix.std()\n",
        "\n",
        "def compose_morpheme_embedding(prefix, root, suffix, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Create embedding via composition: E(word) = α*E(prefix) + β*E(root) + γ*E(suffix)\n",
        "    \"\"\"\n",
        "    alpha, beta, gamma = COMPOSITION_ALPHA, 1 - 2*COMPOSITION_ALPHA, COMPOSITION_ALPHA\n",
        "\n",
        "    # Get component embeddings\n",
        "    prefix_emb = find_morpheme_embedding(prefix, model, tokenizer) if prefix else 0\n",
        "    suffix_emb = find_morpheme_embedding(suffix, model, tokenizer) if suffix else 0\n",
        "\n",
        "    # Root embedding\n",
        "    root_id = tokenizer.convert_tokens_to_ids(root.lower())\n",
        "    if root_id != tokenizer.unk_token_id:\n",
        "        root_emb = model.get_input_embeddings().weight.data[root_id]\n",
        "    else:\n",
        "        root_emb = torch.randn(model.get_input_embeddings().weight.shape[1], device=DEVICE) * 0.02\n",
        "\n",
        "    # Compose\n",
        "    composed = alpha * prefix_emb + beta * root_emb + gamma * suffix_emb\n",
        "\n",
        "    # Add morpheme noise for diversity\n",
        "    noise = torch.randn_like(composed) * MORPHEME_NOISE * composed.std()\n",
        "    composed = composed + noise\n",
        "\n",
        "    return composed"
      ],
      "metadata": {
        "id": "CJ2cMYY2YJXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gen synthetic wake words via morpheme combination"
      ],
      "metadata": {
        "id": "Y7uM8lLfYWSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_morpheme_words(n_samples=1000):\n",
        "    \"\"\"\n",
        "    Generate synthetic Wake words by combining morphemes.\n",
        "    Uses frequency weights to match style.\n",
        "    \"\"\"\n",
        "    generated_words = []\n",
        "\n",
        "    # Extract morpheme lists\n",
        "    prefixes = list(MORPHEME_DATA['prefix_counts'].keys())\n",
        "    suffixes = list(MORPHEME_DATA['suffix_counts'].keys())\n",
        "\n",
        "    # Weight by frequency\n",
        "    prefix_weights = [MORPHEME_DATA['prefix_counts'][p] for p in prefixes]\n",
        "    suffix_weights = [MORPHEME_DATA['suffix_counts'][s] for s in suffixes]\n",
        "\n",
        "    # Common roots from Wake\n",
        "    roots = [\n",
        "        'dream', 'river', 'thunder', 'word', 'night', 'day', 'wake', 'sleep',\n",
        "        'fire', 'water', 'time', 'man', 'woman', 'king', 'queen', 'stone',\n",
        "        'tree', 'moon', 'sun', 'star', 'wind', 'rain', 'storm', 'cloud',\n",
        "        'book', 'letter', 'voice', 'sound', 'song', 'dance', 'walk', 'run'\n",
        "    ]\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        # Sample with frequency weighting\n",
        "        prefix = random.choices(prefixes, weights=prefix_weights)[0] if random.random() < 0.7 else None\n",
        "        root = random.choice(roots)\n",
        "        suffix = random.choices(suffixes, weights=suffix_weights)[0] if random.random() < 0.8 else None\n",
        "\n",
        "        # Construct word\n",
        "        if prefix and suffix:\n",
        "            word = f\"{prefix.strip('-')}{root}{suffix.strip('-')}\"\n",
        "        elif prefix:\n",
        "            word = f\"{prefix.strip('-')}{root}\"\n",
        "        elif suffix:\n",
        "            word = f\"{root}{suffix.strip('-')}\"\n",
        "        else:\n",
        "            word = root\n",
        "\n",
        "        generated_words.append({\n",
        "            'word': word,\n",
        "            'prefix': prefix,\n",
        "            'root': root,\n",
        "            'suffix': suffix\n",
        "        })\n",
        "\n",
        "    return generated_words\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING SYNTHETIC WAKE WORDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "synthetic_words = generate_morpheme_words(n_samples=500)\n",
        "print(f\"✓ Generated {len(synthetic_words)} morphological neologisms\")\n",
        "print(\"\\nExamples:\")\n",
        "for w in synthetic_words[:20]:\n",
        "    print(f\"  {w['word']:20s} ({w['prefix'] or 'Ø'} + {w['root']} + {w['suffix'] or 'Ø'})\")"
      ],
      "metadata": {
        "id": "LOOK4sgZYkq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "token injection"
      ],
      "metadata": {
        "id": "LynN1U3AcYk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_morpheme_tokens(word_data, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Inject tokens with morpheme-aware compositional initialization.\n",
        "    \"\"\"\n",
        "    to_add = []\n",
        "    compositions = []  # Track (word, prefix, root, suffix) for init\n",
        "\n",
        "    for item in word_data:\n",
        "        word = item['word']\n",
        "        # Add both regular and ▁ prefixed versions\n",
        "        variants = [word, f\"▁{word}\"]\n",
        "\n",
        "        for form in variants:\n",
        "            tid = tokenizer.convert_tokens_to_ids(form)\n",
        "            if tid == tokenizer.unk_token_id:\n",
        "                to_add.append(form)\n",
        "                compositions.append(item)\n",
        "\n",
        "    if not to_add:\n",
        "        print(\"⚠ No new tokens to add\")\n",
        "        return []\n",
        "\n",
        "    old_size = model.get_input_embeddings().num_embeddings\n",
        "    n_added = tokenizer.add_tokens(to_add, special_tokens=False)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    print(f\"✓ Injected {n_added} morpheme-composed tokens\")\n",
        "\n",
        "    # Initialize with compositional embeddings\n",
        "    emb = model.get_input_embeddings().weight.data\n",
        "\n",
        "    for i, item in enumerate(compositions):\n",
        "        token_idx = old_size + i\n",
        "        if token_idx < len(tokenizer):\n",
        "            composed_emb = compose_morpheme_embedding(\n",
        "                item['prefix'],\n",
        "                item['root'],\n",
        "                item['suffix'],\n",
        "                model,\n",
        "                tokenizer\n",
        "            )\n",
        "            emb[token_idx] = composed_emb\n",
        "\n",
        "    # Tie weights\n",
        "    if hasattr(model, \"tie_weights\"):\n",
        "        model.tie_weights()\n",
        "\n",
        "    print(f\"✓ Initialized embeddings via morpheme composition\")\n",
        "    print(f\"  Formula: E(word) = {COMPOSITION_ALPHA}*E(prefix) + {1-2*COMPOSITION_ALPHA}*E(root) + {COMPOSITION_ALPHA}*E(suffix)\")\n",
        "\n",
        "    return to_add\n",
        "\n",
        "added_tokens = inject_morpheme_tokens(synthetic_words[:200], model, tok)\n",
        "print(f\"✓ Vocabulary expanded: {len(tok)}\")"
      ],
      "metadata": {
        "id": "86B1swlJcbJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SYNTHETIC TRAINING DATA GENERATION"
      ],
      "metadata": {
        "id": "R_mcGbOecgsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_morpheme_sentences(word_data, per_word=5):\n",
        "    \"\"\"\n",
        "    Generate training sentences that showcase morphological patterns.\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        \"The {word} of {root} echoes through the Wake.\",\n",
        "        \"By {word} and by {root}, the river flows.\",\n",
        "        \"In the {word} of night, {root} speaks.\",\n",
        "        \"From {root} to {word}, the tale unwinds.\",\n",
        "        \"He spoke of {word} as if {root} remembered.\",\n",
        "        \"{word} upon {word}, the {root} multiplies.\",\n",
        "        \"Through {word} and beyond {root}, voices drift.\",\n",
        "        \"The {word} contains the {root} contains the word.\",\n",
        "        \"Call it {word}, call it {root}-become-language.\",\n",
        "        \"Riverrun past {word} and {root} from swerve of shore.\",\n",
        "    ]\n",
        "\n",
        "    sentences = []\n",
        "    for item in word_data:\n",
        "        for _ in range(per_word):\n",
        "            pattern = random.choice(patterns)\n",
        "            sentence = pattern.format(word=item['word'], root=item['root'])\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "synthetic_sentences = generate_morpheme_sentences(synthetic_words[:200], per_word=SYNTHETIC_PER_MORPHEME)\n",
        "random.shuffle(synthetic_sentences)\n",
        "\n",
        "print(f\"\\n✓ Generated {len(synthetic_sentences)} training sentences\")\n",
        "print(\"\\nSample sentences:\")\n",
        "for s in synthetic_sentences[:5]:\n",
        "    print(f\"  {s}\")\n",
        "\n",
        "# Combine with original Wake text\n",
        "COMBINED_TEXT = FW_TEXT + \"\\n\" + \"\\n\".join(synthetic_sentences)\n",
        "print(f\"\\n✓ Combined corpus: {len(COMBINED_TEXT)} chars\")\n",
        "\n",
        "# Save generated words for analysis\n",
        "with open(OUTDIR / \"results\" / \"generated_morpheme_words.json\", \"w\") as f:\n",
        "    json.dump(synthetic_words, f, indent=2)"
      ],
      "metadata": {
        "id": "6blAat0XcjiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data set prep"
      ],
      "metadata": {
        "id": "S-3-8Jx7cpAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def create_blocks(text, tokenizer, block_size):\n",
        "    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False)[\"input_ids\"]\n",
        "    n_blocks = len(ids) // block_size\n",
        "    if n_blocks == 0:\n",
        "        raise ValueError(f\"Text too short for block_size={block_size}\")\n",
        "    ids = ids[:n_blocks * block_size]\n",
        "    arr = np.array(ids, dtype=np.int32).reshape(n_blocks, block_size)\n",
        "    return Dataset.from_dict({\"input_ids\": arr.tolist()})\n",
        "\n",
        "ds = create_blocks(COMBINED_TEXT, tok, BLOCK_SIZE)\n",
        "print(f\"✓ Created {len(ds)} blocks\")\n",
        "\n",
        "ds = ds.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n",
        "\n",
        "n = len(ds)\n",
        "if n > 20:\n",
        "    split_idx = int(n * 0.9)\n",
        "    train_ds = ds.select(range(split_idx))\n",
        "    valid = ds.select(range(split_idx, n))\n",
        "else:\n",
        "    train_ds = ds\n",
        "    valid = None\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Val: {len(valid) if valid else 0}\")"
      ],
      "metadata": {
        "id": "8ef4mivAcrMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training args"
      ],
      "metadata": {
        "id": "k5fSrL-2cyBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import inspect\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "has_eval = valid is not None and len(valid) > 0\n",
        "\n",
        "# Check supported args for version compatibility\n",
        "sig = inspect.signature(TrainingArguments.__init__)\n",
        "supported = sig.parameters.keys()\n",
        "\n",
        "args_dict = {\n",
        "    \"output_dir\": str(OUTDIR / \"checkpoints\"),\n",
        "    \"num_train_epochs\": EPOCHS,\n",
        "    \"per_device_train_batch_size\": BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": GRAD_ACCUM,\n",
        "    \"learning_rate\": LR,\n",
        "    \"weight_decay\": WEIGHT_DECAY,\n",
        "    \"warmup_ratio\": WARMUP_RATIO,\n",
        "    \"logging_steps\": 20,\n",
        "    \"fp16\": False,\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "\n",
        "if \"save_strategy\" in supported:\n",
        "    args_dict.update({\"save_strategy\": \"steps\", \"save_steps\": SAVE_STEPS, \"save_total_limit\": 2})\n",
        "else:\n",
        "    args_dict[\"save_steps\"] = SAVE_STEPS\n",
        "\n",
        "if \"evaluation_strategy\" in supported:\n",
        "    args_dict[\"evaluation_strategy\"] = \"steps\" if has_eval else \"no\"\n",
        "    if has_eval: args_dict[\"eval_steps\"] = SAVE_STEPS\n",
        "elif \"evaluate_during_training\" in supported:\n",
        "    args_dict[\"evaluate_during_training\"] = has_eval\n",
        "    if has_eval and \"eval_steps\" in supported: args_dict[\"eval_steps\"] = SAVE_STEPS\n",
        "\n",
        "if \"report_to\" in supported: args_dict[\"report_to\"] = [\"none\"]\n",
        "if \"bf16\" in supported: args_dict[\"bf16\"] = False\n",
        "if \"remove_unused_columns\" in supported: args_dict[\"remove_unused_columns\"] = False\n",
        "if \"lr_scheduler_type\" in supported: args_dict[\"lr_scheduler_type\"] = \"cosine\"\n",
        "if \"per_device_eval_batch_size\" in supported: args_dict[\"per_device_eval_batch_size\"] = BATCH_SIZE\n",
        "\n",
        "training_args = TrainingArguments(**args_dict)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collator,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid if has_eval else None,\n",
        ")\n",
        "\n",
        "print(f\"Trainer ready\")"
      ],
      "metadata": {
        "id": "lb-eG5CJc2K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pre train snap shot"
      ],
      "metadata": {
        "id": "wkpy6AxEc_2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_snapshot(words, model, tokenizer, name=\"snapshot\"):\n",
        "    model.eval()\n",
        "    emb_matrix = model.get_input_embeddings().weight.data\n",
        "    emb_norm = emb_matrix / emb_matrix.norm(dim=1, keepdim=True)\n",
        "\n",
        "    snapshot = {\"name\": name, \"vocab_size\": len(tokenizer), \"words\": {}}\n",
        "\n",
        "    for word_item in words[:50]:\n",
        "        word = word_item['word']\n",
        "        tid = tokenizer.convert_tokens_to_ids(word)\n",
        "        if tid == tokenizer.unk_token_id:\n",
        "            continue\n",
        "\n",
        "        word_emb_norm = emb_norm[tid]\n",
        "        sims = torch.matmul(word_emb_norm.unsqueeze(0), emb_norm.T)[0]\n",
        "        top_k = torch.topk(sims, 11)\n",
        "\n",
        "        neighbors = []\n",
        "        for idx, sim in zip(top_k.indices[1:], top_k.values[1:]):\n",
        "            neighbors.append({\n",
        "                \"token\": tokenizer.convert_ids_to_tokens(idx.item()),\n",
        "                \"sim\": round(sim.item(), 4)\n",
        "            })\n",
        "\n",
        "        snapshot[\"words\"][word] = {\n",
        "            \"token_id\": tid,\n",
        "            \"composition\": f\"{word_item['prefix'] or 'Ø'}+{word_item['root']}+{word_item['suffix'] or 'Ø'}\",\n",
        "            \"embedding_norm\": round(emb_matrix[tid].norm().item(), 4),\n",
        "            \"top_neighbors\": neighbors[:10]\n",
        "        }\n",
        "\n",
        "    return snapshot\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRE-TRAINING SNAPSHOT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pre_snapshot = get_embedding_snapshot(synthetic_words, model, tok, \"pre_morpheme\")\n",
        "with open(OUTDIR / \"results\" / \"pre_morpheme_snapshot.json\", \"w\") as f:\n",
        "    json.dump(pre_snapshot, f, indent=2)\n",
        "\n",
        "print(f\"✓ Pre-training snapshot: {len(pre_snapshot['words'])} words\")"
      ],
      "metadata": {
        "id": "NDzkjKJAc89q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "6NFvO-6TdKP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING: TEACHING JOYCE'S MORPHOLOGICAL GRAMMAR\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final loss: {result.metrics.get('train_loss', 'N/A'):.4f}\")"
      ],
      "metadata": {
        "id": "0Z_g5LzjdJMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "post training analysis"
      ],
      "metadata": {
        "id": "uZg-o1-uda7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"POST-TRAINING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "post_snapshot = get_embedding_snapshot(synthetic_words, model, tok, \"post_morpheme\")\n",
        "with open(OUTDIR / \"results\" / \"post_morpheme_snapshot.json\", \"w\") as f:\n",
        "    json.dump(post_snapshot, f, indent=2)\n",
        "\n",
        "# Compare\n",
        "comparison = {}\n",
        "for word in list(pre_snapshot[\"words\"].keys())[:10]:\n",
        "    if word not in post_snapshot[\"words\"]:\n",
        "        continue\n",
        "\n",
        "    pre = pre_snapshot[\"words\"][word]\n",
        "    post = post_snapshot[\"words\"][word]\n",
        "\n",
        "    pre_neighbors = {n[\"token\"] for n in pre[\"top_neighbors\"][:5]}\n",
        "    post_neighbors = {n[\"token\"] for n in post[\"top_neighbors\"][:5]}\n",
        "    overlap = len(pre_neighbors & post_neighbors)\n",
        "\n",
        "    comparison[word] = {\n",
        "        \"composition\": pre[\"composition\"],\n",
        "        \"norm_change\": post[\"embedding_norm\"] - pre[\"embedding_norm\"],\n",
        "        \"neighbor_overlap\": overlap,\n",
        "        \"pre_top5\": [n[\"token\"] for n in pre[\"top_neighbors\"][:5]],\n",
        "        \"post_top5\": [n[\"token\"] for n in post[\"top_neighbors\"][:5]]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{word} ({pre['composition']}):\")\n",
        "    print(f\"  Norm: {pre['embedding_norm']:.4f} → {post['embedding_norm']:.4f}\")\n",
        "    print(f\"  Overlap: {overlap}/5\")\n",
        "    print(f\"  Before: {', '.join(comparison[word]['pre_top5'])}\")\n",
        "    print(f\"  After:  {', '.join(comparison[word]['post_top5'])}\")\n",
        "\n",
        "with open(OUTDIR / \"results\" / \"morpheme_comparison.json\", \"w\") as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "# Save final model\n",
        "final_dir = OUTDIR / \"final_morpheme_model\"\n",
        "model.save_pretrained(final_dir)\n",
        "tok.save_pretrained(final_dir)\n",
        "\n",
        "print(f\"\\n Model saved: {final_dir}\")\n",
        "print(f\" Results: {OUTDIR / 'results'}\")\n",
        "print(\"\\n maybe it learned a thing or two\")"
      ],
      "metadata": {
        "id": "_S0ez3NTdNoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}