{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/token_injection_and_training_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89163fd",
      "metadata": {
        "id": "d89163fd"
      },
      "source": [
        "# Wake2vec · Token Injection & Training (Clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03b8d1d",
      "metadata": {
        "id": "e03b8d1d"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Wake2Vec_tokens_and_training.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1EkxHeUONZGHnxcVNV3dqknw4vxV8FC5J\n",
        "\n",
        "# Wake2vec · Token Injection & Training\n",
        "\n",
        "**Goal.** Augments the tokenizer with a Joyce-heavy lexicon, smart-initialises new embeddings, and trains (embeddings-only, optionally with a tiny LoRA) on Wake-dense windows. I finish with geometry + behavior metrics and a portfolio-ready hero figure.\n",
        "\n",
        "---\n",
        "\n",
        "## What this notebook does\n",
        "1. **Load base model & tokenizer** (TinyLlama on GPU, or `distilgpt2` on CPU for smoke tests).\n",
        "2. **Inject tokens** from `wake_lexicon.txt`:\n",
        "   - Adds bare forms and start-of-word variants (`▁token`) for SentencePiece-style tokenization.\n",
        "   - Ties the output head (`lm_head`) to the input embeddings after every resize.\n",
        "3. Smart init new vectors as the mean of base subword embeddings + small noise.\n",
        "4. **Dataset**: build Wake-dense paragraph windows (sliding radius) for focused training.\n",
        "5. **Train**:\n",
        "   - Freeze almost everything; **embeddings-only** are trainable.\n",
        "   - Optionally enable a tiny LoRA on `q/k/v/o` (e.g., `r=8`) to let attention “feel” new tokens.\n",
        "   - Cosine LR + warmup, gradient clipping, early stopping on validation loss.\n",
        "6. **Evaluate**:\n",
        "   - **Geometry**: PIP loss, isotropy, top-k neighbor overlap (before/after snapshot).\n",
        "   - **Behavior**: nearest-neighbor tables + Joyce-style completions.\n",
        "   - Save `results/metrics_summary.csv` and `results/hero.png`.\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs\n",
        "- `data/wake_lexicon.txt` one token per line (from Notebook 00).\n",
        "- `data/finnegans_wake.txt` UTF-8 text used to build Wake-dense windows.\n",
        "\n",
        "## Outputs\n",
        "- `results/metrics_summary.csv` PIP loss, isotropy, top-10 overlap.\n",
        "- `results/hero.png` UMAP of neighborhoods + three sample completions.\n",
        "- `Wake2vec_adapter/minipack/` minimal adapter (rows of the input embedding for new tokens).\n",
        "\n",
        "---\n",
        "\n",
        "## Controls (tweak for experiments)\n",
        "- **Base model**: `BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"` (or `\"distilgpt2\"` on CPU).\n",
        "- **LoRA**: `USE_LORA = True` / False; typical `r=8, alpha=16, dropout=0.05`.\n",
        "- **Windows radius**: `RADIUS = 2` paragraphs (increase to 3 for more context).\n",
        "- **Training**: `epochs=3`, `learning_rate≈6e-4`, `grad_accum=8`, cosine + 10% warmup, `max_grad_norm=1.0`.\n",
        "- **Tokenization**: `max_length=512` (lower to 384 if VRAM is tight).\n",
        "\n",
        "---\n",
        "\n",
        "## Repro & VRAM tips (T4)\n",
        "- During training I set:\n",
        "  - `model.config.use_cache = False`\n",
        "  - `model.gradient_checkpointing_enable()`\n",
        "  - `fp16=True` (T4 prefers fp16; re-enable cache only for generation)\n",
        "- If eval spikes VRAM: `eval_accumulation_steps=16` and/or `max_length=384`.\n",
        "- After any resize or embedding edits, I retie:\n",
        "  `model.get_output_embeddings().weight = model.get_input_embeddings().weight`.\n",
        "\n",
        "---\n",
        "\n",
        "## Baseline → After metrics\n",
        "- I snapshot a baseline before training (probe IDs + pairwise inner-product matrix).\n",
        "- After training I compute:\n",
        "  - **PIP loss** = `||P0 − P1||_F / ||P0||_F`\n",
        "  - **Isotropy** = geomean/mean of covariance eigenvalues\n",
        "  - **Top-10 overlap** of nearest neighbors\n",
        "- I append results to `results/metrics_summary.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "## Demo sampling (consistent settings)\n",
        "- `temperature=1.15`, `top_p=0.95`, `repetition_penalty=1.1`, `no_repeat_ngram_size=3`\n",
        "- I save three fixed-prompt completions into the hero figure for easy comparison.\n",
        "\n",
        "---\n",
        "\n",
        "## Optional bonus (continual pretraining)\n",
        "If I want a neutral-English refresh with optional Joyce style, I run `02b_continual_pretrain_gutenmix.ipynb`:\n",
        "- Mix a small diverse Gutenberg corpus (≈95–98%) with Wake windows (≈2–5%).\n",
        "- Add a `<STYLE=Wake>` control token.\n",
        "- Use a KL-to-teacher guard on general English batches to prevent drift.\n",
        "\n",
        "---\n",
        "\n",
        "## Checklist for a clean run\n",
        "- [ ] Tokens + `▁` variants added; head tied after resize\n",
        "- [ ] Smart init applied\n",
        "- [ ] Wake-dense windows built (radius set)\n",
        "- [ ] Embeddings (± LoRA) are the only trainable params\n",
        "- [ ] Cosine LR + warmup + clip enabled; early stopping on\n",
        "- [ ] Baseline snapshot saved **before** training\n",
        "- [ ] Metrics + hero saved **after** training\n",
        "\"\"\"\n",
        "\n",
        "!pip -q install transformers==4.43.3 accelerate peft==0.11.1 datasets umap-learn faiss-cpu matplotlib==3.8.4 sentencepiece\n",
        "\n",
        "import os, re, math, random, json, unicodedata\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# distilgpt2 for CPU smoke tests\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=DTYPE, device_map=\"auto\")\n",
        "print(\"Base vocab size:\", tok.vocab_size)\n",
        "\n",
        "# wake_lexicon.txt\n",
        "lex_path = None\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    lex_path = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    lex_path = \"/content/wake_lexicon.txt\"\n",
        "\n",
        "raw = []\n",
        "p = Path(lex_path)\n",
        "if p.exists():\n",
        "    raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "else:\n",
        "    print(\"WARNING: wake_lexicon.txt not found; provide a path or upload.\")\n",
        "\n",
        "seen = set(); new_terms = []\n",
        "existing = tok.get_vocab()\n",
        "for w in raw:\n",
        "    t = unicodedata.normalize(\"NFC\", w.strip())\n",
        "    if t and (t not in seen) and (t not in existing):\n",
        "        seen.add(t); new_terms.append(t)\n",
        "\n",
        "print(f\"Loaded {len(new_terms)} NEW tokens.\")\n",
        "print('Sample:', new_terms[:20])\n",
        "\n",
        "added = tok.add_tokens(new_terms, special_tokens=False)\n",
        "if added != len(new_terms):\n",
        "    print(f\"Note: only {added}/{len(new_terms)} newly added (collisions or tokenizer rules).\")\n",
        "model.resize_token_embeddings(len(tok))\n",
        "print(\"New vocab size:\", tok.vocab_size)\n",
        "\n",
        "# come to clean the mess: add start-of-word variants for SentencePiece-style tokenizers and tie output head\n",
        "START = \"▁\"\n",
        "prefixed = [START + t for t in new_terms if not t.startswith(START)]\n",
        "prefixed = [t for t in prefixed if t not in tok.get_vocab()]\n",
        "added2 = tok.add_tokens(prefixed, special_tokens=False)\n",
        "model.resize_token_embeddings(len(tok))\n",
        "print(f\"Added {added2} start-of-word variants. Vocab: {tok.vocab_size}\")\n",
        "\n",
        "# Tie lm_head to input embeddings so generations reflect updated embeddings\n",
        "if model.get_output_embeddings() is not None:\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "    print(\"Tied lm_head to input embeddings.\")\n",
        "\n",
        "emb = model.get_input_embeddings()\n",
        "\n",
        "def init_vec(term: str, noise=0.02):\n",
        "    ids = tok(term, add_special_tokens=False)[\"input_ids\"]\n",
        "    if ids:\n",
        "        base = emb.weight[ids].mean(0, keepdim=True)\n",
        "    else:\n",
        "        base = torch.randn_like(emb.weight[0:1])\n",
        "    return (base + noise * torch.randn_like(base)).squeeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    targets = new_terms + [(\"▁\"+t) for t in new_terms if not t.startswith(\"▁\")]\n",
        "    for t in targets:\n",
        "        idx = tok.convert_tokens_to_ids(t)\n",
        "        if idx != tok.unk_token_id:\n",
        "            emb.weight[idx].copy_(init_vec(t))\n",
        "print(\"Initialised bare + ▁ variants.\")\n",
        "\n",
        "# Freeze almost everything, train embeddings (optional tiny LoRA for attention projections)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "model.get_input_embeddings().weight.requires_grad_(True)\n",
        "\n",
        "USE_LORA = True  # if False isolates pure embedding drift\n",
        "if USE_LORA:\n",
        "    lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "                          target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "                          bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Trainable params:\", trainable)\n",
        "\n",
        "# build Wake-saturated paragraph windows, split into train/val\n",
        "fw_path = None\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    fw_path = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    fw_path = \"/content/finnegans_wake.txt\"\n",
        "\n",
        "fw = Path(fw_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "fw = unicodedata.normalize(\"NFC\", fw).replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
        "\n",
        "import re, random\n",
        "paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", fw) if p.strip()]\n",
        "probe = {t.lower() for t in new_terms if len(t) <= 40}\n",
        "\n",
        "hits = [i for i,p in enumerate(paras) if any(t in p.lower() for t in probe)]\n",
        "RADIUS = 2\n",
        "windows = [\" \".join(paras[max(0,i-RADIUS):min(len(paras), i+RADIUS+1)]) for i in hits]\n",
        "windows = list(dict.fromkeys(windows))\n",
        "random.shuffle(windows)\n",
        "if len(windows) < 500:\n",
        "    windows += random.sample(paras, k=min(2000, len(paras)))\n",
        "\n",
        "split = int(0.9 * len(windows))\n",
        "train_texts, val_texts = windows[:split], windows[split:]\n",
        "\n",
        "def tok_fn(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\"text\": train_texts}).map(tok_fn, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": Dataset.from_dict({\"text\": val_texts}).map(tok_fn, batched=True, remove_columns=[\"text\"]),\n",
        "})\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "print(\"Train/Val sizes:\", len(ds[\"train\"]), len(ds[\"validation\"]))\n",
        "\n",
        "# BASELINE SNAPSHOT\n",
        "import os, time, json, numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "timestamp = int(time.time())\n",
        "BASE_DIR = f\"/content/Wake2vec_baseline_{timestamp}\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# pick a probe of new tokens + a small control of common anchors\n",
        "probe_new = new_terms[:200]\n",
        "anchors   = [\"river\",\"history\",\"book\",\"dream\",\"night\",\"language\",\"irish\",\"cork\",\"dublin\"]\n",
        "probe = [t for t in probe_new if tok.convert_tokens_to_ids(t) != tok.unk_token_id][:150] + anchors\n",
        "\n",
        "ids = [tok.convert_tokens_to_ids(t) for t in probe]\n",
        "W  = model.get_input_embeddings().weight.detach().cpu().float().numpy() # Convert to float32 before numpy\n",
        "P0 = W[ids] @ W[ids].T  # pairwise inner products\n",
        "\n",
        "np.save(f\"{BASE_DIR}/probe_ids.npy\", np.array(ids, dtype=np.int32))\n",
        "np.save(f\"{BASE_DIR}/P0.npy\", P0)\n",
        "json.dump(probe, open(f\"{BASE_DIR}/probe_tokens.json\",\"w\"), ensure_ascii=False, indent=2)\n",
        "\n",
        "# neighbor table (top-8) for a small visible subset\n",
        "inv = {i:t for t,i in tok.get_vocab().items()}\n",
        "def neighbors(idx, k=8):\n",
        "    sims = cosine_similarity(W[idx:idx+1], W)[0]\n",
        "    top = np.argsort(-sims)[:k]\n",
        "    return [inv.get(int(j), f\"<{j}>\") for j in top]\n",
        "\n",
        "nn0 = {inv[ids[i]]: neighbors(ids[i], 8) for i in range(min(30,len(ids)))}\n",
        "json.dump(nn0, open(f\"{BASE_DIR}/neighbors_before.json\",\"w\"), ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Baseline saved:\", BASE_DIR)\n",
        "\n",
        "# metrics\n",
        "import numpy as np, json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# baseline\n",
        "BASE_DIRS = sorted([p for p in os.listdir(\"/content\") if p.startswith(\"Wake2vec_baseline_\")])\n",
        "assert BASE_DIRS, \"Run the baseline snapshot first.\"\n",
        "BASE_DIR = \"/content/\" + BASE_DIRS[-1]\n",
        "ids  = np.load(f\"{BASE_DIR}/probe_ids.npy\")\n",
        "P0   = np.load(f\"{BASE_DIR}/P0.npy\")\n",
        "probe_tokens = json.load(open(f\"{BASE_DIR}/probe_tokens.json\"))\n",
        "\n",
        "# current embeddings\n",
        "W = model.get_input_embeddings().weight.detach().cpu().float().numpy() # Convert to float32 before numpy\n",
        "P1 = W[ids] @ W[ids].T\n",
        "\n",
        "# PIP loss (Frobenius)\n",
        "pip = np.linalg.norm(P0 - P1, 'fro') / (np.linalg.norm(P0, 'fro') + 1e-12)\n",
        "\n",
        "# isotropy (geomean/mean of eigenvalues of covariance on the probe vectors)\n",
        "X = W[ids] - W[ids].mean(0, keepdims=True)\n",
        "C = (X.T @ X) / (len(ids)-1)\n",
        "evals = np.clip(np.linalg.eigvalsh(C), 1e-12, None)\n",
        "isotropy = (np.exp(np.log(evals).mean())) / (evals.mean())\n",
        "\n",
        "# top-k overlap (k=10) before/after\n",
        "def topk_neighbors(mat, k=10):\n",
        "    return np.argsort(-mat, axis=1)[:, :k]\n",
        "\n",
        "N0 = topk_neighbors(P0, k=10)\n",
        "N1 = topk_neighbors(P1, k=10)\n",
        "overlap = np.mean([len(set(N0[i]).intersection(set(N1[i]))) / 10.0 for i in range(len(ids))])\n",
        "\n",
        "import pandas as pd, os\n",
        "os.makedirs(\"/content/results\", exist_ok=True)\n",
        "pd.DataFrame([{\n",
        "    \"pip_loss\": float(pip),\n",
        "    \"isotropy\": float(isotropy),\n",
        "    \"top10_overlap\": float(overlap),\n",
        "    \"num_probe_tokens\": int(len(ids))\n",
        "}]).to_csv(\"/content/results/metrics_summary.csv\", index=False)\n",
        "print(\"metrics:\", {\"pip_loss\":pip, \"isotropy\":isotropy, \"top10_overlap\":overlap})\n",
        "print(\"Wrote /content/results/metrics_summary.csv\")\n",
        "\n",
        "# Training args, embeddings + tiny LoRA\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/Wake2vec_adapter\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=6e-4,\n",
        "    num_train_epochs=3,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    logging_steps=25,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.10,                   # ~10% warmup\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    weight_decay=0.0,\n",
        "    seed=42,\n",
        "    dataloader_num_workers=2,\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# Optional for speed\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/Wake2vec_adapter/final\")\n",
        "tok.save_pretrained(\"/content/Wake2vec_adapter/final_tok\")\n",
        "print(\"Saved adapter + tokenizer to /content/Wake2vec_adapter\")\n",
        "\n",
        "# tie output head to input embeddings (paranoia pass)\n",
        "if model.get_output_embeddings() is not None:\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "model.config.use_cache = True\n",
        "\n",
        "import unicodedata, re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "LETTER = re.compile(r\"^[A-Za-zÀ-ÖØ-öø-ÿ][A-Za-zÀ-ÖØ-öø-ÿ'’-]*$\")\n",
        "\n",
        "def is_latin(ch):\n",
        "    try: return (\"LATIN\" in unicodedata.name(ch)) or ch in \"'’-\"\n",
        "    except ValueError: return False\n",
        "\n",
        "def is_wordlike(tok):\n",
        "    if \" \" in tok: return False\n",
        "    if not LETTER.match(tok): return False\n",
        "    return all(is_latin(c) for c in tok)\n",
        "\n",
        "vocab = tok.get_vocab(); inv = {i:t for t,i in vocab.items()}\n",
        "allowed_ids = [i for i,t in inv.items() if is_wordlike(t) or t in new_terms or (\" \"+t) in new_terms]\n",
        "\n",
        "W = model.get_input_embeddings().weight.detach().cpu().float().numpy() # Convert to float32 before numpy\n",
        "Wa = W[allowed_ids] / (np.linalg.norm(W[allowed_ids], axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def clean_neighbors(terms, k=8):\n",
        "    out = {}\n",
        "    for t in terms[:30]:\n",
        "        tid = tok.convert_tokens_to_ids(t)\n",
        "        if tid == tok.unk_token_id: continue\n",
        "        v = W[tid:tid+1] / (np.linalg.norm(W[tid:tid+1], axis=1, keepdims=True) + 1e-12)\n",
        "        sims = (v @ Wa.T)[0]\n",
        "        top = np.argpartition(-sims, range(min(k, len(sims))))[:k]\n",
        "        top = top[np.argsort(-sims[top])]\n",
        "        out[t] = [inv[allowed_ids[j]] for j in top]\n",
        "    return out\n",
        "\n",
        "probe = new_terms[:12]\n",
        "nn = clean_neighbors(probe, k=8)\n",
        "for k,v in nn.items():\n",
        "    print(f\"{k:>20} -> {v}\")\n",
        "\n",
        "from transformers import LogitsProcessorList, RepetitionPenaltyLogitsProcessor, NoRepeatNGramLogitsProcessor\n",
        "\n",
        "def complete_calm(prompt, max_new_tokens=80):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    processors = LogitsProcessorList([\n",
        "        RepetitionPenaltyLogitsProcessor(penalty=1.18),\n",
        "        NoRepeatNGramLogitsProcessor(4),\n",
        "    ])\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.95,\n",
        "            top_p=0.90,\n",
        "            top_k=50,\n",
        "            typical_p=0.95,\n",
        "            logits_processor=processors,\n",
        "            renormalize_logits=True,\n",
        "            cache_implementation=\"static\"\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# Smoothing pass: embeddings-only, short & gentle\n",
        "for p in model.parameters(): p.requires_grad = False\n",
        "model.get_input_embeddings().weight.requires_grad_(True)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "args2 = TrainingArguments(\n",
        "    output_dir=\"/content/Wake2vec_adapter_smooth\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=3e-4,      # lower LR\n",
        "    num_train_epochs=0.7,    # < 1 epoch\n",
        "    bf16=torch.cuda.is_available(), # Changed from fp16=True\n",
        "    logging_steps=25,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=[]\n",
        ")\n",
        "Trainer(model=model, args=args2, train_dataset=ds[\"train\"], data_collator=collator).train()\n",
        "\n",
        "from transformers import (\n",
        "    LogitsProcessor, LogitsProcessorList,\n",
        "    RepetitionPenaltyLogitsProcessor,\n",
        "    NoRepeatNGramLogitsProcessor\n",
        ")\n",
        "import torch, math\n",
        "\n",
        "# tiny bias helper: nudge Wake tokens, common stopwords down\n",
        "class LogitBiasProcessor(LogitsProcessor):\n",
        "    def __init__(self, bias_ids, bias_values):\n",
        "        self.bias = torch.zeros(max(bias_ids)+1, dtype=torch.float32, device=bias_values.device)\n",
        "        self.bias[:]=0\n",
        "        self.bias[bias_ids] = bias_values\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        # scores: [batch, vocab]\n",
        "        return scores + self.bias\n",
        "\n",
        "def build_decoder_processors(tok, new_terms, wake_boost=0.35, stop_suppress=0.25):\n",
        "    # Wake lexicon (+ ▁variants)\n",
        "    wake_tokens = set()\n",
        "    for t in new_terms[:1000]:\n",
        "        for form in (t, \"▁\"+t if not t.startswith(\"▁\") else t):\n",
        "            tid = tok.convert_tokens_to_ids(form)\n",
        "            if tid != tok.unk_token_id:\n",
        "                wake_tokens.add(tid)\n",
        "\n",
        "    # A tiny English stopword set\n",
        "    stopish = \"\"\"\n",
        "    the of and to a in that it is you i me we he she they my your his her our their for on as with was be are\n",
        "    this at from or an so if but by have has had not just do did does can will would could should\n",
        "    \"\"\".split()\n",
        "    stop_ids = [tok.convert_tokens_to_ids(w) for w in stopish if tok.convert_tokens_to_ids(w) != tok.unk_token_id]\n",
        "\n",
        "    # Bias vector (same length as vocab, added to logits each step)\n",
        "    vocab_size = len(tok)\n",
        "    bias = torch.zeros(vocab_size)\n",
        "    # +wake_boost to Wake tokens, -stop_suppress to very common function words\n",
        "    for wid in wake_tokens: bias[wid] += wake_boost\n",
        "    for sid in stop_ids:    bias[sid]  -= stop_suppress\n",
        "\n",
        "    processors = LogitsProcessorList([\n",
        "        RepetitionPenaltyLogitsProcessor(1.20),\n",
        "        NoRepeatNGramLogitsProcessor(4),\n",
        "        # plug a LogitBiasProcessor at call-time (needs device)\n",
        "    ])\n",
        "    return processors, bias\n",
        "\n",
        "processors_base, bias_vec = build_decoder_processors(tok, new_terms, wake_boost=0.35, stop_suppress=0.25)\n",
        "\n",
        "def complete_portfolio(prompt, max_new_tokens=90):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # move bias to device and wrap as processor\n",
        "    bias_proc = LogitBiasProcessor(\n",
        "        bias_ids=torch.arange(len(bias_vec), device=model.device),\n",
        "        bias_values=bias_vec.to(model.device),\n",
        "    )\n",
        "    processors = LogitsProcessorList(list(processors_base) + [bias_proc])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.90,      # cooler\n",
        "            top_p=0.88,\n",
        "            top_k=40,\n",
        "            typical_p=0.95,\n",
        "            logits_processor=processors,\n",
        "            renormalize_logits=True,\n",
        "            no_repeat_ngram_size=4,\n",
        "            cache_implementation=\"static\",\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def complete(prompt, max_new_tokens=60):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=1.15,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "tests = [\n",
        "    \"By the river I thought of — Earwicker, the prankquean, and\",\n",
        "    \"At night I dream of\",\n",
        "    \"In the story of this book,\",\n",
        "    \"Explain gradient descent in the style of Joyce: riverrun,\"\n",
        "]\n",
        "for p in tests:\n",
        "    print(\"\\n===\", p, \"\\n\", complete(p))\n",
        "\n",
        "# Recompute metrics and append (keeps history)\n",
        "import numpy as np, pandas as pd, json, os\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# If you snapped a baseline BEFORE training, reuse it; otherwise just log \"after\" once.\n",
        "# Expect: PIP > 0 (some drift), isotropy reasonable (~0.2–0.9 depending on base), overlap < 1.0.\n",
        "W = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "\n",
        "# Probe terms = mix of new tokens + anchors\n",
        "anchors = [\"river\",\"history\",\"book\",\"dream\",\"night\",\"language\",\"irish\",\"dublin\",\"cork\"]\n",
        "probe = [t for t in new_terms[:150] if tok.convert_tokens_to_ids(t) != tok.unk_token_id] + anchors\n",
        "ids = [tok.convert_tokens_to_ids(t) for t in probe]\n",
        "P = W[ids] @ W[ids].T\n",
        "\n",
        "# If you have a saved baseline P0.npy + probe_ids.npy, load and compute PIP\n",
        "pip = None\n",
        "try:\n",
        "    import glob\n",
        "    base_dir = sorted(glob.glob(\"/content/Wake2vec_baseline_*\"))[-1]\n",
        "    base_ids = np.load(base_dir + \"/probe_ids.npy\")\n",
        "    P0 = np.load(base_dir + \"/P0.npy\")\n",
        "    # align if probe changed size/order\n",
        "    if np.array_equal(base_ids, np.array(ids)):\n",
        "        pip = float(np.linalg.norm(P0 - P, 'fro') / (np.linalg.norm(P0, 'fro') + 1e-12))\n",
        "except Exception as e:\n",
        "    pip = None\n",
        "\n",
        "# Isotropy\n",
        "X = W[ids] - W[ids].mean(0, keepdims=True)\n",
        "C = (X.T @ X) / max(1, (len(ids)-1))\n",
        "evals = np.clip(np.linalg.eigvalsh(C), 1e-12, None)\n",
        "isotropy = float(np.exp(np.log(evals).mean()) / evals.mean())\n",
        "\n",
        "# Top-10 overlap (if baseline available)\n",
        "topk = 10\n",
        "top_now = np.argsort(-P, axis=1)[:, :topk]\n",
        "overlap = None\n",
        "if pip is not None:\n",
        "    top_base = np.argsort(-P0, axis=1)[:, :topk]\n",
        "    overlap = float(np.mean([len(set(top_base[i]).intersection(set(top_now[i]))) / topk for i in range(len(ids))]))\n",
        "\n",
        "row = {\"pip_loss\": pip if pip is not None else np.nan,\n",
        "       \"isotropy\": isotropy,\n",
        "       \"top10_overlap\": overlap if overlap is not None else np.nan,\n",
        "       \"num_probe_tokens\": len(ids)}\n",
        "df_path = \"results/metrics_summary.csv\"\n",
        "pd.DataFrame([row]).to_csv(df_path, mode=(\"a\" if os.path.exists(df_path) else \"w\"),\n",
        "                           header=not os.path.exists(df_path), index=False)\n",
        "print(\"Appended metrics to\", df_path)\n",
        "\n",
        "# Neighbor table (cleaned) for README appendix\n",
        "import unicodedata, re\n",
        "LETTER = re.compile(r\"^[A-Za-zÀ-ÖØ-öø-ÿ][A-Za-zÀ-ÖØ-öø-ÿ'’-]*$\")\n",
        "def is_latin(ch):\n",
        "    try: return (\"LATIN\" in unicodedata.name(ch)) or ch in \"'’-\"\n",
        "    except ValueError: return False\n",
        "def is_wordlike(tok):\n",
        "    return (\"▁\" not in tok) and LETTER.match(tok) and all(is_latin(c) for c in tok)\n",
        "\n",
        "vocab = tok.get_vocab(); inv = {i:t for t,i in vocab.items()}\n",
        "allowed_ids = [i for i,t in inv.items() if is_wordlike(t) or t in new_terms or (\"▁\"+t) in new_terms]\n",
        "Wa = W[allowed_ids] / (np.linalg.norm(W[allowed_ids], axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def clean_neighbors_one(idx, k=8):\n",
        "    v = W[idx:idx+1] / (np.linalg.norm(W[idx:idx+1], axis=1, keepdims=True) + 1e-12)\n",
        "    sims = (v @ Wa.T)[0]\n",
        "    top = np.argpartition(-sims, range(min(k, len(sims))))[:k]\n",
        "    top = top[np.argsort(-sims[top])]\n",
        "    return [inv[allowed_ids[j]] for j in top]\n",
        "\n",
        "nn_after = {}\n",
        "for t in probe[:20]:\n",
        "    tid = tok.convert_tokens_to_ids(t)\n",
        "    if tid != tok.unk_token_id:\n",
        "        nn_after[t] = clean_neighbors_one(tid, k=8)\n",
        "json.dump(nn_after, open(\"results/neighbors_after.json\",\"w\"), ensure_ascii=False, indent=2)\n",
        "print(\"Wrote results/neighbors_after.json\")\n",
        "\n",
        "# hero fig\n",
        "import umap, matplotlib.pyplot as plt, numpy as np, textwrap, os, random\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "probe_show = list(dict.fromkeys(new_terms[:40] + [\"river\",\"history\",\"book\",\"dream\",\"night\",\"language\",\"irish\",\"dublin\"]))\n",
        "probe_ids = [tok.convert_tokens_to_ids(t) for t in probe_show if tok.convert_tokens_to_ids(t) != tok.unk_token_id]\n",
        "vecs = model.get_input_embeddings().weight[probe_ids].detach().cpu().numpy()\n",
        "\n",
        "mapper = umap.UMAP(n_neighbors=8, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
        "xy = mapper.fit_transform(vecs)\n",
        "\n",
        "# three calmer samples\n",
        "samples = {\n",
        "  \"By the river I thought of — Earwicker, the prankquean, and\": complete_portfolio(\"By the river I thought of — Earwicker, the prankquean, and\", 120),\n",
        "  \"In the story of this book, HCE remembers that\": complete_portfolio(\"In the story of this book, HCE remembers that\", 120),\n",
        "  \"Explain gradient descent in the style of Joyce: riverrun,\": complete_portfolio(\"Explain gradient descent in the style of Joyce: riverrun,\", 120),\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(xy[:,0], xy[:,1])\n",
        "inv = {i:t for t,i in tok.get_vocab().items()}\n",
        "for pid, (x,y) in zip(probe_ids, xy):\n",
        "    plt.text(x, y, inv.get(int(pid), \"?\"), fontsize=8)\n",
        "plt.title(\"Wake2vec: neighborhood map (UMAP)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "y0=1.0\n",
        "for k,v in samples.items():\n",
        "    txt = k + \"\\n\" + textwrap.fill(v.replace(\"\\n\",\" \"), width=44)\n",
        "    plt.text(0.0, y0, txt, fontsize=8, va='top')\n",
        "    y0 -= 0.32\n",
        "plt.axis('off'); plt.title(\"Sample completions\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/hero.png\", dpi=200)\n",
        "print(\"Saved results/hero.png\")\n",
        "\n",
        "# Export minimal adapter pack: changed embedding rows (bare +   forms)\n",
        "save_dir = Path(\"/content/Wake2vec_adapter/minipack\"); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "changed = [tok.convert_tokens_to_ids(t) for t in (new_terms + [\" \"+t for t in new_terms if not t.startswith(\" \")])]\n",
        "changed = [i for i in changed if i != tok.unk_token_id]\n",
        "emb_slice = model.get_input_embeddings().weight[changed].detach().cpu().float().numpy() # Convert to float32 before numpy\n",
        "np.save(save_dir / \"new_token_ids.npy\", np.array(changed, dtype=np.int32))\n",
        "np.save(save_dir / \"new_token_vectors.npy\", emb_slice)\n",
        "(Path(save_dir / \"README.txt\").write_text(\n",
        "    \"Rows from input embedding for Wake2vec tokens (bare +  forms).\\n\"\n",
        "    \"Apply to base model by index assignment.\\n\", encoding=\"utf-8\"))\n",
        "print(\"Mini pack saved:\", save_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}