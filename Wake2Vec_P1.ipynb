{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wake2Vec Phase 1: Embedding-Only Fine-Tuning (Steps 0-1300)\n",
    "\n",
    "Complete training pipeline for embedding-only fine-tuning of TinyLlama-1.1B with Finnegans Wake lexicon injection.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements embedding-only fine-tuning for TinyLlama-1.1B augmented with 44,990 Wake-specific tokens. The training methodology isolates vocabulary embeddings through gradient masking while keeping pre-trained model parameters frozen, enabling lexical integration into the existing semantic space without catastrophic forgetting.\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "**Phase:** Complete training from initialization (step 0) to step 1300  \n",
    "**Model:** TinyLlama/TinyLlama-1.1B-Chat-v1.0  \n",
    "**Vocabulary extension:** 32,000 (base) + 44,990 (Wake) = 76,500 tokens  \n",
    "**Training strategy:** Embedding-only optimization with gradient masking on base vocabulary  \n",
    "**Data:** Finnegans Wake corpus (910 training samples, 48 validation samples)  \n",
    "**Optimization:** Adafactor (learning rate 5e-4, 5% warmup)  \n",
    "**Hardware:** Single T4 GPU (15GB VRAM) via Google Colab  \n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized into sequential cells for reproducible execution:\n",
    "\n",
    "1. Dependency pinning and compatibility patches (transformers 4.57.1, accelerate 1.2.1)\n",
    "2. Vocabulary extension: Wake lexicon injection and embedding initialization\n",
    "3. Model loading with gradient masking (76.5M trainable parameters)\n",
    "4. Dataset loading and sequence truncation (max length 256 tokens)\n",
    "5. Training callback definitions (monitoring, backups, snapshots)\n",
    "6. Training execution from checkpoint-0\n",
    "\n",
    "## Monitoring and Backup Systems\n",
    "\n",
    "The notebook provides automated systems for long-running training stability:\n",
    "\n",
    "- Training and validation loss tracking via Trainer logging\n",
    "- Checkpoint validation (weights, optimizer state, trainer state)\n",
    "- Sentry mirror system for automated checkpoint backups to Google Drive\n",
    "- Embedding snapshot capture at 50-step intervals for geometric analysis\n",
    "- Step timer diagnostics with 10-step rolling average\n",
    "- Heartbeat metadata for remote monitoring\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "Training proceeds from checkpoint-0 with the following hyperparameters:\n",
    "\n",
    "- Batch size: 1 (effective batch size 16 via gradient accumulation)\n",
    "- Gradient accumulation steps: 16\n",
    "- Learning rate: 5e-4 with 5% warmup (65 steps)\n",
    "- Sequence length: 256 tokens (runtime truncation)\n",
    "- Save frequency: 100 steps\n",
    "- Evaluation frequency: 200 steps\n",
    "- Checkpoint retention: 20 most recent\n",
    "- Gradient clipping: maximum norm 1.0\n",
    "- Gradient checkpointing: enabled for memory efficiency\n",
    "\n",
    "All training callbacks (evaluation triggers, sentry mirroring, embedding snapshots, throughput monitoring) are integrated into the Hugging Face Trainer workflow for automated execution. The notebook includes compatibility patches for transformers/accelerate version mismatches to ensure stable execution on Colab environments.\n",
    "\n",
    "## Vocabulary Extension Methodology\n",
    "\n",
    "Wake tokens are added to the base TinyLlama tokenizer and new embeddings are initialized using mean initialization from existing embeddings. The model's input and output embeddings are tied to ensure consistency during generation. Only embedding parameters are trainable (76.5M parameters), while all transformer layers remain frozen (2.05B parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Dependency Pinning and Compatibility Patches\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def pin(pkg, ver):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        assert m.__version__ == ver\n",
    "        print(f\"[OK] {pkg} {m.__version__}\")\n",
    "    except Exception:\n",
    "        print(f\"[PIN] {pkg}=={ver}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
    "        m = importlib.import_module(pkg)\n",
    "        print(f\"[OK] {pkg} {m.__version__}\")\n",
    "\n",
    "pin(\"transformers\", \"4.57.1\")\n",
    "pin(\"accelerate\", \"1.2.1\")\n",
    "pin(\"datasets\", \"2.21.0\")\n",
    "\n",
    "import accelerate\n",
    "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
    "    _orig = accelerate.Accelerator.unwrap_model\n",
    "    def _shim(self, model, *args, **kw):\n",
    "        kw.pop(\"keep_torch_compile\", None)\n",
    "        return _orig(self, model, *args, **kw)\n",
    "    accelerate.Accelerator.unwrap_model = _shim\n",
    "    accelerate.Accelerator._w2v_patched = True\n",
    "    print(\"[PATCH] unwrap_model compatibility active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Vocabulary Extension\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[WAKE2VEC P1: Vocab Extension]\")\n",
    "\n",
    "WAKE2VEC_ROOT = Path(\"/content/drive/MyDrive/wake2vecP1\")\n",
    "WAKE2VEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"\\n1. Loading base model: {base_model}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "print(f\"   Base vocab: {len(tok)} tokens\")\n",
    "\n",
    "wake_tokens_file = \"/content/wake_lexicon.txt\"\n",
    "print(f\"\\n2. Loading Wake lexicon...\")\n",
    "\n",
    "with open(wake_tokens_file, 'r', encoding='utf-8') as f:\n",
    "    wake_tokens = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"   Wake tokens: {len(wake_tokens)}\")\n",
    "\n",
    "print(f\"\\n3. Extending tokenizer\")\n",
    "num_added = tok.add_tokens(wake_tokens)\n",
    "print(f\"   New vocab size: {len(tok)}\")\n",
    "\n",
    "print(f\"\\n4. Resizing model embeddings\")\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "print(f\"\\n5. Initializing new embeddings\")\n",
    "with torch.no_grad():\n",
    "    old_embeddings = model.get_input_embeddings().weight[:32000]\n",
    "    avg_embedding = old_embeddings.mean(dim=0)\n",
    "    model.get_input_embeddings().weight[32000:] = avg_embedding\n",
    "\n",
    "print(f\"\\n6. Tying embeddings\")\n",
    "with torch.no_grad():\n",
    "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
    "\n",
    "save_path = WAKE2VEC_ROOT / \"checkpoint-0\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n7. Saving checkpoint-0\")\n",
    "model.save_pretrained(str(save_path))\n",
    "tok.save_pretrained(str(save_path))\n",
    "\n",
    "print(f\"\\n[VOCAB EXTENSION COMPLETE: {len(tok)} tokens]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Training Pipeline\n",
    "import os, gc, torch, time, json, shutil\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          DataCollatorForLanguageModeling, TrainingArguments,\n",
    "                          Trainer, TrainerCallback)\n",
    "\n",
    "WAKE2VEC_ROOT = Path(\"/content/drive/MyDrive/wake2vecP1\")\n",
    "LOCAL_RUN = Path(\"/content/runs/wake2vecP1\")\n",
    "SENTRY = WAKE2VEC_ROOT / \"sentry_backups\"\n",
    "RESUME_FROM = WAKE2VEC_ROOT / \"checkpoint-0\"\n",
    "DATASETS = Path(\"/content/drive/MyDrive/wake2vec/datasets\")\n",
    "\n",
    "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
    "SENTRY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"[WAKE2VEC P1: 0â†’1300]\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True, local_files_only=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(RESUME_FROM),\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=None,\n",
    "    low_cpu_mem_usage=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "model.config.use_cache = False\n",
    "model.config.attn_implementation = \"eager\"\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "emb = model.get_input_embeddings()\n",
    "emb.weight.requires_grad = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.get_output_embeddings().weight = emb.weight\n",
    "\n",
    "model.train()\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "train_ds = load_from_disk(str(DATASETS/\"train_ds\"))\n",
    "valid_ds = load_from_disk(str(DATASETS/\"valid_ds\"))\n",
    "\n",
    "base_dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "class TruncatingCollator:\n",
    "    def __init__(self, base, max_len=256):\n",
    "        self.base, self.max_len = base, max_len\n",
    "    def __call__(self, feats):\n",
    "        out = self.base(feats)\n",
    "        for k,v in list(out.items()):\n",
    "            if isinstance(v, torch.Tensor) and v.dim()==2:\n",
    "                out[k] = v[:, :self.max_len]\n",
    "        return out\n",
    "\n",
    "dc = TruncatingCollator(base_dc)\n",
    "\n",
    "class EvalEveryNSteps(TrainerCallback):\n",
    "    def __init__(self, n=200):\n",
    "        self.n = n\n",
    "    def on_step_end(self, args, state, control, **kw):\n",
    "        s = int(state.global_step or 0)\n",
    "        if s and s % self.n == 0:\n",
    "            control.should_evaluate = True\n",
    "\n",
    "class SentryMirror(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kw):\n",
    "        try:\n",
    "            cks = sorted(LOCAL_RUN.glob(\"checkpoint-*\"),\n",
    "                        key=lambda p: int(p.name.split(\"-\")[-1]),\n",
    "                        reverse=True)\n",
    "            if not cks:\n",
    "                return\n",
    "            ck = cks[0]\n",
    "            has_weights = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
    "            if not has_weights:\n",
    "                return\n",
    "            dst = SENTRY / ck.name\n",
    "            if not dst.exists():\n",
    "                shutil.copytree(ck, dst)\n",
    "                print(f\"[SENTRY] {ck.name} backed up\")\n",
    "            os.sync()\n",
    "        except Exception as e:\n",
    "            print(f\"[SENTRY] ERROR: {e}\")\n",
    "\n",
    "class EmbeddingSnap(TrainerCallback):\n",
    "    def __init__(self, every=50):\n",
    "        self.every = every\n",
    "        (WAKE2VEC_ROOT/\"emb_snaps\").mkdir(parents=True, exist_ok=True)\n",
    "    def on_step_end(self, args, state, control, **kw):\n",
    "        s = int(state.global_step or 0)\n",
    "        if s and s % self.every == 0:\n",
    "            try:\n",
    "                E = model.get_input_embeddings().weight.detach().cpu()\n",
    "                path = (WAKE2VEC_ROOT/\"emb_snaps\")/f\"emb_step{s:04d}.pt\"\n",
    "                torch.save(E, path)\n",
    "                print(f\"[SNAP] {s}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[SNAP] fail: {e}\")\n",
    "\n",
    "class StepTimer(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.step_times = []\n",
    "        self.last_step = None\n",
    "        self.last_time = None\n",
    "    def on_step_end(self, args, state, control, **kw):\n",
    "        s = int(state.global_step or 0)\n",
    "        now = time.time()\n",
    "        if self.last_step is not None:\n",
    "            self.step_times.append(now - self.last_time)\n",
    "            if s % 10 == 0:\n",
    "                avg = sum(self.step_times[-10:]) / len(self.step_times[-10:])\n",
    "                print(f\"[{s:4d}] {avg:.1f}s/step\")\n",
    "        self.last_step, self.last_time = s, now\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(LOCAL_RUN),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    max_steps=1300,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    optim=\"adafactor\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=20,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=[\"none\"],\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    data_collator=dc,\n",
    "    callbacks=[EvalEveryNSteps(200), SentryMirror(), EmbeddingSnap(50), StepTimer()],\n",
    ")\n",
    "\n",
    "print(\"[TRAINING START]\")\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "print(f\"[COMPLETE] {(time.time()-t0)/60:.1f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
