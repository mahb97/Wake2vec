{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2vec_heartbeat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec Heartbeat (Resume from 300)\n",
        "Read-only monitor + safety mirror. Uses latest local run in `/content/runs/t4_*` and mirrors the newest full checkpoint to Drive.\n"
      ],
      "metadata": {
        "id": "ynOqEGeqU5u5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQKnp813U5EQ",
        "outputId": "5b30774a-6b17-4dfa-c530-7ad58976e38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resolve active RUN"
      ],
      "metadata": {
        "id": "LuVQgnjyVrU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, time\n",
        "\n",
        "RUN_ID = None\n",
        "\n",
        "LOCAL_ROOT = pathlib.Path(\"/content/runs\")\n",
        "DRIVE_ROOT = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "def latest_run(root):\n",
        "    if not root.exists(): return None\n",
        "    runs = []\n",
        "    for p in root.glob(\"t4_*\"):\n",
        "        try: runs.append((p.stat().st_mtime, p))\n",
        "        except FileNotFoundError: pass\n",
        "    return max(runs, key=lambda x: x[0])[1] if runs else None\n",
        "\n",
        "LOCAL_RUN  = (LOCAL_ROOT/RUN_ID) if RUN_ID else latest_run(LOCAL_ROOT)\n",
        "DRIVE_RUN  = (DRIVE_ROOT/\"runs\"/RUN_ID) if RUN_ID else latest_run(DRIVE_ROOT/\"runs\")\n",
        "RUN = LOCAL_RUN or DRIVE_RUN\n",
        "assert RUN is not None, \"No local or Drive t4_* run found.\"\n",
        "\n",
        "print(\"Watching:\", RUN, \"| mtime:\", time.ctime(RUN.stat().st_mtime))\n",
        "SENTRY = DRIVE_ROOT/\"sentry_backups\"/RUN.name\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "sSwAWS8BVK-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6acb3d-8520-4ad9-a41b-bbb995a56d47"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Watching: /content/drive/MyDrive/wake2vec/runs/t4_1762376560 | mtime: Sun Nov  9 22:54:00 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss tail"
      ],
      "metadata": {
        "id": "XYZ2FZoFVdKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    tail = logs[-5:]\n",
        "    print(\"[LOSS stream] last:\", [(d[\"step\"], round(float(d[\"loss\"]),4)) for d in tail])\n",
        "else:\n",
        "    # Fallback\n",
        "    cks = sorted(RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "    if cks and (cks[-1]/\"trainer_state.json\").exists():\n",
        "        state = json.loads((cks[-1]/\"trainer_state.json\").read_text())\n",
        "        tail = [(d[\"step\"], d[\"loss\"]) for d in state.get(\"log_history\", []) if \"loss\" in d][-5:]\n",
        "        print(\"[LOSS state ] last:\", tail if tail else \"—\")\n",
        "    else:\n",
        "        print(\"[LOSS] no logs yet\")"
      ],
      "metadata": {
        "id": "j8ZlK6S3VOYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a1d1e0-c046-49cf-f707-609467716aa1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS stream] last: [(350, 5.4883), (400, 5.3082), (450, 4.7776), (500, 4.0891), (550, 3.2604)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval tail"
      ],
      "metadata": {
        "id": "MtiVwK5aV4L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "cks = sorted(RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "if cks:\n",
        "    p = cks[-1]/\"trainer_state.json\"\n",
        "    if p.exists():\n",
        "        state = json.loads(p.read_text())\n",
        "        evals = [d for d in state.get(\"log_history\", []) if \"eval_loss\" in d][-3:]\n",
        "        print(\"[EVAL] tail:\", evals if evals else \"—\")\n",
        "    else:\n",
        "        print(\"[EVAL] none yet (next at 400/600/800...)\")\n",
        "else:\n",
        "    print(\"[EVAL] no checkpoints yet\")"
      ],
      "metadata": {
        "id": "nKxTbFa5V8-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e5e157-284a-4a03-99d1-9f8708410723"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EVAL] tail: [{'epoch': 3.50989010989011, 'eval_loss': 6.237724304199219, 'eval_runtime': 13.5689, 'eval_samples_per_second': 3.538, 'eval_steps_per_second': 0.442, 'step': 200}, {'epoch': 7.0175824175824175, 'eval_loss': 6.416950225830078, 'eval_runtime': 13.5887, 'eval_samples_per_second': 3.532, 'eval_steps_per_second': 0.442, 'step': 400}, {'epoch': 10.527472527472527, 'eval_loss': 7.096441268920898, 'eval_runtime': 13.6439, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'step': 600}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checkpoint audit"
      ],
      "metadata": {
        "id": "FlUMH5iyV-As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, time\n",
        "\n",
        "def latest_full_ckpt(root):\n",
        "    cks = sorted(root.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cks:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "src = latest_full_ckpt(RUN)\n",
        "if src is None:\n",
        "    print(\"[SENTRY] No full checkpoint yet; wait for next save.\")\n",
        "else:\n",
        "    dst = SENTRY/src.name\n",
        "    if not dst.exists():\n",
        "        shutil.copytree(src, dst)\n",
        "        print(f\"[SENTRY] mirrored {src.name} (mtime {time.ctime(src.stat().st_mtime)})\")\n",
        "    else:\n",
        "        print(\"[SENTRY] already has\", src.name)\n",
        "    # Mirror metrics\n",
        "    msrc = RUN/\"metrics\"; mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    if msrc.exists():\n",
        "        for f in msrc.glob(\"*.json\"):\n",
        "            shutil.copy2(f, mdst/f.name); copied += 1\n",
        "    print(f\"[SENTRY] metrics mirrored ({copied} files)\")"
      ],
      "metadata": {
        "id": "68OG0U5TV_-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acd3ee4-0076-42af-8596-ea1eb018295a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] already has checkpoint-300\n",
            "[SENTRY] metrics mirrored (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding snapshot quick view"
      ],
      "metadata": {
        "id": "Eg4OE_TVWJt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SNAPS_DIR = DRIVE_ROOT/\"emb_snaps\"/RUN.name\n",
        "if SNAPS_DIR.exists():\n",
        "    snaps = sorted(SNAPS_DIR.glob(\"emb_step*.pt\"))\n",
        "    print(f\"[SNAPS] count={len(snaps)}  latest=\", snaps[-1].name if snaps else \"—\")\n",
        "    hb = SNAPS_DIR/\"heartbeat.json\"\n",
        "    if hb.exists(): print(\"[SNAPS] heartbeat:\", hb.read_text())\n",
        "else:\n",
        "    print(\"[SNAPS] none yet (first at step 350 if every 50)\")"
      ],
      "metadata": {
        "id": "XEvsO-XAWOm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5acb2f-1057-46b7-f4cd-ef33e2306ac1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAPS] count=9  latest= emb_step0750.pt\n",
            "[SNAPS] heartbeat: {\n",
            "  \"step\": 750,\n",
            "  \"rows\": 32000,\n",
            "  \"dim\": 2048,\n",
            "  \"ts\": 1762729811.8068688\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0HV11oD6Xuj",
        "outputId": "f121d76d-b270-46ec-f6e0-bfc26539a22b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "opt light sync"
      ],
      "metadata": {
        "id": "Mltz83NCWR-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, pathlib\n",
        "touch = RUN/\"_heartbeat.touch\"\n",
        "touch.write_text(str(time.time()))\n",
        "os.sync()\n",
        "print(\"[SYNC] touched + sync hinted →\", touch)"
      ],
      "metadata": {
        "id": "H9oPq1LhWVol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4b7968-0939-4aac-f4f4-f11b0160f565"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] touched + sync hinted → /content/drive/MyDrive/wake2vec/runs/t4_1762376560/_heartbeat.touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, pathlib\n",
        "RUN = pathlib.Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560\")\n",
        "SENTRY = pathlib.Path(\"/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\")\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "src = RUN/\"checkpoint-750\"; dst = SENTRY/\"checkpoint-750\"\n",
        "if src.exists() and not dst.exists():\n",
        "    shutil.copytree(src, dst); print(\"[SENTRY] mirrored checkpoint-750\")"
      ],
      "metadata": {
        "id": "5rhzlcL373m-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "RUN = pathlib.Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560\")\n",
        "ck = RUN/\"checkpoint-750\"\n",
        "print(\"750 exists:\", ck.exists(),\n",
        "      \"| weights:\", (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoKLm8Op7yez",
        "outputId": "a2be1183-6d34-48c8-ea1c-8c98d30e89a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750 exists: False | weights: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A1) If Drive is mounted, cleanly unmount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.flush_and_unmount()\n",
        "    print(\"[OK] flushed & unmounted\")\n",
        "except Exception as e:\n",
        "    print(\"[INFO] flush/unmount skipped:\", e)\n",
        "\n",
        "# A2) Remove stale mountpoint contents (safe in Colab)\n",
        "import shutil, os\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "    shutil.rmtree(\"/content/drive\", ignore_errors=True)\n",
        "    print(\"[OK] removed /content/drive\")\n",
        "\n",
        "# A3) Fresh mount to the correct Google account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)   # choose the account with wake2vec\n",
        "\n",
        "# A4) Sanity check\n",
        "import pathlib\n",
        "BASE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "print(\"[BASE exists]\", BASE.exists())\n",
        "print(\"[BASE contents]\", [p.name for p in BASE.iterdir()] if BASE.exists() else \"—\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv5y4SBQAFvu",
        "outputId": "0792c47e-58be-4721-e5b4-efe1723c2f46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "[OK] flushed & unmounted\n",
            "[OK] removed /content/drive\n",
            "Mounted at /content/drive\n",
            "[BASE exists] True\n",
            "[BASE contents] ['runs', 'adapters', 'reports', 'archives', 'notebooks', 'datasets', 'sentry_backups', 'emb_snaps']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "BASE   = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS   = BASE/\"runs\"\n",
        "SENTRY = BASE/\"sentry_backups\"\n",
        "SNAPS  = BASE/\"emb_snaps\"\n",
        "\n",
        "def audit(run_root):\n",
        "    print(f\"\\n[{run_root}] exists:\", run_root.exists())\n",
        "    if not run_root.exists(): return\n",
        "    for run in sorted(run_root.glob(\"t4_*\")):\n",
        "        print(\" \", run.name)\n",
        "        for ck in sorted(run.glob(\"checkpoint-*\"), key=lambda p:int(p.name.split(\"-\")[-1])):\n",
        "            step = int(ck.name.split(\"-\")[-1]) if ck.name.count(\"-\") else -1\n",
        "            has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "            print(f\"   {ck.name:>20}  weights={int(has_w)}\")\n",
        "\n",
        "audit(RUNS)\n",
        "audit(SENTRY)\n",
        "\n",
        "print(\"\\n[SNAPS] exists:\", SNAPS.exists())\n",
        "if SNAPS.exists():\n",
        "    for r in sorted(SNAPS.glob(\"t4_*\")):\n",
        "        snaps = sorted(r.glob(\"emb_step*.pt\"))\n",
        "        print(\" \", r.name, \"| snaps:\", len(snaps), \"| latest:\", snaps[-1].name if snaps else \"—\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GicjBh8rBFOL",
        "outputId": "d86a924f-611b-4e37-c920-e8f61315f89e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[/content/drive/MyDrive/wake2vec/runs] exists: True\n",
            "  t4_1762100254\n",
            "  t4_1762104879\n",
            "  t4_1762105026\n",
            "  t4_1762113417\n",
            "  t4_1762375997\n",
            "  t4_1762376307\n",
            "  t4_1762376560\n",
            "         checkpoint-100  weights=1\n",
            "         checkpoint-200  weights=1\n",
            "         checkpoint-300  weights=1\n",
            "         checkpoint-400  weights=0\n",
            "         checkpoint-500  weights=0\n",
            "         checkpoint-600  weights=0\n",
            "         checkpoint-700  weights=0\n",
            "\n",
            "[/content/drive/MyDrive/wake2vec/sentry_backups] exists: True\n",
            "  t4_1762376560\n",
            "         checkpoint-300  weights=0\n",
            "         checkpoint-400  weights=0\n",
            "         checkpoint-500  weights=0\n",
            "         checkpoint-600  weights=0\n",
            "         checkpoint-700  weights=0\n",
            "\n",
            "[SNAPS] exists: True\n",
            "  t4_1762376560 | snaps: 9 | latest: emb_step0750.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace STEP with 400/500/600/700 etc. and RUN_ID as needed\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "STEP = 700\n",
        "ck = RUNS/RUN_ID/f\"checkpoint-{STEP}\"\n",
        "print(ck)\n",
        "print(\"\\nFiles:\")\n",
        "for p in sorted(ck.glob(\"*\")):\n",
        "    print(\" -\", p.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt_B_VGTChLa",
        "outputId": "967dfdbc-d221-4aad-e62e-fca038bc4418"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-700\n",
            "\n",
            "Files:\n",
            " - config.json\n",
            " - generation_config.json\n",
            " - optimizer.pt\n",
            " - rng_state.pth\n",
            " - scheduler.pt\n",
            " - special_tokens_map.json\n",
            " - tokenizer.json\n",
            " - tokenizer.model\n",
            " - tokenizer_config.json\n",
            " - trainer_state.json\n",
            " - training_args.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild a loadable checkpoint-750 from the embedding snapshot\n",
        "import pathlib, torch, shutil, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "BASE   = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS   = BASE/\"runs\"\n",
        "SENTRY = BASE/\"sentry_backups\"\n",
        "SNAPS  = BASE/\"emb_snaps\"\n",
        "\n",
        "# Pick RUN_ID from snapshots (since runs may be partial)\n",
        "RUN_IDS = sorted([p.name for p in SNAPS.glob(\"t4_*\")])\n",
        "assert RUN_IDS, \"No t4_* in emb_snaps — check Drive mount/account.\"\n",
        "RUN_ID = RUN_IDS[-1]\n",
        "print(\"[RUN_ID]\", RUN_ID)\n",
        "\n",
        "# Find the **last full** base checkpoint ≤ 750 (prefer sentry, then runs)\n",
        "def full_ckpts(root):\n",
        "    out = []\n",
        "    d = root/RUN_ID\n",
        "    if not d.exists(): return out\n",
        "    for ck in d.glob(\"checkpoint-*\"):\n",
        "        step = int(ck.name.split(\"-\")[-1])\n",
        "        has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists() \\\n",
        "                or list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\"))\n",
        "        if has_w: out.append((step, ck))\n",
        "    return sorted(out, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "bases = full_ckpts(SENTRY) + full_ckpts(RUNS)\n",
        "assert bases, \"No base checkpoints with weights found in sentry_backups/ or runs/.\"\n",
        "base_step, BASE_CK = next(((s, p) for s, p in bases if s <= 750), bases[-1])\n",
        "print(f\"[BASE] Using {BASE_CK} (step {base_step})\")\n",
        "\n",
        "# Load embedding snapshot @ 750\n",
        "EMB = SNAPS/RUN_ID/\"emb_step0750.pt\"\n",
        "assert EMB.exists(), \"emb_step0750.pt not found.\"\n",
        "emb = torch.load(EMB, map_location=\"cpu\")\n",
        "\n",
        "# Load base and inject embeddings; re-tie head\n",
        "tok = AutoTokenizer.from_pretrained(str(BASE_CK), use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(str(BASE_CK), torch_dtype=torch.float32, device_map=\"cpu\")\n",
        "with torch.no_grad():\n",
        "    model.get_input_embeddings().weight[:emb.size(0), :].copy_(emb)\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "print(\"[REBUILD] Injected emb_step0750 into base\")\n",
        "\n",
        "# Save as checkpoint-750-rebuilt (creates weights files)\n",
        "OUT_RUN    = RUNS/RUN_ID/\"checkpoint-750-rebuilt\"\n",
        "OUT_SENTRY = SENTRY/RUN_ID/\"checkpoint-750-rebuilt\"\n",
        "for d in (OUT_RUN, OUT_SENTRY):\n",
        "    if d.exists(): shutil.rmtree(d)\n",
        "model.save_pretrained(str(OUT_RUN), safe_serialization=True)\n",
        "tok.save_pretrained(str(OUT_RUN))\n",
        "shutil.copytree(OUT_RUN, OUT_SENTRY)\n",
        "print(\"[SAVED] →\", OUT_RUN)\n",
        "print(\"[MIRRORED] →\", OUT_SENTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhZdpEosENvp",
        "outputId": "68ca0380-f8fd-4dfe-affb-7e9d0482834b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN_ID] t4_1762376560\n",
            "[BASE] Using /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 (step 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REBUILD] Injected emb_step0750 into base\n",
            "[SAVED] → /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\n",
            "[MIRRORED] → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-750-rebuilt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "ck = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "has_w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists() \\\n",
        "        or list(ck.glob(\"model-*-of-*.safetensors\")) or list(ck.glob(\"pytorch_model-*-of-*.bin\"))\n",
        "print(\"750-rebuilt loadable:\", bool(has_w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkre3dB1Jysb",
        "outputId": "608a822e-8cdf-4544-b160-5779a458a781"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750-rebuilt loadable: True\n"
          ]
        }
      ]
    }
  ]
}