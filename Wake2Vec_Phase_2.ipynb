{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyPWNcrYHQRYhDjN4aF416U8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a778ca845884167be9ca2253bba85f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6e57b8d29be4deaa567e59de739e8e9",
              "IPY_MODEL_2c2e6c4a602d4e77af8e9a9b7bf33996",
              "IPY_MODEL_334a368193fb417da3d029ff09f91443"
            ],
            "layout": "IPY_MODEL_33961250d6b240239f5082e6daa2bee4"
          }
        },
        "b6e57b8d29be4deaa567e59de739e8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75bc276c8c3e4d15b38d4c63f76a3168",
            "placeholder": "​",
            "style": "IPY_MODEL_e606b78410be42baa4937a7ae24995dc",
            "value": "config.json: 100%"
          }
        },
        "2c2e6c4a602d4e77af8e9a9b7bf33996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b136767bd34e3a810bb2e988541f51",
            "max": 608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63538df0185049b5a9a4e11197e6f26a",
            "value": 608
          }
        },
        "334a368193fb417da3d029ff09f91443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1faaa8e18914f9fb2a3ea09ac098e77",
            "placeholder": "​",
            "style": "IPY_MODEL_a525858dceef4191b6276af3d9a4e2ac",
            "value": " 608/608 [00:00&lt;00:00, 31.8kB/s]"
          }
        },
        "33961250d6b240239f5082e6daa2bee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75bc276c8c3e4d15b38d4c63f76a3168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e606b78410be42baa4937a7ae24995dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0b136767bd34e3a810bb2e988541f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63538df0185049b5a9a4e11197e6f26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1faaa8e18914f9fb2a3ea09ac098e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a525858dceef4191b6276af3d9a4e2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5ac6f69f7044fc69cdb8dbacf77bda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37c22dbd643440db80bee415629de308",
              "IPY_MODEL_f2b10489ab0c4e1b94ae2028d705a19a",
              "IPY_MODEL_048385e45c42483ab70daabf3ac682db"
            ],
            "layout": "IPY_MODEL_8b4c7dbb52c44ac985c1115c51c83b5c"
          }
        },
        "37c22dbd643440db80bee415629de308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98318a3c0c204771b0011cdb1423f473",
            "placeholder": "​",
            "style": "IPY_MODEL_ca90e692f5514ff9a596ef38454d789f",
            "value": "model.safetensors: 100%"
          }
        },
        "f2b10489ab0c4e1b94ae2028d705a19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0183516c4764aa0b7314db63212dc16",
            "max": 2200119864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e283667263d41469903ef122e110301",
            "value": 2200119864
          }
        },
        "048385e45c42483ab70daabf3ac682db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c7897ea16524524b4c2e37304528a16",
            "placeholder": "​",
            "style": "IPY_MODEL_b5fb3a4cb9e44df383635e1e35cb5e78",
            "value": " 2.20G/2.20G [00:23&lt;00:00, 85.9MB/s]"
          }
        },
        "8b4c7dbb52c44ac985c1115c51c83b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98318a3c0c204771b0011cdb1423f473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca90e692f5514ff9a596ef38454d789f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0183516c4764aa0b7314db63212dc16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e283667263d41469903ef122e110301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c7897ea16524524b4c2e37304528a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5fb3a4cb9e44df383635e1e35cb5e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ff79617f63480987da126bf474aa8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4638af0d9a0452db61849afce3cdb24",
              "IPY_MODEL_2ad1abb110b34af5a0fcb732c2b72fbd",
              "IPY_MODEL_f9f0117b2bf447a1a9e60de66e26bc7f"
            ],
            "layout": "IPY_MODEL_e5e9e991ae574ac38a8656f7fc31fe03"
          }
        },
        "f4638af0d9a0452db61849afce3cdb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cef37ab81f84ae2b2e6337a334795ed",
            "placeholder": "​",
            "style": "IPY_MODEL_b02b15e24bde4be59f307e537848791f",
            "value": "generation_config.json: 100%"
          }
        },
        "2ad1abb110b34af5a0fcb732c2b72fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ebb404b78042b4847fb07524b60885",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16da5ac5df644467ac91a83e1a543f1e",
            "value": 124
          }
        },
        "f9f0117b2bf447a1a9e60de66e26bc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6fc75e0ef0411c8fe4616c5c615f95",
            "placeholder": "​",
            "style": "IPY_MODEL_275aafae2fad49939754a8ce66fadcde",
            "value": " 124/124 [00:00&lt;00:00, 7.56kB/s]"
          }
        },
        "e5e9e991ae574ac38a8656f7fc31fe03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cef37ab81f84ae2b2e6337a334795ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02b15e24bde4be59f307e537848791f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ebb404b78042b4847fb07524b60885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16da5ac5df644467ac91a83e1a543f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6fc75e0ef0411c8fe4616c5c615f95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "275aafae2fad49939754a8ce66fadcde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_Phase_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec Phase 2: Full Model Fine-Tune\n",
        "\n",
        "**Notebook:** `Wake2Vec Phase 2.ipynb`  \n",
        "**Model:** TinyLlama-1.1B  \n",
        "**Hardware:** Google Colab T4 GPU  \n",
        "\n",
        "## Overview\n",
        "\n",
        "Phase 2 performs full model fine-tuning on Finnegans Wake after P1's embedding-only warmup. Unlike P1, which trained only the expanded vocabulary embeddings, P2 unfreezes the entire model and uses LoRA adapters on attention and MLP layers to adapt the model's behavior to Wake's linguistic patterns.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Phase 1 completed (embeddings trained to step 1300)\n",
        "- P1 final artifacts saved in `/content/drive/MyDrive/wake2vecP1/final/`\n",
        "- Finnegans Wake text file uploaded (`FW_TEXT.txt`)\n",
        "\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. **Environment Setup** - Installs compatible package versions for Nov 2025 Colab\n",
        "2. **Load P1 State** - Loads tokenizer and trained embeddings from Phase 1\n",
        "3. **Data Split** - Creates 90/10 train/validation split from Finnegans Wake\n",
        "4. **Model Setup** - Initializes TinyLlama-1.1B with P1 embeddings and LoRA adapters\n",
        "5. **Training** - Fine-tunes for 2 epochs with validation monitoring and early stopping\n",
        "6. **Evaluation** - Generates loss curves and performance metrics\n",
        "\n",
        "### Key Differences from Phase 1\n",
        "\n",
        "| Aspect | Phase 1 | Phase 2 |\n",
        "|--------|---------|---------|\n",
        "| **Trainable params** | Embeddings only (~13M) | Full model (~1.1B + LoRA) |\n",
        "| **LoRA targets** | `q_proj` (frozen) | `q_proj`, `v_proj`, MLP layers |\n",
        "| **Learning rate** | 5e-4 | 2e-5 |\n",
        "| **Training duration** | 1300 steps | 2 epochs |\n",
        "| **Validation** | None | Held-out set with early stopping |\n",
        "| **Batch size** | 1 | 8 |\n",
        "| **Gradient accumulation** | 16 | 2 |\n",
        "| **Objective** | Warm up Wake embeddings | Adapt model behavior to Wake |\n",
        "\n",
        "## Hyperparameters\n",
        "```python\n",
        "EPOCHS = 2                    # Training epochs\n",
        "LR = 2e-5                     # Learning rate\n",
        "WARMUP_RATIO = 0.10           # 10% warmup\n",
        "BATCH_SIZE = 8                # Per-device batch size\n",
        "GRAD_ACCUM = 2                # Gradient accumulation steps\n",
        "WEIGHT_DECAY = 0.01           # L2 regularization\n",
        "SAVE_STEPS = 200              # Checkpoint frequency\n",
        "SEQ_LEN = 512                 # Sequence length (256 for safer memory)\n",
        "LORA_RANK = 8                 # LoRA adapter rank\n",
        "EARLY_STOP_PATIENCE = 2       # Early stopping patience\n",
        "```\n",
        "\n",
        "**Effective batch size:** 8 × 2 = 16 samples per optimizer step\n",
        "\n",
        "## Data\n",
        "\n",
        "### Dataset Split\n",
        "- **Total blocks:** ~1,740 (512 tokens each)\n",
        "- **Train blocks:** ~1,566 (90%)\n",
        "- **Validation blocks:** ~174 (10%)\n",
        "\n",
        "### Vocabulary\n",
        "- **Base tokenizer:** 32,000 tokens (TinyLlama-1.1B)\n",
        "- **Wake additions:** ~447-534 tokens (from P1)\n",
        "- **Final vocab size:** ~32,500-33,098 tokens\n",
        "\n",
        "### Input Format\n",
        "- **Sequence length:** 512 tokens (256 for P1 compatibility)\n",
        "- **Stride:** 512 tokens (non-overlapping blocks)\n",
        "- **Corpus:** Finnegans Wake plain text\n",
        "\n",
        "## Memory Budget (T4)\n",
        "\n",
        "| Component | VRAM Usage |\n",
        "|-----------|------------|\n",
        "| Model (4-bit quantized) | ~1.5 GB |\n",
        "| LoRA adapters (rank 8) | ~0.3 GB |\n",
        "| Optimizer states | ~2-3 GB |\n",
        "| Activations (batch=8) | ~4-5 GB |\n",
        "| **Total** | **~8-10 GB** |\n",
        "\n",
        "**T4 capacity:** 15 GB  \n",
        "**Safety margin:** ~5-7 GB (comfortable)\n",
        "\n",
        "### Fallback Options (if OOM)\n",
        "1. Reduce `SEQ_LEN` to 256 (P1 standard)\n",
        "2. Reduce `BATCH_SIZE` to 4 and increase `GRAD_ACCUM` to 4\n",
        "3. Reduce `LORA_RANK` to 4\n",
        "\n",
        "## Training Strategy\n",
        "\n",
        "### LoRA Configuration\n",
        "```python\n",
        "LoraConfig(\n",
        "    r=8,                    # Low rank\n",
        "    lora_alpha=16,          # 2× rank scaling\n",
        "    lora_dropout=0.1,       # Regularization\n",
        "    target_modules=[\n",
        "        \"q_proj\",           # Query projections\n",
        "        \"v_proj\",           # Value projections\n",
        "        \"gate_proj\",        # MLP gate\n",
        "        \"up_proj\",          \n",
        "        \"down_proj\"        \n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "### Training Loop\n",
        "- **Epochs:** 2 full passes through training data\n",
        "- **Validation:** Every 200 steps\n",
        "- **Checkpointing:** Save every 200 steps, keep best 3\n",
        "- **Early stopping:** Stop if validation loss doesn't improve for 2 checks\n",
        "- **Backups:** Automatic Drive mirroring via Sentry callback\n",
        "\n",
        "## Expected Outcomes\n",
        "\n",
        "### Performance Targets\n",
        "- **Training time:** 3-5 hours on T4 (2 epochs, faster than Llama-3.2-1B)\n",
        "- **Final validation loss:** < 2.5 (perplexity ~12)\n",
        "- **Convergence:** Within 2 epochs with early stopping\n",
        "\n",
        "### Quality Indicators\n",
        "- Validation loss decreases consistently\n",
        "- Model generates coherent Wake-style text\n",
        "- Wake tokens used appropriately in context\n",
        "- P1 embedding geometry preserved\n",
        "- No catastrophic forgetting of base vocabulary\n",
        "\n",
        "## File Structure\n",
        "```\n",
        "wake2vecP2/\n",
        "├── sentry_backups/          # Drive backups\n",
        "│   ├── checkpoint-200/\n",
        "│   ├── checkpoint-400/\n",
        "│   └── checkpoint-best/\n",
        "├── final/                   # Final model artifacts\n",
        "│   ├── adapter_model.safetensors\n",
        "│   ├── adapter_config.json\n",
        "│   ├── tokenizer_config.json\n",
        "│   └── special_tokens_map.json\n",
        "└── p2_loss_curve.png        # Training/validation plot\n",
        "```\n",
        "\n",
        "## Environment Notes (Nov 2025 Colab)\n",
        "\n",
        "This notebook uses aggressive package management to handle Colab's Nov 2025 updates:\n",
        "\n",
        "- **Default Colab:** torch 2.8.0, CUDA 12.9, JAX 0.7.2\n",
        "- **this stack:** torch 2.5.1+cu121, bitsandbytes 0.43.3, triton 3.1.0\n",
        "\n",
        "See `COLAB_NOV2025_UPDATE.md` for detailed version compatibility notes.\n",
        "\n",
        "**Last updated:** 2025-11-24  \n",
        "**Model:** TinyLlama-1.1B (1.1B parameters)  \n",
        "**Phase 1 completion:** Step 1300, loss ~0.079  \n",
        "**Next phase:** P3 (morpheme-aware regularization)"
      ],
      "metadata": {
        "id": "G_SNlws8J3_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Environment Setup (nov 2025 colab)"
      ],
      "metadata": {
        "id": "cj7nA4L-E9JV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLxagXGPEqXZ",
        "outputId": "98697f10-23e2-4081-fdea-fcf588122a13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Found existing installation: triton 3.4.0\n",
            "Uninstalling triton-3.4.0:\n",
            "  Successfully uninstalled triton-3.4.0\n",
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: transformers 4.57.0\n",
            "Uninstalling transformers-4.57.0:\n",
            "  Successfully uninstalled transformers-4.57.0\n",
            "Found existing installation: accelerate 1.10.1\n",
            "Uninstalling accelerate-1.10.1:\n",
            "  Successfully uninstalled accelerate-1.10.1\n",
            "Found existing installation: peft 0.17.1\n",
            "Uninstalling peft-0.17.1:\n",
            "  Successfully uninstalled peft-0.17.1\n",
            "Found existing installation: jax 0.5.3\n",
            "Uninstalling jax-0.5.3:\n",
            "  Successfully uninstalled jax-0.5.3\n",
            "Found existing installation: jaxlib 0.5.3\n",
            "Uninstalling jaxlib-0.5.3:\n",
            "  Successfully uninstalled jaxlib-0.5.3\n",
            "Found existing installation: flax 0.10.6\n",
            "Uninstalling flax-0.10.6:\n",
            "  Successfully uninstalled flax-0.10.6\n",
            "Found existing installation: cupy-cuda12x 13.3.0\n",
            "Uninstalling cupy-cuda12x-13.3.0:\n",
            "  Successfully uninstalled cupy-cuda12x-13.3.0\n",
            "Found existing installation: numba-cuda 0.11.0\n",
            "Uninstalling numba-cuda-0.11.0:\n",
            "  Successfully uninstalled numba-cuda-0.11.0\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n"
          ]
        }
      ],
      "source": [
        "# NUCLEAR OPT\n",
        "!pip uninstall -y torch torchvision torchaudio triton bitsandbytes transformers accelerate peft \\\n",
        "    jax jaxlib flax cupy-cuda12x numba-cuda -y\n",
        "!pip cache purge\n",
        "\n",
        "# rr\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compat versions"
      ],
      "metadata": {
        "id": "xQWIoO-hFGX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop TorchAO\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHAO\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# exact versions with explicit CUDA 12.1\n",
        "!pip install --no-cache-dir \\\n",
        "    torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 \\\n",
        "    --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -q --no-cache-dir \\\n",
        "    triton==3.1.0 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    transformers==4.45.2 \\\n",
        "    accelerate==0.34.2 \\\n",
        "    peft==0.13.2 \\\n",
        "    scikit-learn\n",
        "\n",
        "# verify\n",
        "import torch, bitsandbytes as bnb, triton\n",
        "print(\"=\"*60)\n",
        "print(\"PACKAGE VERSIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"torch: {torch.__version__} | cuda: {torch.version.cuda}\")\n",
        "print(f\"bitsandbytes: {bnb.__version__}\")\n",
        "print(f\"triton: {triton.__version__}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgyveIDcFKSx",
        "outputId": "21d5c09e-5e0e-4f7d-df4a-66151fb80e06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.5.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m221.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.20.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.5.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m223.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m196.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m150.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m198.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m294.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m305.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m201.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m332.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m322.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1+cu121) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1+cu121) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1+cu121) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1+cu121) (3.0.3)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m222.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m367.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m369.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m273.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h============================================================\n",
            "PACKAGE VERSIONS\n",
            "============================================================\n",
            "torch: 2.5.1+cu121 | cuda: 12.1\n",
            "bitsandbytes: 0.43.3\n",
            "triton: 3.1.0\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "verify bitsandbytes"
      ],
      "metadata": {
        "id": "3Bru6_NbFkZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify bitsandbytes CUDA\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "print(\"Verification:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# test 4-bit quantization works\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    test_layer = Linear4bit(10, 10, bias=False)\n",
        "    test_layer.cuda()\n",
        "    test_input = torch.randn(1, 10).cuda()\n",
        "    with torch.no_grad():\n",
        "        output = test_layer(test_input)\n",
        "    print(\"  ✓ bitsandbytes CUDA working!\")\n",
        "except Exception as e:\n",
        "    print(f\"  ✗ bitsandbytes test failed: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQVWL7JHtSc",
        "outputId": "1eb2d760-7e77-4f10-f2eb-fa4ca6e7986a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification:\n",
            "  CUDA available: True\n",
            "  CUDA device: Tesla T4\n",
            "  ✓ bitsandbytes CUDA working!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:569: UserWarning: Some matrices hidden dimension is not a multiple of 64 and efficient inference kernels are not supported for these (slow). Matrix input size found: torch.Size([1, 10])\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "drive and HF login"
      ],
      "metadata": {
        "id": "MvbSkV7SH3Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = getpass(\"Paste your HF token (hidden): \")\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAuSjClkH5f0",
        "outputId": "a4b6a221-b231-49dd-ed7e-4d10213c2df7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Paste your HF token (hidden): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check GPU"
      ],
      "metadata": {
        "id": "ewdGe62uH8A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# clean\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU Check:\")\n",
        "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHj5t2WgH-RS",
        "outputId": "a56fb39b-91dc-4cd5-d4b6-f0a48bcef92c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Check:\n",
            "  Device: Tesla T4\n",
            "  Memory: 15.83 GB\n",
            "  Allocated: 0.01 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get P1 final state"
      ],
      "metadata": {
        "id": "eAdmlI8gICre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# P1 paths - TinyLlama structure\n",
        "P1_ROOT = Path(\"/content/drive/MyDrive/wake2vecP1\")\n",
        "P1_TOKENIZER = P1_ROOT / \"checkpoint-0\"\n",
        "P1_EMBEDDINGS = P1_ROOT / \"emb_snaps/emb_step1300.pt\"\n",
        "\n",
        "# Check P1 artifacts exist\n",
        "if not P1_TOKENIZER.exists():\n",
        "    raise FileNotFoundError(f\"P1 tokenizer not found: {P1_TOKENIZER}\")\n",
        "if not P1_EMBEDDINGS.exists():\n",
        "    raise FileNotFoundError(f\"P1 embeddings not found: {P1_EMBEDDINGS}\")\n",
        "\n",
        "print(\"Loading P1 final state (step 1300)...\")\n",
        "\n",
        "# Load tokenizer (with Wake vocabulary)\n",
        "tok = AutoTokenizer.from_pretrained(str(P1_TOKENIZER), use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "print(f\"✓ Tokenizer loaded: {len(tok)} tokens\")\n",
        "\n",
        "# Load final embeddings from step 1300\n",
        "embed_weights = torch.load(P1_EMBEDDINGS, map_location=\"cpu\")\n",
        "print(f\"✓ Embeddings loaded: {embed_weights.shape}\")\n",
        "\n",
        "# Calculate vocab expansion\n",
        "BASE_VOCAB = 32000  # TinyLlama base vocab\n",
        "WAKE_TOKENS = len(tok) - BASE_VOCAB\n",
        "print(f\"✓ Wake tokens added in P1: {WAKE_TOKENS}\")\n",
        "print(f\"✓ Training completed at step 1300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbFrxPmmfO-1",
        "outputId": "3a01761a-5b1d-4370-94fd-3e9d3e9e886a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading P1 final state (step 1300)...\n",
            "✓ Tokenizer loaded: 76500 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2251160750.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  embed_weights = torch.load(P1_EMBEDDINGS, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embeddings loaded: torch.Size([76500, 2048])\n",
            "✓ Wake tokens added in P1: 44500\n",
            "✓ Training completed at step 1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2 config"
      ],
      "metadata": {
        "id": "EW2izJXVIPs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# P2 paths - matching TinyLlama naming convention\n",
        "RUN_DIR = Path(\"/content/drive/MyDrive/wake2vecP2\")\n",
        "LOCAL_RUN = Path(\"/content/runs/wake2vecP2\")\n",
        "SENTRY = RUN_DIR / \"sentry_backups\"\n",
        "\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training config\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "FW_TEXT = \"/content/FW_TEXT.txt\"\n",
        "\n",
        "# P2 hyperparameters\n",
        "MAX_STEPS = 2000\n",
        "LR = 2e-5\n",
        "WARMUP_RATIO = 0.10\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUM = 2\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_STEPS = 200\n",
        "SEQ_LEN = 256  # TinyLlama P1 used 256\n",
        "LORA_RANK = 8\n",
        "EARLY_STOP_PATIENCE = 2\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"WAKE2VEC PHASE 2 CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"P1 source: wake2vecP1 (step 1300)\")\n",
        "print(f\"P2 output: wake2vecP2\")\n",
        "print(f\"\")\n",
        "print(f\"Steps: {MAX_STEPS}\")\n",
        "print(f\"Learning rate: {LR}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Gradient accumulation: {GRAD_ACCUM}\")\n",
        "print(f\"Effective batch: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"LoRA rank: {LORA_RANK}\")\n",
        "print(f\"Sequence length: {SEQ_LEN}\")\n",
        "print(f\"Save every: {SAVE_STEPS} steps\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSVMfgHfidQ",
        "outputId": "be2c536a-7c2d-4ee9-d174-31f6e1a63df7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WAKE2VEC PHASE 2 CONFIGURATION\n",
            "============================================================\n",
            "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "P1 source: wake2vecP1 (step 1300)\n",
            "P2 output: wake2vecP2\n",
            "\n",
            "Steps: 2000\n",
            "Learning rate: 2e-05\n",
            "Batch size: 8\n",
            "Gradient accumulation: 2\n",
            "Effective batch: 16\n",
            "LoRA rank: 8\n",
            "Sequence length: 256\n",
            "Save every: 200 steps\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train/val split"
      ],
      "metadata": {
        "id": "5Qg1a4U-IWqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class BlockDataset(Dataset):\n",
        "    def __init__(self, blocks, tokenizer, seq_len=256):  # Changed default to 256\n",
        "        self.blocks = blocks\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.blocks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.tensor(self.blocks[idx], dtype=torch.long)\n",
        "        return {\n",
        "            \"input_ids\": ids,\n",
        "            \"labels\": ids.clone(),\n",
        "            \"attention_mask\": torch.ones_like(ids)\n",
        "        }\n",
        "\n",
        "# Load and tokenize full text\n",
        "print(\"Loading Finnegans Wake...\")\n",
        "if not os.path.exists(FW_TEXT):\n",
        "    raise FileNotFoundError(f\"FW text not found: {FW_TEXT}\")\n",
        "\n",
        "with open(FW_TEXT, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize\n",
        "ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# Create blocks with SEQ_LEN from config (256)\n",
        "blocks = []\n",
        "stride = SEQ_LEN\n",
        "for i in range(0, len(ids) - SEQ_LEN + 1, stride):\n",
        "    chunk = ids[i:i + SEQ_LEN]\n",
        "    if len(chunk) == SEQ_LEN:\n",
        "        blocks.append(chunk)\n",
        "\n",
        "print(f\"Total blocks: {len(blocks)}\")\n",
        "\n",
        "# Split 90/10 train/val\n",
        "train_blocks, val_blocks = train_test_split(\n",
        "    blocks,\n",
        "    test_size=0.10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train blocks: {len(train_blocks)}\")\n",
        "print(f\"Val blocks: {len(val_blocks)}\")\n",
        "\n",
        "# Create datasets\n",
        "train_ds = BlockDataset(train_blocks, tok, SEQ_LEN)\n",
        "val_ds = BlockDataset(val_blocks, tok, SEQ_LEN)\n",
        "\n",
        "print(f\"Datasets ready (seq_len={SEQ_LEN})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPsyBROafyKj",
        "outputId": "17c01413-fccf-47b4-c4f6-e8f69d55305d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Finnegans Wake...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (369716 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 369716\n",
            "Total blocks: 1444\n",
            "Train blocks: 1299\n",
            "Val blocks: 145\n",
            "Datasets ready (seq_len=256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get model with P1 embeds"
      ],
      "metadata": {
        "id": "0m2WGvCsIkJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    max_memory={0: \"13GB\", \"cpu\": \"30GB\"}\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "model.config.tie_word_embeddings = True\n",
        "if hasattr(model, \"tie_weights\"):\n",
        "    model.tie_weights()\n",
        "\n",
        "print(\"Base model loaded\")\n",
        "\n",
        "# resize embeddings to match P1 vocab\n",
        "print(f\"Resizing embeddings to {len(tok)}...\")\n",
        "model.resize_token_embeddings(len(tok))\n",
        "\n",
        "# load P1 embeds\n",
        "wte = model.get_input_embeddings()\n",
        "if hasattr(model, \"lm_head\"):\n",
        "    model.lm_head.weight = wte.weight\n",
        "\n",
        "with torch.no_grad():\n",
        "    wte.weight.copy_(embed_weights.to(wte.weight.device))\n",
        "\n",
        "print(\"P1 embeddings loaded\")\n",
        "\n",
        "# get LoRA adapters for P2\n",
        "print(\"Adding LoRA adapters...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_RANK * 2,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "3a778ca845884167be9ca2253bba85f7",
            "b6e57b8d29be4deaa567e59de739e8e9",
            "2c2e6c4a602d4e77af8e9a9b7bf33996",
            "334a368193fb417da3d029ff09f91443",
            "33961250d6b240239f5082e6daa2bee4",
            "75bc276c8c3e4d15b38d4c63f76a3168",
            "e606b78410be42baa4937a7ae24995dc",
            "d0b136767bd34e3a810bb2e988541f51",
            "63538df0185049b5a9a4e11197e6f26a",
            "c1faaa8e18914f9fb2a3ea09ac098e77",
            "a525858dceef4191b6276af3d9a4e2ac",
            "b5ac6f69f7044fc69cdb8dbacf77bda2",
            "37c22dbd643440db80bee415629de308",
            "f2b10489ab0c4e1b94ae2028d705a19a",
            "048385e45c42483ab70daabf3ac682db",
            "8b4c7dbb52c44ac985c1115c51c83b5c",
            "98318a3c0c204771b0011cdb1423f473",
            "ca90e692f5514ff9a596ef38454d789f",
            "e0183516c4764aa0b7314db63212dc16",
            "4e283667263d41469903ef122e110301",
            "4c7897ea16524524b4c2e37304528a16",
            "b5fb3a4cb9e44df383635e1e35cb5e78",
            "d2ff79617f63480987da126bf474aa8b",
            "f4638af0d9a0452db61849afce3cdb24",
            "2ad1abb110b34af5a0fcb732c2b72fbd",
            "f9f0117b2bf447a1a9e60de66e26bc7f",
            "e5e9e991ae574ac38a8656f7fc31fe03",
            "9cef37ab81f84ae2b2e6337a334795ed",
            "b02b15e24bde4be59f307e537848791f",
            "c7ebb404b78042b4847fb07524b60885",
            "16da5ac5df644467ac91a83e1a543f1e",
            "4e6fc75e0ef0411c8fe4616c5c615f95",
            "275aafae2fad49939754a8ce66fadcde"
          ]
        },
        "id": "H9FcY5MZImy8",
        "outputId": "59ed494e-b179-4471-9e09-e2ed39871dbc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a778ca845884167be9ca2253bba85f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5ac6f69f7044fc69cdb8dbacf77bda2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2ff79617f63480987da126bf474aa8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded\n",
            "Resizing embeddings to 76500...\n",
            "P1 embeddings loaded\n",
            "Adding LoRA adapters...\n",
            "trainable params: 5,181,440 || all params: 1,130,829,824 || trainable%: 0.4582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "call my daughter LoRA"
      ],
      "metadata": {
        "id": "Xgbelv7BguEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CRITICAL: Enable gradients on LoRA parameters\n",
        "print(\"Enabling gradients on trainable parameters...\")\n",
        "\n",
        "trainable_params = []\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params.append(name)\n",
        "\n",
        "print(f\"Trainable parameters: {len(trainable_params)}\")\n",
        "for name in trainable_params[:10]:  # Show first 10\n",
        "    print(f\"  {name}\")\n",
        "\n",
        "# Force enable gradients on LoRA adapters\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name.lower():\n",
        "        param.requires_grad = True\n",
        "        print(f\"✓ Enabled: {name}\")\n",
        "\n",
        "# Verify\n",
        "trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n✓ Total trainable parameters: {trainable_count:,}\")\n",
        "\n",
        "# Make sure at least some params are trainable\n",
        "if trainable_count == 0:\n",
        "    raise RuntimeError(\"No trainable parameters found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6g5c4DZgw9e",
        "outputId": "2a821e29-c52a-4651-8bfd-f18d4c2d83ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling gradients on trainable parameters...\n",
            "Trainable parameters: 220\n",
            "  base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "  base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "  base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "✓ Enabled: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "\n",
            "✓ Total trainable parameters: 5,181,440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainer with val"
      ],
      "metadata": {
        "id": "0QHn-cbCIx1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CRITICAL: Disable gradient checkpointing on the model\n",
        "# (TrainingArguments setting isn't enough for PEFT models)\n",
        "if hasattr(model, 'gradient_checkpointing_disable'):\n",
        "    model.gradient_checkpointing_disable()\n",
        "    print(\"Gradient checkpointing disabled on model\")\n",
        "\n",
        "# Verify\n",
        "print(f\"Model gradient checkpointing: {model.is_gradient_checkpointing}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8NQhCXlkC0F",
        "outputId": "bd23a4ad-1455-46ee-fee8-b5d9a8a7ed0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient checkpointing disabled on model\n",
            "Model gradient checkpointing: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback\n",
        "\n",
        "# Sentry backup callback\n",
        "def has_weights(ck):\n",
        "    return (ck / \"adapter_model.safetensors\").exists() or (ck / \"pytorch_model.bin\").exists()\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            cks = sorted(\n",
        "                LOCAL_RUN.glob(\"checkpoint-*\"),\n",
        "                key=lambda p: int(p.name.split(\"-\")[-1]),\n",
        "                reverse=True\n",
        "            )\n",
        "            if not cks:\n",
        "                return\n",
        "\n",
        "            ck = cks[0]\n",
        "            if not has_weights(ck):\n",
        "                print(f\"[SENTRY] {ck.name} no weights, skip\")\n",
        "                return\n",
        "\n",
        "            dst = SENTRY / ck.name\n",
        "            if not dst.exists():\n",
        "                print(f\"[SENTRY] Mirroring {ck.name}...\")\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] {ck.name} backed up to Drive\")\n",
        "            os.sync()\n",
        "        except Exception as e:\n",
        "            print(f\"[SENTRY] ERROR: {e}\")\n",
        "\n",
        "# Training arguments - CRITICAL: gradient_checkpointing=False\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    max_steps=MAX_STEPS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=False,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Trainer with early stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    callbacks=[\n",
        "        SentryMirror(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer ready\")\n",
        "print(f\"Training for {MAX_STEPS} epochs with validation every {SAVE_STEPS} steps\")\n",
        "print(f\" Gradient checkpointing DISABLED (required for 4-bit + LoRA)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OP_g6FliitN",
        "outputId": "8d251c01-0202-4f75-be94-98dfd58b3930"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer ready\n",
            "Training for 2000 epochs with validation every 200 steps\n",
            " Gradient checkpointing DISABLED (required for 4-bit + LoRA)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hit it"
      ],
      "metadata": {
        "id": "ZTzCOK5MI8My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"WAKE2VEC PHASE 2: FULL MODEL FINE-TUNE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Train samples: {len(train_ds)}\")\n",
        "print(f\"Val samples: {len(val_ds)}\")\n",
        "print(f\"Total steps: {MAX_STEPS}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# CRITICAL: Re-enable gradients right before training\n",
        "# call my daughter LoRA\n",
        "print(\"\\nRe-enabling LoRA gradients...\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name.lower():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Verify trainable params\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable:,}\")\n",
        "\n",
        "if trainable == 0:\n",
        "    raise RuntimeError(\"No trainable parameters! LoRA not enabled properly.\")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "DjJROSvTjDGQ",
        "outputId": "25876e60-dc81-442b-cc9b-fb53f45523e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "WAKE2VEC PHASE 2: FULL MODEL FINE-TUNE\n",
            "================================================================================\n",
            "Train samples: 1299\n",
            "Val samples: 145\n",
            "Total steps: 2000\n",
            "Effective batch size: 16\n",
            "================================================================================\n",
            "\n",
            "Re-enabling LoRA gradients...\n",
            "Trainable parameters: 5,181,440\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1035' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1035/2000 2:51:30 < 2:40:13, 0.10 it/s, Epoch 12.69/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.441400</td>\n",
              "      <td>5.338883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.895800</td>\n",
              "      <td>4.942523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>4.757600</td>\n",
              "      <td>4.855441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>4.612100</td>\n",
              "      <td>4.817270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.557100</td>\n",
              "      <td>4.805680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirroring checkpoint-200...\n",
            "[SENTRY] checkpoint-200 backed up to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirroring checkpoint-400...\n",
            "[SENTRY] checkpoint-400 backed up to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirroring checkpoint-600...\n",
            "[SENTRY] checkpoint-600 backed up to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirroring checkpoint-800...\n",
            "[SENTRY] checkpoint-800 backed up to Drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirroring checkpoint-1000...\n",
            "[SENTRY] checkpoint-1000 backed up to Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save"
      ],
      "metadata": {
        "id": "DUW9hQOOJCoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final model\n",
        "final_dir = RUN_DIR / \"final\"\n",
        "final_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Saving final P2 model...\")\n",
        "model.save_pretrained(str(final_dir))\n",
        "tok.save_pretrained(str(final_dir))\n",
        "\n",
        "print(f\"Model saved to {final_dir}\")\n",
        "print(f\"Phase 2 complete!\")"
      ],
      "metadata": {
        "id": "ZucJ8NnWJD3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval"
      ],
      "metadata": {
        "id": "yGgktwyuJHwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# load training history\n",
        "history_file = LOCAL_RUN / \"trainer_state.json\"\n",
        "if history_file.exists():\n",
        "    with open(history_file) as f:\n",
        "        state = json.load(f)\n",
        "\n",
        "    logs = state.get(\"log_history\", [])\n",
        "\n",
        "    # extract losses\n",
        "    train_loss = [(d[\"step\"], d[\"loss\"]) for d in logs if \"loss\" in d and \"eval_loss\" not in d]\n",
        "    val_loss = [(d[\"step\"], d[\"eval_loss\"]) for d in logs if \"eval_loss\" in d]\n",
        "\n",
        "    if train_loss and val_loss:\n",
        "        # plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        train_steps, train_losses = zip(*train_loss)\n",
        "        val_steps, val_losses = zip(*val_loss)\n",
        "\n",
        "        plt.plot(train_steps, train_losses, 'o-', label=\"Training Loss\", alpha=0.7)\n",
        "        plt.plot(val_steps, val_losses, 's-', label=\"Validation Loss\", alpha=0.7)\n",
        "        plt.xlabel(\"Steps\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Wake2Vec P2: Training & Validation Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plot_path = RUN_DIR / \"p2_loss_curve.png\"\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
        "        print(f\"✓ Plot saved: {plot_path}\")\n",
        "        plt.show()\n",
        "\n",
        "        # sum\n",
        "        print(\"\\nP2 Summary:\")\n",
        "        print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
        "        print(f\"  Final val loss: {val_losses[-1]:.4f}\")\n",
        "        print(f\"  Best val loss: {min(val_losses):.4f}\")"
      ],
      "metadata": {
        "id": "sAi1VweCJJAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}