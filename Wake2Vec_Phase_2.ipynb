{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdDbryWDYbdyIn5aEqJ+Eh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_Phase_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec Phase 2: Full Model Fine-Tune\n",
        "\n",
        "**Notebook:** `Wake2Vec Phase 2.ipynb`  \n",
        "**Model:** TinyLlama-1.1B  \n",
        "**Hardware:** Google Colab T4 GPU  \n",
        "\n",
        "## Overview\n",
        "\n",
        "Phase 2 performs full model fine-tuning on Finnegans Wake after P1's embedding-only warmup. Unlike P1, which trained only the expanded vocabulary embeddings, P2 unfreezes the entire model and uses LoRA adapters on attention and MLP layers to adapt the model's behavior to Wake's linguistic patterns.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Phase 1 completed (embeddings trained to step 1300)\n",
        "- P1 final artifacts saved in `/content/drive/MyDrive/wake2vecP1/final/`\n",
        "- Finnegans Wake text file uploaded (`FW_TEXT.txt`)\n",
        "\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. **Environment Setup** - Installs compatible package versions for Nov 2025 Colab\n",
        "2. **Load P1 State** - Loads tokenizer and trained embeddings from Phase 1\n",
        "3. **Data Split** - Creates 90/10 train/validation split from Finnegans Wake\n",
        "4. **Model Setup** - Initializes TinyLlama-1.1B with P1 embeddings and LoRA adapters\n",
        "5. **Training** - Fine-tunes for 2 epochs with validation monitoring and early stopping\n",
        "6. **Evaluation** - Generates loss curves and performance metrics\n",
        "\n",
        "### Key Differences from Phase 1\n",
        "\n",
        "| Aspect | Phase 1 | Phase 2 |\n",
        "|--------|---------|---------|\n",
        "| **Trainable params** | Embeddings only (~13M) | Full model (~1.1B + LoRA) |\n",
        "| **LoRA targets** | `q_proj` (frozen) | `q_proj`, `v_proj`, MLP layers |\n",
        "| **Learning rate** | 5e-4 | 2e-5 |\n",
        "| **Training duration** | 1300 steps | 2 epochs |\n",
        "| **Validation** | None | Held-out set with early stopping |\n",
        "| **Batch size** | 1 | 8 |\n",
        "| **Gradient accumulation** | 16 | 2 |\n",
        "| **Objective** | Warm up Wake embeddings | Adapt model behavior to Wake |\n",
        "\n",
        "## Hyperparameters\n",
        "```python\n",
        "EPOCHS = 2                    # Training epochs\n",
        "LR = 2e-5                     # Learning rate\n",
        "WARMUP_RATIO = 0.10           # 10% warmup\n",
        "BATCH_SIZE = 8                # Per-device batch size\n",
        "GRAD_ACCUM = 2                # Gradient accumulation steps\n",
        "WEIGHT_DECAY = 0.01           # L2 regularization\n",
        "SAVE_STEPS = 200              # Checkpoint frequency\n",
        "SEQ_LEN = 512                 # Sequence length (256 for safer memory)\n",
        "LORA_RANK = 8                 # LoRA adapter rank\n",
        "EARLY_STOP_PATIENCE = 2       # Early stopping patience\n",
        "```\n",
        "\n",
        "**Effective batch size:** 8 × 2 = 16 samples per optimizer step\n",
        "\n",
        "## Data\n",
        "\n",
        "### Dataset Split\n",
        "- **Total blocks:** ~1,740 (512 tokens each)\n",
        "- **Train blocks:** ~1,566 (90%)\n",
        "- **Validation blocks:** ~174 (10%)\n",
        "\n",
        "### Vocabulary\n",
        "- **Base tokenizer:** 32,000 tokens (TinyLlama-1.1B)\n",
        "- **Wake additions:** ~447-534 tokens (from P1)\n",
        "- **Final vocab size:** ~32,500-33,098 tokens\n",
        "\n",
        "### Input Format\n",
        "- **Sequence length:** 512 tokens (256 for P1 compatibility)\n",
        "- **Stride:** 512 tokens (non-overlapping blocks)\n",
        "- **Corpus:** Finnegans Wake plain text\n",
        "\n",
        "## Memory Budget (T4)\n",
        "\n",
        "| Component | VRAM Usage |\n",
        "|-----------|------------|\n",
        "| Model (4-bit quantized) | ~1.5 GB |\n",
        "| LoRA adapters (rank 8) | ~0.3 GB |\n",
        "| Optimizer states | ~2-3 GB |\n",
        "| Activations (batch=8) | ~4-5 GB |\n",
        "| **Total** | **~8-10 GB** |\n",
        "\n",
        "**T4 capacity:** 15 GB  \n",
        "**Safety margin:** ~5-7 GB (comfortable)\n",
        "\n",
        "### Fallback Options (if OOM)\n",
        "1. Reduce `SEQ_LEN` to 256 (P1 standard)\n",
        "2. Reduce `BATCH_SIZE` to 4 and increase `GRAD_ACCUM` to 4\n",
        "3. Reduce `LORA_RANK` to 4\n",
        "\n",
        "## Training Strategy\n",
        "\n",
        "### LoRA Configuration\n",
        "```python\n",
        "LoraConfig(\n",
        "    r=8,                    # Low rank\n",
        "    lora_alpha=16,          # 2× rank scaling\n",
        "    lora_dropout=0.1,       # Regularization\n",
        "    target_modules=[\n",
        "        \"q_proj\",           # Query projections\n",
        "        \"v_proj\",           # Value projections\n",
        "        \"gate_proj\",        # MLP gate\n",
        "        \"up_proj\",          \n",
        "        \"down_proj\"        \n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "### Training Loop\n",
        "- **Epochs:** 2 full passes through training data\n",
        "- **Validation:** Every 200 steps\n",
        "- **Checkpointing:** Save every 200 steps, keep best 3\n",
        "- **Early stopping:** Stop if validation loss doesn't improve for 2 checks\n",
        "- **Backups:** Automatic Drive mirroring via Sentry callback\n",
        "\n",
        "## Expected Outcomes\n",
        "\n",
        "### Performance Targets\n",
        "- **Training time:** 3-5 hours on T4 (2 epochs, faster than Llama-3.2-1B)\n",
        "- **Final validation loss:** < 2.5 (perplexity ~12)\n",
        "- **Convergence:** Within 2 epochs with early stopping\n",
        "\n",
        "### Quality Indicators\n",
        "- Validation loss decreases consistently\n",
        "- Model generates coherent Wake-style text\n",
        "- Wake tokens used appropriately in context\n",
        "- P1 embedding geometry preserved\n",
        "- No catastrophic forgetting of base vocabulary\n",
        "\n",
        "## File Structure\n",
        "```\n",
        "wake2vecP2/\n",
        "├── sentry_backups/          # Drive backups\n",
        "│   ├── checkpoint-200/\n",
        "│   ├── checkpoint-400/\n",
        "│   └── checkpoint-best/\n",
        "├── final/                   # Final model artifacts\n",
        "│   ├── adapter_model.safetensors\n",
        "│   ├── adapter_config.json\n",
        "│   ├── tokenizer_config.json\n",
        "│   └── special_tokens_map.json\n",
        "└── p2_loss_curve.png        # Training/validation plot\n",
        "```\n",
        "\n",
        "## Environment Notes (Nov 2025 Colab)\n",
        "\n",
        "This notebook uses aggressive package management to handle Colab's Nov 2025 updates:\n",
        "\n",
        "- **Default Colab:** torch 2.8.0, CUDA 12.9, JAX 0.7.2\n",
        "- **this stack:** torch 2.5.1+cu121, bitsandbytes 0.43.3, triton 3.1.0\n",
        "\n",
        "See `COLAB_NOV2025_UPDATE.md` for detailed version compatibility notes.\n",
        "\n",
        "**Last updated:** 2025-11-24  \n",
        "**Model:** TinyLlama-1.1B (1.1B parameters)  \n",
        "**Phase 1 completion:** Step 1300, loss ~0.079  \n",
        "**Next phase:** P3 (morpheme-aware regularization)"
      ],
      "metadata": {
        "id": "G_SNlws8J3_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Environment Setup (nov 2025 colab)"
      ],
      "metadata": {
        "id": "cj7nA4L-E9JV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLxagXGPEqXZ"
      },
      "outputs": [],
      "source": [
        "# NUCLEAR OPT\n",
        "!pip uninstall -y torch torchvision torchaudio triton bitsandbytes transformers accelerate peft \\\n",
        "    jax jaxlib flax cupy-cuda12x numba-cuda -y\n",
        "!pip cache purge\n",
        "\n",
        "# rr\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compat versions"
      ],
      "metadata": {
        "id": "xQWIoO-hFGX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop TorchAO\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHAO\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# exact versions with explicit CUDA 12.1\n",
        "!pip install --no-cache-dir \\\n",
        "    torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 \\\n",
        "    --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -q --no-cache-dir \\\n",
        "    triton==3.1.0 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    transformers==4.45.2 \\\n",
        "    accelerate==0.34.2 \\\n",
        "    peft==0.13.2 \\\n",
        "    scikit-learn\n",
        "\n",
        "# verify\n",
        "import torch, bitsandbytes as bnb, triton\n",
        "print(\"=\"*60)\n",
        "print(\"PACKAGE VERSIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"torch: {torch.__version__} | cuda: {torch.version.cuda}\")\n",
        "print(f\"bitsandbytes: {bnb.__version__}\")\n",
        "print(f\"triton: {triton.__version__}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "tgyveIDcFKSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "verify bitsandbytes"
      ],
      "metadata": {
        "id": "3Bru6_NbFkZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify bitsandbytes CUDA\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "print(\"Verification:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# test 4-bit quantization works\n",
        "try:\n",
        "    from bitsandbytes.nn import Linear4bit\n",
        "    test_layer = Linear4bit(10, 10, bias=False)\n",
        "    test_layer.cuda()\n",
        "    test_input = torch.randn(1, 10).cuda()\n",
        "    with torch.no_grad():\n",
        "        output = test_layer(test_input)\n",
        "    print(\"  ✓ bitsandbytes CUDA working!\")\n",
        "except Exception as e:\n",
        "    print(f\"  ✗ bitsandbytes test failed: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "WOQVWL7JHtSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "drive and HF login"
      ],
      "metadata": {
        "id": "MvbSkV7SH3Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = getpass(\"Paste your HF token (hidden): \")\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "fAuSjClkH5f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check GPU"
      ],
      "metadata": {
        "id": "ewdGe62uH8A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# clean\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU Check:\")\n",
        "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "QHj5t2WgH-RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get P1 final state"
      ],
      "metadata": {
        "id": "eAdmlI8gICre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# P1 paths\n",
        "P1_DIR = Path(\"/content/drive/MyDrive/wake_llama_P1/final\")\n",
        "\n",
        "# P1 artifacts\n",
        "if not P1_DIR.exists():\n",
        "    raise FileNotFoundError(f\"P1 final directory not found: {P1_DIR}\")\n",
        "\n",
        "print(\"Loading P1 final state...\")\n",
        "\n",
        "# tokenizer (with Wake vocab)\n",
        "tok = AutoTokenizer.from_pretrained(str(P1_DIR), use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "print(f\"✓ Tokenizer loaded: {len(tok)} tokens\")\n",
        "\n",
        "# load embeds\n",
        "embed_weights = torch.load(P1_DIR / \"embed_tokens.pt\", map_location=\"cpu\")\n",
        "print(f\"✓ Embeddings loaded: {embed_weights.shape}\")\n",
        "\n",
        "# calc vocab expansion\n",
        "BASE_VOCAB = 128256\n",
        "WAKE_TOKENS = len(tok) - BASE_VOCAB\n",
        "print(f\"✓ Wake tokens added in P1: {WAKE_TOKENS}\")"
      ],
      "metadata": {
        "id": "eYSMcajPIE5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2 config"
      ],
      "metadata": {
        "id": "EW2izJXVIPs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# P2 paths\n",
        "RUN_DIR = Path(\"/content/drive/MyDrive/wake_llama_P2\")\n",
        "LOCAL_RUN = Path(\"/content/runs/wake_llama_P2\")\n",
        "SENTRY = RUN_DIR / \"sentry_backups\"\n",
        "\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# training config\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "FW_TEXT = \"/content/FW_TEXT.txt\"\n",
        "\n",
        "# P2 hyperparameters\n",
        "EPOCHS = 2\n",
        "LR = 2e-5\n",
        "WARMUP_RATIO = 0.10\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUM = 2\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_STEPS = 200\n",
        "SEQ_LEN = 512\n",
        "LORA_RANK = 8\n",
        "EARLY_STOP_PATIENCE = 2\n",
        "\n",
        "print(\"P2 Configuration:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning rate: {LR}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {GRAD_ACCUM}\")\n",
        "print(f\"  LoRA rank: {LORA_RANK}\")\n",
        "print(f\"  Sequence length: {SEQ_LEN}\")"
      ],
      "metadata": {
        "id": "08HGCLsnIRkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train/val split"
      ],
      "metadata": {
        "id": "5Qg1a4U-IWqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class BlockDataset(Dataset):\n",
        "    def __init__(self, blocks, tokenizer, seq_len=512):\n",
        "        self.blocks = blocks\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.blocks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.tensor(self.blocks[idx], dtype=torch.long)\n",
        "        return {\n",
        "            \"input_ids\": ids,\n",
        "            \"labels\": ids.clone(),\n",
        "            \"attention_mask\": torch.ones_like(ids)\n",
        "        }\n",
        "\n",
        "# tokenize full text\n",
        "print(\"Loading Finnegans Wake...\")\n",
        "if not os.path.exists(FW_TEXT):\n",
        "    raise FileNotFoundError(f\"FW text not found: {FW_TEXT}\")\n",
        "\n",
        "with open(FW_TEXT, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "ids = tok(text, add_special_tokens=False)[\"input_ids\"]\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# blocks\n",
        "blocks = []\n",
        "stride = SEQ_LEN\n",
        "for i in range(0, len(ids) - SEQ_LEN + 1, stride):\n",
        "    chunk = ids[i:i + SEQ_LEN]\n",
        "    if len(chunk) == SEQ_LEN:\n",
        "        blocks.append(chunk)\n",
        "\n",
        "print(f\"Total blocks: {len(blocks)}\")\n",
        "\n",
        "# pplit 90/10 train/val\n",
        "train_blocks, val_blocks = train_test_split(\n",
        "    blocks,\n",
        "    test_size=0.10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train blocks: {len(train_blocks)}\")\n",
        "print(f\"Val blocks: {len(val_blocks)}\")\n",
        "\n",
        "# datasets\n",
        "train_ds = BlockDataset(train_blocks, tok, SEQ_LEN)\n",
        "val_ds = BlockDataset(val_blocks, tok, SEQ_LEN)\n",
        "\n",
        "print(f\"✓ Datasets ready\")"
      ],
      "metadata": {
        "id": "hhmpkLDmIWYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get model with P1 embeds"
      ],
      "metadata": {
        "id": "0m2WGvCsIkJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    max_memory={0: \"13GB\", \"cpu\": \"30GB\"}\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "model.config.tie_word_embeddings = True\n",
        "if hasattr(model, \"tie_weights\"):\n",
        "    model.tie_weights()\n",
        "\n",
        "print(\"✓ Base model loaded\")\n",
        "\n",
        "# resize embeddings to match P1 vocab\n",
        "print(f\"Resizing embeddings to {len(tok)}...\")\n",
        "model.resize_token_embeddings(len(tok))\n",
        "\n",
        "# load P1 embeds\n",
        "wte = model.get_input_embeddings()\n",
        "if hasattr(model, \"lm_head\"):\n",
        "    model.lm_head.weight = wte.weight\n",
        "\n",
        "with torch.no_grad():\n",
        "    wte.weight.copy_(embed_weights.to(wte.weight.device))\n",
        "\n",
        "print(\"✓ P1 embeddings loaded\")\n",
        "\n",
        "# get LoRA adapters for P2\n",
        "print(\"Adding LoRA adapters...\")\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_RANK * 2,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "H9FcY5MZImy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trainer with val"
      ],
      "metadata": {
        "id": "0QHn-cbCIx1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback\n",
        "\n",
        "# backup callback\n",
        "def has_weights(ck):\n",
        "    return (ck / \"adapter_model.safetensors\").exists() or (ck / \"pytorch_model.bin\").exists()\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            cks = sorted(\n",
        "                LOCAL_RUN.glob(\"checkpoint-*\"),\n",
        "                key=lambda p: int(p.name.split(\"-\")[-1]),\n",
        "                reverse=True\n",
        "            )\n",
        "            if not cks:\n",
        "                return\n",
        "\n",
        "            ck = cks[0]\n",
        "            if not has_weights(ck):\n",
        "                print(f\"[SENTRY] {ck.name} no weights, skip\")\n",
        "                return\n",
        "\n",
        "            dst = SENTRY / ck.name\n",
        "            if not dst.exists():\n",
        "                print(f\"[SENTRY] Mirroring {ck.name}...\")\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] {ck.name} backed up to Drive\")\n",
        "            os.sync()\n",
        "        except Exception as e:\n",
        "            print(f\"[SENTRY] ERROR: {e}\")\n",
        "\n",
        "# training arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    logging_steps=50,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# trainer, early stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    callbacks=[\n",
        "        SentryMirror(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer ready\")\n",
        "print(f\"Training for {EPOCHS} epochs with validation every {SAVE_STEPS} steps\")"
      ],
      "metadata": {
        "id": "vGCR4U0uIzUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hit it"
      ],
      "metadata": {
        "id": "ZTzCOK5MI8My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"WAKE2VEC PHASE 2: FULL MODEL FINE-TUNE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Train samples: {len(train_ds)}\")\n",
        "print(f\"Val samples: {len(val_ds)}\")\n",
        "print(f\"Total epochs: {EPOCHS}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# train\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "CrhI9eePI9T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save"
      ],
      "metadata": {
        "id": "DUW9hQOOJCoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final model\n",
        "final_dir = RUN_DIR / \"final\"\n",
        "final_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Saving final P2 model...\")\n",
        "model.save_pretrained(str(final_dir))\n",
        "tok.save_pretrained(str(final_dir))\n",
        "\n",
        "print(f\"✓ Model saved to {final_dir}\")\n",
        "print(f\"✓ Phase 2 complete!\")"
      ],
      "metadata": {
        "id": "ZucJ8NnWJD3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval"
      ],
      "metadata": {
        "id": "yGgktwyuJHwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# load training history\n",
        "history_file = LOCAL_RUN / \"trainer_state.json\"\n",
        "if history_file.exists():\n",
        "    with open(history_file) as f:\n",
        "        state = json.load(f)\n",
        "\n",
        "    logs = state.get(\"log_history\", [])\n",
        "\n",
        "    # extract losses\n",
        "    train_loss = [(d[\"step\"], d[\"loss\"]) for d in logs if \"loss\" in d and \"eval_loss\" not in d]\n",
        "    val_loss = [(d[\"step\"], d[\"eval_loss\"]) for d in logs if \"eval_loss\" in d]\n",
        "\n",
        "    if train_loss and val_loss:\n",
        "        # plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        train_steps, train_losses = zip(*train_loss)\n",
        "        val_steps, val_losses = zip(*val_loss)\n",
        "\n",
        "        plt.plot(train_steps, train_losses, 'o-', label=\"Training Loss\", alpha=0.7)\n",
        "        plt.plot(val_steps, val_losses, 's-', label=\"Validation Loss\", alpha=0.7)\n",
        "        plt.xlabel(\"Steps\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Wake2Vec P2: Training & Validation Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plot_path = RUN_DIR / \"p2_loss_curve.png\"\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
        "        print(f\"✓ Plot saved: {plot_path}\")\n",
        "        plt.show()\n",
        "\n",
        "        # sum\n",
        "        print(\"\\nP2 Summary:\")\n",
        "        print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
        "        print(f\"  Final val loss: {val_losses[-1]:.4f}\")\n",
        "        print(f\"  Best val loss: {min(val_losses):.4f}\")"
      ],
      "metadata": {
        "id": "sAi1VweCJJAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}