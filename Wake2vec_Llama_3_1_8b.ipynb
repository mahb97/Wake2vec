{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "mount_file_id": "19NmxnOQ7c82bZJaM5yckEiwfqjIwQMZn",
      "authorship_tag": "ABX9TyOIvU2ZoV4CbUquaxRsa1sB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af93a0c8f6bc4245aa8d28401e3d3ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdf9dad1c0e34d50a39d5640339d8039",
              "IPY_MODEL_54fb053e78714c0a981dcbd146ceadfc",
              "IPY_MODEL_d3e09fef714d472b81ac198809362105"
            ],
            "layout": "IPY_MODEL_0242769b4a794329971317dca987bb4c"
          }
        },
        "cdf9dad1c0e34d50a39d5640339d8039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2dfad9781c4098afd92a750bb44c5e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e5b475d092ba4be4a1ccf8b9050853c2",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "54fb053e78714c0a981dcbd146ceadfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98246972f11f4967ba92731f1b4903bb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7bc9941189745d8adc8afc575825054",
            "value": 2
          }
        },
        "d3e09fef714d472b81ac198809362105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fa3ab4e51ff43b3ae0cd40da2b4290a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e6ed08c1d1074798889c0d3186f90fc3",
            "value": "â€‡2/2â€‡[00:34&lt;00:00,â€‡15.36s/it]"
          }
        },
        "0242769b4a794329971317dca987bb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2dfad9781c4098afd92a750bb44c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b475d092ba4be4a1ccf8b9050853c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98246972f11f4967ba92731f1b4903bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7bc9941189745d8adc8afc575825054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fa3ab4e51ff43b3ae0cd40da2b4290a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ed08c1d1074798889c0d3186f90fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2vec_Llama_3_1_8b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wake2Vec â€œ*F* the Embeddingsâ€ (T4 Edn)\n",
        "This notebook is a colab-friendly, embedding-only finetune pipeline for large decoder LMs (Mistral-7B / Llama-2-13B / Llama-3.1-8B) using a Wake lexicon injection. It adds Joyce-specific tokens, initializes them on a sphere, and trains only the input embedding rows (optionally with a minimal LoRA r=1 on q_proj to satisfy quantized-training rules). The goal is to bend local geometry (neighbors, isotropy) while keeping the rest of the model frozen.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XsHKTTvNbso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# p2\n",
        "max_steps: 1500-2500\n",
        "\n",
        "lr: 5e-4 â†’ 1e-4 (cosine decay)\n",
        "\n",
        "batch_size: 1\n",
        "\n",
        "grad_accum: 16\n",
        "\n",
        "Custom loss = LM_loss + Î»â‚Â·attraction + Î»â‚‚Â·repulsion + Î»â‚ƒÂ·morphological + Î»â‚„Â·adversarial"
      ],
      "metadata": {
        "id": "NWn6AfTFVKGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "guardrail"
      ],
      "metadata": {
        "id": "Hxmoqws8wF_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NUCLEAR OPT\n",
        "!pip uninstall -y torch torchvision torchaudio triton bitsandbytes transformers accelerate peft fastai timm\n",
        "\n",
        "# rr\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-dZtk9WtPdcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324f9732-5aa7-42d9-8284-94a866d5e945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Found existing installation: triton 3.4.0\n",
            "Uninstalling triton-3.4.0:\n",
            "  Successfully uninstalled triton-3.4.0\n",
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: transformers 4.57.0\n",
            "Uninstalling transformers-4.57.0:\n",
            "  Successfully uninstalled transformers-4.57.0\n",
            "Found existing installation: accelerate 1.10.1\n",
            "Uninstalling accelerate-1.10.1:\n",
            "  Successfully uninstalled accelerate-1.10.1\n",
            "Found existing installation: peft 0.17.1\n",
            "Uninstalling peft-0.17.1:\n",
            "  Successfully uninstalled peft-0.17.1\n",
            "Found existing installation: fastai 2.8.4\n",
            "Uninstalling fastai-2.8.4:\n",
            "  Successfully uninstalled fastai-2.8.4\n",
            "Found existing installation: timm 1.0.20\n",
            "Uninstalling timm-1.0.20:\n",
            "  Successfully uninstalled timm-1.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop TorchAO\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHAO\"] = \"1\"\n",
        "\n",
        "# compat versions\n",
        "!pip install -q --no-cache-dir \\\n",
        "    torch==2.5.1 \\\n",
        "    triton==3.1.0 \\\n",
        "    bitsandbytes==0.43.3 \\\n",
        "    transformers==4.45.2 \\\n",
        "    accelerate==0.34.2 \\\n",
        "    peft==0.13.2\n",
        "\n",
        "# Verify\n",
        "import torch, bitsandbytes as bnb, triton\n",
        "print(\"torch:\", torch.__version__, \"| cuda:\", torch.version.cuda)\n",
        "print(\"bnb:\", bnb.__version__, \"| triton:\", triton.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dR0gBKNPmSd",
        "outputId": "164975dc-d23e-44e0-a5f1-34e20b5f35b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m282.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m230.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m181.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m379.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m353.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m239.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m250.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m417.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m221.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m237.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m215.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m217.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m202.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m181.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m232.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htorch: 2.5.1+cu124 | cuda: 12.4\n",
            "bnb: 0.43.3 | triton: 3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop TorchAO\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "from getpass import getpass\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = getpass(\"Paste your HF token (hidden): \")\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv0Oy84mIC99",
        "outputId": "9df5fb6c-27d3-4500-98b1-ab5279ec51ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your HF token (hidden): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Val dataset\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class BlockDataset(Dataset):\n",
        "    \"\"\"Sliding window dataset for causal LM training.\"\"\"\n",
        "    def __init__(self, txt_path, tokenizer, seq_len=512, stride=512):\n",
        "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        self.tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n",
        "        self.seq_len = seq_len\n",
        "        self.stride = stride\n",
        "        self.starts = list(range(0, len(self.tokens) - seq_len + 1, stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.starts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.starts[idx]\n",
        "        chunk = self.tokens[start:start + self.seq_len]\n",
        "        return {'input_ids': torch.tensor(chunk, dtype=torch.long)}\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Testing dataset creation...\")\n",
        "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\", use_fast=True)\n",
        "\n",
        "ds = BlockDataset(\"/content/FW_TEXT.txt\", tok, seq_len=512, stride=512)\n",
        "print(f\"  Dataset size: {len(ds)} blocks\")\n",
        "print(f\"  First block shape: {ds[0]['input_ids'].shape}\")\n",
        "\n",
        "print(\"\\nDataset validated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al0M_37DSO2y",
        "outputId": "ab5bb572-3211-48d2-ce7c-a58ef9f05105"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dataset creation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (378828 > 131072). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dataset size: 739 blocks\n",
            "  First block shape: torch.Size([512])\n",
            "\n",
            "Dataset validated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def read_lines(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "class BlockDataset(Dataset):\n",
        "    \"\"\"Sliding window dataset for causal LM training.\"\"\"\n",
        "    def __init__(self, txt_path, tokenizer, seq_len=512, stride=512):\n",
        "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        self.tokens = tokenizer(text, add_special_tokens=False)['input_ids']\n",
        "        self.seq_len = seq_len\n",
        "        self.stride = stride\n",
        "        self.starts = list(range(0, len(self.tokens) - seq_len + 1, stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.starts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.starts[idx]\n",
        "        chunk = self.tokens[start:start + self.seq_len]\n",
        "        return {'input_ids': torch.tensor(chunk, dtype=torch.long)}"
      ],
      "metadata": {
        "id": "yHRVOg8mROif"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-train\n",
        "import os\n",
        "\n",
        "print(\"GPU Check:\")\n",
        "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\nFile Check:\")\n",
        "files = {\n",
        "    \"Wake Lexicon\": \"/content/wake_lexicon.txt\",\n",
        "    \"FW Text\": \"/content/FW_TEXT.txt\"\n",
        "}\n",
        "for name, path in files.items():\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"Found\" if exists else \"MISSING\"\n",
        "    print(f\"  {name}: {status}\")\n",
        "    if exists:\n",
        "        size = os.path.getsize(path) / 1024\n",
        "        print(f\"    Size: {size:.1f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE2V5Kt9RUY4",
        "outputId": "4e71fbc2-3742-48ca-f3aa-d6c83dbdd25c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Check:\n",
            "  Device: Tesla T4\n",
            "  Memory: 15.83 GB\n",
            "  Allocated: 15.63 GB\n",
            "\n",
            "File Check:\n",
            "  Wake Lexicon: Found\n",
            "    Size: 403.0 KB\n",
            "  FW Text: Found\n",
            "    Size: 1358.4 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "# Verify clean slate\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "print(f\"GPU memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DtCox9CdX0D",
        "outputId": "a688d890-7883-41e8-8cc3-59fc696e47b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 10.42 GB\n",
            "GPU memory cached: 15.66 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# memory footprint\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B\"\n",
        "\n",
        "print(f\"Testing {MODEL_NAME} load on T4\")\n",
        "\n",
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "print(f\"  Vocab size: {len(tok)}\")\n",
        "\n",
        "print(\"\\nLoading model with 4-bit quantization...\")\n",
        "torch.cuda.empty_cache()\n",
        "initial_mem = torch.cuda.memory_allocated(0) / 1e9\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "loaded_mem = torch.cuda.memory_allocated(0) / 1e9\n",
        "print(f\"  Model loaded: {loaded_mem:.2f} GB\")\n",
        "print(f\"  Delta: {loaded_mem - initial_mem:.2f} GB\")\n",
        "\n",
        "# Validate forward pass\n",
        "print(\"\\nTesting forward pass...\")\n",
        "test_ids = torch.tensor([[1, 2, 3, 4, 5]], device=\"cuda\")\n",
        "with torch.no_grad():\n",
        "    out = model(test_ids)\n",
        "    print(f\"  Output shape: {out.logits.shape}\")\n",
        "    print(f\"  Memory after forward: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(f\"\\nPeak memory: {torch.cuda.max_memory_allocated(0) / 1e9:.2f} GB\")\n",
        "print(\"Model validated successfully\")\n",
        "\n",
        "# Cleanup\n",
        "del model, tok\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Memory cleared for main run\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "af93a0c8f6bc4245aa8d28401e3d3ced",
            "cdf9dad1c0e34d50a39d5640339d8039",
            "54fb053e78714c0a981dcbd146ceadfc",
            "d3e09fef714d472b81ac198809362105",
            "0242769b4a794329971317dca987bb4c",
            "1e2dfad9781c4098afd92a750bb44c5e",
            "e5b475d092ba4be4a1ccf8b9050853c2",
            "98246972f11f4967ba92731f1b4903bb",
            "e7bc9941189745d8adc8afc575825054",
            "7fa3ab4e51ff43b3ae0cd40da2b4290a",
            "e6ed08c1d1074798889c0d3186f90fc3"
          ]
        },
        "id": "D3pkobp4SAnn",
        "outputId": "9cd5fec3-8cf2-483a-acca-5b3384f12efe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing meta-llama/Llama-3.2-3B load on T4\n",
            "\n",
            "Loading tokenizer...\n",
            "  Vocab size: 128256\n",
            "\n",
            "Loading model with 4-bit quantization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af93a0c8f6bc4245aa8d28401e3d3ced"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model loaded: 9.07 GB\n",
            "  Delta: -1.35 GB\n",
            "\n",
            "Testing forward pass...\n",
            "  Output shape: torch.Size([1, 5, 128256])\n",
            "  Memory after forward: 9.08 GB\n",
            "\n",
            "Peak memory: 15.63 GB\n",
            "Model validated successfully\n",
            "Memory cleared for main run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, json, random, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
        "                          TrainingArguments, Trainer, set_seed)\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED=42; set_seed(SEED)\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
        "WAKE_LEX_PATH = \"/content/wake_lexicon.txt\"\n",
        "CORPUS_TXT    = \"/content/finnegans_wake.txt\"\n",
        "RUN_DIR       = \"/content/wake_llama_embs\"\n",
        "SEQ_LEN=512; STRIDE=512\n",
        "MAX_STEPS=1100; LOG_STEPS=20; SAVE_STEPS=200\n",
        "LR=5e-5                     # â†“ safer than 8e-4 for fp16 stability, adjusted for stability\n",
        "GRAD_ACCUM=8\n",
        "REPULSION_W=0.0\n",
        "TARGET_NORM=None\n",
        "MAX_ROW_NORM=None\n",
        "REPORT_SAMPLE=1500\n",
        "\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "# 4-bit load (bnb)\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                         bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, # Changed to bfloat16 for T4 compatibility\n",
        "                         llm_int8_enable_fp32_cpu_offload=True)\n",
        "\n",
        "# pass token=HF_TOKEN if the repo is gated\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, quantization_config=bnb, torch_dtype=torch.bfloat16, device_map=\"auto\", # Changed to bfloat16 for T4 compatibility\n",
        "    max_memory={0: \"13GB\", \"cpu\": \"30GB\"}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "model.config.tie_word_embeddings = True\n",
        "if hasattr(model, \"tie_weights\"): model.tie_weights()\n",
        "\n",
        "# tiny PEFT adapter (kept FROZEN)\n",
        "peft_cfg = LoraConfig(r=1, lora_alpha=1, lora_dropout=0.0,\n",
        "                      target_modules=[\"q_proj\"], bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "for n,p in model.named_parameters(): p.requires_grad=False\n",
        "\n",
        "# inject wake lexicon\n",
        "def read_lines(p):\n",
        "    return [x.strip() for x in open(p, encoding=\"utf-8\") if x.strip()] if os.path.exists(p) else []\n",
        "wake = read_lines(WAKE_LEX_PATH)\n",
        "missing = [t for t in wake if tok.convert_tokens_to_ids(t)==tok.unk_token_id]\n",
        "num_added = tok.add_tokens(missing, special_tokens=False)\n",
        "\n",
        "old_vocab = model.get_input_embeddings().weight.shape[0]\n",
        "model.resize_token_embeddings(len(tok))\n",
        "wte = model.get_input_embeddings()\n",
        "if hasattr(model, \"lm_head\"): model.lm_head.weight = wte.weight\n",
        "\n",
        "# spherical kick init\n",
        "with torch.no_grad():\n",
        "    base = wte.weight[:old_vocab]; dim = base.shape[1]\n",
        "    std = base.std().item(); base_radius = std * math.sqrt(dim)\n",
        "    target_radius = TARGET_NORM or (1.5 * base_radius)\n",
        "    if num_added>0:\n",
        "        new = torch.randn((num_added, dim), device=wte.weight.device)\n",
        "        new = new/(new.norm(dim=1, keepdim=True)+1e-8)*target_radius\n",
        "        wte.weight.data[old_vocab:old_vocab+num_added] = new\n",
        "\n",
        "# trainables = ONLY embeds\n",
        "wte.weight.requires_grad=True\n",
        "new_rows = torch.arange(old_vocab, old_vocab+num_added, device=wte.weight.device) if num_added>0 else None\n",
        "base_rows = torch.arange(0, old_vocab, device=wte.weight.device)\n",
        "def mask_grad(grad):\n",
        "    if grad is None or new_rows is None: return grad\n",
        "    grad[base_rows]=0; return grad\n",
        "wte.weight.register_hook(mask_grad)\n",
        "\n",
        "def clamp_rows_(emb, max_norm):\n",
        "    if max_norm is None or new_rows is None: return\n",
        "    rows = emb.weight.data[old_vocab:old_vocab+num_added]\n",
        "    norms = rows.norm(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "    scale = (max_norm/norms).clamp_max(1.0)\n",
        "    emb.weight.data[old_vocab:old_vocab+num_added] = rows*scale\n",
        "\n",
        "# robust torch-only dataset\n",
        "class BlockDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, seq_len=512, stride=512):\n",
        "        if not os.path.exists(path):\n",
        "            stub = (\"riverrun, past Eve and Adamâ€™s, from swerve of shore to bend of bay, \"\n",
        "                    \"brings us by a commodius vicus of recirculation to Howth Castle and Environs. \")*2000 # Increased stub length for more data if path is empty\n",
        "            text = stub\n",
        "        else:\n",
        "            text = open(path, \"r\", encoding=\"utf-8\").read()\n",
        "        ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
        "        blocks=[]\n",
        "        # More robust chunking logic:\n",
        "        for i in range(0, max(1, len(ids)-seq_len), stride):\n",
        "            chunk = ids[i:i+seq_len]\n",
        "            if len(chunk) >= seq_len//2: # Only append if chunk is at least half the seq_len\n",
        "                blocks.append(chunk[:seq_len])\n",
        "        self.blocks = blocks\n",
        "    def __len__(self): return len(self.blocks)\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.tensor(self.blocks[idx], dtype=torch.long)\n",
        "        return {\"input_ids\": ids, \"labels\": ids.clone(), \"attention_mask\": torch.ones_like(ids)}\n",
        "\n",
        "train_ds = BlockDataset(CORPUS_TXT, tok, SEQ_LEN, STRIDE)\n",
        "print(f\"[Data] chunks={len(train_ds)}; tokens/step={SEQ_LEN}\")\n",
        "\n",
        "# tiny sanity\n",
        "sample = train_ds[0]\n",
        "assert sample[\"input_ids\"].ndim==1 and sample[\"labels\"].ndim==1\n",
        "assert sample[\"input_ids\"].shape[0]==SEQ_LEN and sample[\"labels\"].shape[0]==SEQ_LEN\n",
        "assert sample[\"input_ids\"].sum().item()!=0, \"Input looks empty; check CORPUS_TXT or tokenizer.\"\n",
        "\n",
        "# geometry receipts (pre)\n",
        "@torch.no_grad()\n",
        "def isotropy(M):\n",
        "    u, s, v = torch.pca_lowrank(M - M.mean(0, keepdim=True), q=min(128, M.shape[1]-1))\n",
        "    return float((s.max() / s.min().clamp_min(1e-8)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def pip_loss(A, B):\n",
        "    return float(torch.norm((A@A.T)-(B@B.T), p='fro')/(A.shape[0]**2))\n",
        "\n",
        "@torch.no_grad()\n",
        "def topk_overlap(M1, M2, k=10, sample=1000):\n",
        "    W1 = M1/(M1.norm(dim=1, keepdim=True)+1e-8); W2 = M2/(M2.norm(dim=1, keepdim=True)+1e-8)\n",
        "    vocab = W1.shape[0]; idxs = random.sample(range(vocab), min(sample, vocab))\n",
        "    acc = 0.0\n",
        "    for i in idxs:\n",
        "        c1 = torch.topk(W1 @ W1[i], k+1).indices.tolist(); c1=[j for j in c1 if j!=i][:k]\n",
        "        c2 = torch.topk(W2 @ W2[i], k+1).indices.tolist(); c2=[j for j in c2 if j!=i][:k]\n",
        "        acc += len(set(c1)&set(c2))/k\n",
        "    return float(acc/len(idxs))\n",
        "\n",
        "with torch.no_grad():\n",
        "    pre_full = wte.weight.detach().clone().cpu()\n",
        "    pre_new  = pre_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, pre_full.shape[1])\n",
        "\n",
        "# custom Trainer to\n",
        "#(a) only optimize embeddings,\n",
        "#(b) add repulsion,\n",
        "#(c) clamp,\n",
        "#(d) clip grads\n",
        "from transformers import Trainer\n",
        "class EmbOnlyTrainer(Trainer):\n",
        "    def create_optimizer(self):\n",
        "        from torch.optim import AdamW\n",
        "        if not hasattr(self, \"optimizer\") or self.optimizer is None:\n",
        "            self.optimizer = AdamW([{\"params\": [wte.weight], \"lr\": LR, \"weight_decay\": 0.0}],\n",
        "                                   betas=(0.9, 0.999), eps=1e-8)\n",
        "        return self.optimizer\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        out = model(**inputs, use_cache=False)\n",
        "        loss = out.loss\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            raise RuntimeError(\"NaN/Inf loss detected â€” check data/labels.\")\n",
        "        if num_added and num_added>1 and REPULSION_W>0:\n",
        "            E = model.get_input_embeddings().weight[old_vocab:old_vocab+num_added]\n",
        "            E = E - E.mean(0, keepdim=True); E = E/(E.norm(dim=1, keepdim=True)+1e-8)\n",
        "            sims = (E @ E.t()); repul = (sims - torch.eye(E.shape[0], device=E.device)).pow(2).mean()\n",
        "            loss = loss + REPULSION_W*repul\n",
        "        return (loss, out) if return_outputs else loss\n",
        "    def training_step(self, *args, **kwargs):\n",
        "        out = super().training_step(*args, **kwargs)\n",
        "        clamp_rows_(model.get_input_embeddings(), MAX_ROW_NORM)\n",
        "        return out\n",
        "\n",
        "steps_per_epoch = max(1, len(train_ds) // GRAD_ACCUM)\n",
        "print(f\"[Epochs] steps/epochâ‰ˆ{steps_per_epoch} | epochs@{MAX_STEPS}â‰ˆ{MAX_STEPS/steps_per_epoch:.2f}\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=RUN_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_steps=max(20, MAX_STEPS//20),\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.0,\n",
        "    fp16=False, bf16=True, # Aligning with bfloat16 compute dtype for T4\n",
        "    logging_steps=LOG_STEPS,\n",
        "    save_steps=SAVE_STEPS, save_total_limit=6,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "trainer = EmbOnlyTrainer(model=model, args=args, train_dataset=train_ds, data_collator=None)\n",
        "print(f\"[Run] base={MODEL_NAME} | steps={MAX_STEPS} | seq_len={SEQ_LEN} | accum={GRAD_ACCUM} | LR={LR}\")\n",
        "trainer.train()\n",
        "\n",
        "# save artifacts\n",
        "save_dir = os.path.join(RUN_DIR, \"embedding_only\"); os.makedirs(save_dir, exist_ok=True)\n",
        "torch.save(wte.weight.detach().cpu(), os.path.join(save_dir, \"embed_tokens.pt\"))\n",
        "with open(os.path.join(save_dir, \"added_tokens.json\"), \"w\") as f:\n",
        "    json.dump({\"added_tokens\": missing, \"old_vocab\": old_vocab, \"num_added\": num_added}, f, indent=2)\n",
        "tok.save_pretrained(RUN_DIR)\n",
        "\n",
        "# receipts (post)\n",
        "with torch.no_grad():\n",
        "    post_full = wte.weight.detach().clone().cpu()\n",
        "    post_new  = post_full[old_vocab:old_vocab+num_added].clone() if num_added>0 else torch.empty(0, pre_full.shape[1])\n",
        "\n",
        "report = {\n",
        "  \"model\": MODEL_NAME, \"added_tokens\": int(num_added), \"old_vocab\": int(old_vocab),\n",
        "  \"pip_loss_full\": pip_loss(pre_full, post_full),\n",
        "  \"topk_overlap_all\": topk_overlap(pre_full, post_full, k=10, sample=min(REPORT_SAMPLE, pre_full.shape[0]-1)),\n",
        "  \"isotropy_pre\": isotropy(pre_full), \"isotropy_post\": isotropy(post_full),\n",
        "  \"pip_loss_new_rows\": (pip_loss(pre_new, post_new) if num_added>1 else None),\n",
        "  \"isotropy_new_rows\": (isotropy(post_new) if num_added>1 else None),\n",
        "}\n",
        "json.dump(report, open(os.path.join(RUN_DIR, \"geometry_report.json\"), \"w\"), indent=2)\n",
        "print(\"\\n=== GEOMETRY REPORT ===\")\n",
        "for k,v in report.items(): print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "GPwmUl3-Fq86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a83b91d6-da76-4e29-fb24-c9e1226c1737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data] chunks=167; tokens/step=512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epochs] steps/epochâ‰ˆ20 | epochs@1100â‰ˆ55.00\n",
            "[Run] base=meta-llama/Llama-3.2-1B | steps=1100 | seq_len=512 | accum=8 | LR=5e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  38/1100 09:34 < 4:42:37, 0.06 it/s, Epoch 1.77/55]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.148400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}