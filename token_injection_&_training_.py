# -*- coding: utf-8 -*-
"""token_injection_&_training_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4vMjFjRzd4a3AnbpGWvgpdhWzS_ybTT

# Wake2vec Token Injection & Training
"""

from google.colab import drive; drive.mount('/content/drive')
from datetime import datetime
from pathlib import Path

RUN_ID = datetime.now().strftime("wake2vec_%Y%m%d_%H%M")
BASE = Path(f"/content/drive/MyDrive/Wake2vec_runs/{RUN_ID}")
ADAPTER_DIR = BASE/'adapter'; RESULTS = BASE/'results'
for p in (ADAPTER_DIR, RESULTS): p.mkdir(parents=True, exist_ok=True)
print("Saving to:", BASE)

"""model + tok"""

BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # CPU fallback: "distilgpt2"
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

"""Add tokens, resize, tie head"""

# Load lex
from pathlib import Path, PurePath
LEX_PATH = Path("/content/drive/MyDrive/Wake2vec_runs/wake_lexicon.txt")  # adjust if needed
lex = [t.strip() for t in LEX_PATH.read_text(encoding="utf-8").splitlines() if t.strip()]

new_terms = []
for t in lex:
    if not t: continue
    new_terms.append(t)
    if not t.startswith("▁"):
        new_terms.append("▁"+t)

# add tokens
added = tok.add_tokens(list(dict.fromkeys(new_terms)))
print("Added tokens:", added)
model.resize_token_embeddings(len(tok))

# tie lm_head to input embeddings
if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
    # after tok.add_tokens(...)
try:
    model.resize_token_embeddings(len(tok), mean_resizing=False)
except TypeError:
    # older Transformers without the flag
    model.resize_token_embeddings(len(tok))

# immediately tie head to inputs
with torch.no_grad():
    if model.get_output_embeddings() is not None:
        model.get_output_embeddings().weight = model.get_input_embeddings().weight

"""Make embeddings trainable"""

# freeze everything
for p in model.parameters():
    p.requires_grad = False

# unfreeze input embeddings (KEY)
emb = model.get_input_embeddings()
emb.weight.requires_grad_(True)

# Opt LoRA tiny
USE_LORA = True
if USE_LORA:
    from peft import LoraConfig, get_peft_model, TaskType
    lcfg = LoraConfig(
        task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05,
        target_modules=["q_proj","k_proj","v_proj","o_proj"]
    )
    model = get_peft_model(model, lcfg)
    model.print_trainable_parameters()
def enable_embeddings_and_lora_only(model):
    # freeze everything
    for _, p in model.named_parameters():
        p.requires_grad = False

    # turn on LoRA params
    for name, p in model.named_parameters():
        if "lora_" in name:
            p.requires_grad = True

    # turn on input embeddings
    emb = model.get_input_embeddings()
    emb.weight.requires_grad_(True)

    # report
    total = sum(p.numel() for p in model.parameters())
    train = sum(p.numel() for p in model.parameters() if p.requires_grad)
    emb_train = emb.weight.numel() if emb.weight.requires_grad else 0
    print(f"Total params: {total:,} | Trainable: {train:,}")
    print(f"Embeddings trainable? {emb.weight.requires_grad} | embed rows: {emb_train:,}")

# AFTER LoRA wrap & AFTER any tokenizer resize/tie
enable_embeddings_and_lora_only(model)

# trainability report
total = sum(p.numel() for p in model.parameters())
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total params: {total:,} | Trainable: {trainable:,}")
print("Embeddings trainable?", emb.weight.requires_grad)

for n,p in model.named_parameters():
    if p.requires_grad and ("embed" in n or "lora_" in n):
        print(" •", n, p.shape)

baseline_dir = RESULTS / "baseline_now"
baseline_dir.mkdir(parents=True, exist_ok=True)

anchors = ["river","history","book","dream","night","language","irish","dublin","cork"]
probe = [t for t in new_terms[:150] if tok.convert_tokens_to_ids(t)!=tok.unk_token_id] + anchors
ids = [tok.convert_tokens_to_ids(t) for t in probe]
W = model.get_input_embeddings().weight.detach().cpu().numpy()
import numpy as np, json
P0 = W[ids] @ W[ids].T
np.save(baseline_dir/"probe_ids.npy", np.array(ids, dtype=np.int32))
np.save(baseline_dir/"P0.npy", P0)
(baseline_dir/"probe_tokens.json").write_text(json.dumps(probe, ensure_ascii=False, indent=2))
print("Baseline saved to", baseline_dir)

"""Wake-dense dataset"""

from datasets import Dataset, DatasetDict
import re, unicodedata

RAW_WAKE = Path("/content/drive/MyDrive/Wake2vec_runs/fw.txt").read_text(encoding="utf-8")
txt = unicodedata.normalize("NFC", RAW_WAKE)
# anchor at 'riverrun'
i = txt.lower().find("riverrun")
if i > 0: txt = txt[i:]

paras = [re.sub(r"\s+"," ",p.strip()) for p in re.split(r"\n\s*\n", txt) if len(p.split())>6]
RADIUS = 2
windows = []
for idx in range(len(paras)):
    lo, hi = max(0, idx-RADIUS), min(len(paras), idx+RADIUS+1)
    windows.append(" ".join(paras[lo:hi]))

def tok_map(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding=False)

split = int(0.95*len(windows))
ds = DatasetDict({
    "train": Dataset.from_dict({"text": windows[:split]}).map(tok_map, batched=True, remove_columns=["text"]),
    "validation": Dataset.from_dict({"text": windows[split:]}).map(tok_map, batched=True, remove_columns=["text"]),
})
from transformers import default_data_collator as collator
print(ds)

"""snapshot before training"""

import numpy as np, json, time
BAS_DIR = RESULTS / f"baseline_{int(time.time())}"
BAS_DIR.mkdir(parents=True, exist_ok=True)

anchors = ["river","history","book","dream","night","language","irish","dublin","cork"]
probe_new = [t for t in lex if tok.convert_tokens_to_ids(t)!=tok.unk_token_id][:150]
probe = probe_new + anchors
ids = [tok.convert_tokens_to_ids(t) for t in probe]

W = model.get_input_embeddings().weight.detach().cpu().numpy()
P0 = W[ids] @ W[ids].T
np.save(BAS_DIR/"probe_ids.npy", np.array(ids, dtype=np.int32))
np.save(BAS_DIR/"P0.npy", P0)
(BAS_DIR/"probe_tokens.json").write_text(json.dumps(probe, ensure_ascii=False, indent=2))
print("Baseline saved to", BAS_DIR)

"""Train"""

emb = model.get_input_embeddings().weight
print("Embeddings trainable?", emb.requires_grad)

# how many of the *new* tokens exist + will learn?
new_ids = [tok.convert_tokens_to_ids(t) for t in new_terms]
new_ids = sorted({i for i in new_ids if i != tok.unk_token_id})
print("New token rows:", len(new_ids))
print("Added trainable params from embeddings ~", len(new_ids)*emb.shape[1])

# Make sure the tokenizer can pad (use EOS as PAD)
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
    model.config.pad_token_id = tok.pad_token_id

# Tokenize WITHOUT padding
def tok_map(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding=False)

ds = DatasetDict({
    "train": Dataset.from_dict({"text": windows[:split]}).map(tok_map, batched=True, remove_columns=["text"]),
    "validation": Dataset.from_dict({"text": windows[split:]}).map(tok_map, batched=True, remove_columns=["text"]),
})

# Use causal-LM collator
from transformers import DataCollatorForLanguageModeling
collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, pad_to_multiple_of=8)

MAX_LEN = 384
def tok_map(batch): return tok(batch["text"], truncation=True, max_length=MAX_LEN, padding=False)

# collator
from transformers import DataCollatorForLanguageModeling
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
    model.config.pad_token_id = tok.pad_token_id
collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, pad_to_multiple_of=8)

from transformers import TrainingArguments
args = TrainingArguments(
    output_dir=str(ADAPTER_DIR),
    per_device_train_batch_size=1,      # ↓
    gradient_accumulation_steps=16,      # ↑ keeps effective batch
    learning_rate=6e-4,
    num_train_epochs=3,
    fp16=False, bf16=False,              # ← no AMP, no GradScaler
    logging_steps=25,

    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,

    lr_scheduler_type="cosine",
    warmup_ratio=0.10,
    max_grad_norm=0.0,
    eval_accumulation_steps=16,
    gradient_checkpointing=True,
    report_to=[]
)
model.gradient_checkpointing_enable()
model.config.use_cache = False

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds["train"],
    eval_dataset=ds["validation"],
    data_collator=collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
trainer.train()
# save final to Drive
trainer.save_model(str(ADAPTER_DIR/"final"))
tok.save_pretrained(str(ADAPTER_DIR/"final_tok"))
print("Saved to", ADAPTER_DIR)

ck = ADAPTER_DIR/"manual_ckpt_epoch{:.2f}".format(trainer.state.epoch or 0.0)
ck.mkdir(parents=True, exist_ok=True)
trainer.save_model(str(ck))
tok.save_pretrained(str(ck/"tok"))
print("Saved:", ck)

if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
model.config.use_cache = True

"""if t4 disconnects"""

# point to the *latest* dir inside ADAPTER_DIR
last = sorted(ADAPTER_DIR.glob("*"), key=lambda p: p.stat().st_mtime)[-1]
print("Resuming from:", last)
trainer.train(resume_from_checkpoint=str(last))

"""Metrics + Hero"""

# retie for generation
if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
model.config.use_cache = True

import numpy as np, pandas as pd, json, glob
import umap, matplotlib.pyplot as plt, textwrap, os

# load most recent baseline from RESULTS
bdirs = sorted([p for p in RESULTS.glob("baseline_*")], key=lambda p: p.stat().st_mtime)
ids = np.load(bdirs[-1]/"probe_ids.npy"); P0 = np.load(bdirs[-1]/"P0.npy")
W = model.get_input_embeddings().weight.detach().cpu().numpy()
P1 = W[ids] @ W[ids].T

pip = float(np.linalg.norm(P0 - P1, 'fro') / (np.linalg.norm(P0, 'fro') + 1e-12))
X = W[ids] - W[ids].mean(0, keepdims=True)
C = (X.T @ X) / max(1, len(ids)-1)
evals = np.clip(np.linalg.eigvalsh(C), 1e-12, None)
isotropy = float(np.exp(np.log(evals).mean()) / evals.mean())
topk=10
N0 = np.argsort(-P0,axis=1)[:,:topk]; N1 = np.argsort(-P1,axis=1)[:,:topk]
overlap = float(np.mean([len(set(N0[i]).intersection(set(N1[i]))) / topk for i in range(len(ids))]))
pd.DataFrame([{"pip_loss":pip,"isotropy":isotropy,"top10_overlap":overlap,"num_probe_tokens":int(len(ids))}]).to_csv(RESULTS/"metrics_summary.csv", index=False)
print("Wrote", RESULTS/"metrics_summary.csv")

# Hero
probe_show = list(dict.fromkeys(lex[:40] + ["river","history","book","dream","night","language","irish","dublin"]))
probe_ids = [tok.convert_tokens_to_ids(t) for t in probe_show if tok.convert_tokens_to_ids(t)!=tok.unk_token_id]
vecs = model.get_input_embeddings().weight[probe_ids].detach().cpu().numpy()
xy = umap.UMAP(n_neighbors=8, min_dist=0.1, metric="cosine", random_state=42).fit_transform(vecs)

def complete_portfolio(prompt, max_new_tokens=120):
    ins = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(**ins, max_new_tokens=max_new_tokens, do_sample=True,
                             temperature=0.90, top_p=0.88, repetition_penalty=1.2,
                             no_repeat_ngram_size=4, cache_implementation="static")
    return tok.decode(out[0], skip_special_tokens=True)

samples = {
  "By the river I thought of — Earwicker, the prankquean, and": complete_portfolio("By the river I thought of — Earwicker, the prankquean, and", 120),
  "In the story of this book, HCE remembers that": complete_portfolio("In the story of this book, HCE remembers that", 120),
  "Explain gradient descent in the style of Joyce: riverrun,": complete_portfolio("Explain gradient descent in the style of Joyce: riverrun,", 120),
}

plt.figure(figsize=(10,6))
plt.subplot(1,2,1); plt.scatter(xy[:,0], xy[:,1])
inv = {i:t for t,i in tok.get_vocab().items()}
for pid,(x,y) in zip(probe_ids, xy): plt.text(x,y,inv.get(int(pid),"."),fontsize=8)
plt.title("Wake2vec: neighborhood map (UMAP)")
plt.subplot(1,2,2); y0=1.0
for k,v in samples.items():
    plt.text(0.0,y0,k+"\n"+textwrap.fill(v.replace("\n"," "), width=44), fontsize=8, va='top'); y0-=0.32
plt.axis('off'); plt.title("Sample completions")
plt.tight_layout(); plt.savefig(RESULTS/"hero.png", dpi=200)
print("Saved", RESULTS/"hero.png")

"""tbc"""