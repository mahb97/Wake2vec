# -*- coding: utf-8 -*-
"""token_injection_&_training_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4vMjFjRzd4a3AnbpGWvgpdhWzS_ybTT

# Wake2vec Token Injection & Training
"""

from google.colab import drive; drive.mount('/content/drive')
from datetime import datetime
from pathlib import Path

RUN_ID = datetime.now().strftime("wake2vec_%Y%m%d_%H%M")
BASE = Path(f"/content/drive/MyDrive/Wake2vec_runs/{RUN_ID}")
ADAPTER_DIR = BASE/'adapter'; RESULTS = BASE/'results'
for p in (ADAPTER_DIR, RESULTS): p.mkdir(parents=True, exist_ok=True)
print("Saving to:", BASE)

"""model + tok"""

BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # CPU fallback: "distilgpt2"
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

"""Add tokens, resize, tie head"""

# Load lex
from pathlib import Path, PurePath
LEX_PATH = Path("/content/drive/MyDrive/Wake2vec_runs/wake_lexicon.txt")  # adjust if needed
lex = [t.strip() for t in LEX_PATH.read_text(encoding="utf-8").splitlines() if t.strip()]

new_terms = []
for t in lex:
    if not t: continue
    new_terms.append(t)
    if not t.startswith("▁"):
        new_terms.append("▁"+t)

# add tokens
added = tok.add_tokens(list(dict.fromkeys(new_terms)))
print("Added tokens:", added)
model.resize_token_embeddings(len(tok))

# tie lm_head to input embeddings
if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
    # after tok.add_tokens(...)
try:
    model.resize_token_embeddings(len(tok), mean_resizing=False)
except TypeError:
    # older Transformers without the flag
    model.resize_token_embeddings(len(tok))

# immediately tie head to inputs
with torch.no_grad():
    if model.get_output_embeddings() is not None:
        model.get_output_embeddings().weight = model.get_input_embeddings().weight

"""Make embeddings trainable"""

# freeze everything
for p in model.parameters():
    p.requires_grad = False

# unfreeze input embeddings (KEY)
emb = model.get_input_embeddings()
emb.weight.requires_grad_(True)

# Opt LoRA tiny
USE_LORA = True
if USE_LORA:
    from peft import LoraConfig, get_peft_model, TaskType
    lcfg = LoraConfig(
        task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05,
        target_modules=["q_proj","k_proj","v_proj","o_proj"]
    )
    model = get_peft_model(model, lcfg)
    model.print_trainable_parameters()
def enable_embeddings_and_lora_only(model):
    # freeze everything
    for _, p in model.named_parameters():
        p.requires_grad = False

    # turn on LoRA params
    for name, p in model.named_parameters():
        if "lora_" in name:
            p.requires_grad = True

    # turn on input embeddings
    emb = model.get_input_embeddings()
    emb.weight.requires_grad_(True)

    # report
    total = sum(p.numel() for p in model.parameters())
    train = sum(p.numel() for p in model.parameters() if p.requires_grad)
    emb_train = emb.weight.numel() if emb.weight.requires_grad else 0
    print(f"Total params: {total:,} | Trainable: {train:,}")
    print(f"Embeddings trainable? {emb.weight.requires_grad} | embed rows: {emb_train:,}")

# AFTER LoRA wrap & AFTER any tokenizer resize/tie
enable_embeddings_and_lora_only(model)

# trainability report
total = sum(p.numel() for p in model.parameters())
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total params: {total:,} | Trainable: {trainable:,}")
print("Embeddings trainable?", emb.weight.requires_grad)

for n,p in model.named_parameters():
    if p.requires_grad and ("embed" in n or "lora_" in n):
        print(" •", n, p.shape)

baseline_dir = RESULTS / "baseline_now"
baseline_dir.mkdir(parents=True, exist_ok=True)

anchors = ["river","history","book","dream","night","language","irish","dublin","cork"]
probe = [t for t in new_terms[:150] if tok.convert_tokens_to_ids(t)!=tok.unk_token_id] + anchors
ids = [tok.convert_tokens_to_ids(t) for t in probe]
W = model.get_input_embeddings().weight.detach().cpu().numpy()
import numpy as np, json
P0 = W[ids] @ W[ids].T
np.save(baseline_dir/"probe_ids.npy", np.array(ids, dtype=np.int32))
np.save(baseline_dir/"P0.npy", P0)
(baseline_dir/"probe_tokens.json").write_text(json.dumps(probe, ensure_ascii=False, indent=2))
print("Baseline saved to", baseline_dir)

"""Wake-dense dataset"""

from datasets import Dataset, DatasetDict
import re, unicodedata

RAW_WAKE = Path("/content/drive/MyDrive/Wake2vec_runs/fw.txt").read_text(encoding="utf-8")
txt = unicodedata.normalize("NFC", RAW_WAKE)
# anchor at 'riverrun'
i = txt.lower().find("riverrun")
if i > 0: txt = txt[i:]

paras = [re.sub(r"\s+"," ",p.strip()) for p in re.split(r"\n\s*\n", txt) if len(p.split())>6]
RADIUS = 2
windows = []
for idx in range(len(paras)):
    lo, hi = max(0, idx-RADIUS), min(len(paras), idx+RADIUS+1)
    windows.append(" ".join(paras[lo:hi]))

def tok_map(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding=False)

split = int(0.95*len(windows))
ds = DatasetDict({
    "train": Dataset.from_dict({"text": windows[:split]}).map(tok_map, batched=True, remove_columns=["text"]),
    "validation": Dataset.from_dict({"text": windows[split:]}).map(tok_map, batched=True, remove_columns=["text"]),
})
from transformers import default_data_collator as collator
print(ds)

"""snapshot before training"""

import numpy as np, json, time
BAS_DIR = RESULTS / f"baseline_{int(time.time())}"
BAS_DIR.mkdir(parents=True, exist_ok=True)

anchors = ["river","history","book","dream","night","language","irish","dublin","cork"]
probe_new = [t for t in lex if tok.convert_tokens_to_ids(t)!=tok.unk_token_id][:150]
probe = probe_new + anchors
ids = [tok.convert_tokens_to_ids(t) for t in probe]

W = model.get_input_embeddings().weight.detach().cpu().numpy()
P0 = W[ids] @ W[ids].T
np.save(BAS_DIR/"probe_ids.npy", np.array(ids, dtype=np.int32))
np.save(BAS_DIR/"P0.npy", P0)
(BAS_DIR/"probe_tokens.json").write_text(json.dumps(probe, ensure_ascii=False, indent=2))
print("Baseline saved to", BAS_DIR)

"""Train"""

emb = model.get_input_embeddings().weight
print("Embeddings trainable?", emb.requires_grad)

# how many of the *new* tokens exist + will learn?
new_ids = [tok.convert_tokens_to_ids(t) for t in new_terms]
new_ids = sorted({i for i in new_ids if i != tok.unk_token_id})
print("New token rows:", len(new_ids))
print("Added trainable params from embeddings ~", len(new_ids)*emb.shape[1])

# Make sure the tokenizer can pad (use EOS as PAD)
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
    model.config.pad_token_id = tok.pad_token_id

# Tokenize WITHOUT padding
def tok_map(batch):
    return tok(batch["text"], truncation=True, max_length=512, padding=False)

ds = DatasetDict({
    "train": Dataset.from_dict({"text": windows[:split]}).map(tok_map, batched=True, remove_columns=["text"]),
    "validation": Dataset.from_dict({"text": windows[split:]}).map(tok_map, batched=True, remove_columns=["text"]),
})

# Use causal-LM collator
from transformers import DataCollatorForLanguageModeling
collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, pad_to_multiple_of=8)

MAX_LEN = 384
def tok_map(batch): return tok(batch["text"], truncation=True, max_length=MAX_LEN, padding=False)

# collator
from transformers import DataCollatorForLanguageModeling
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
    model.config.pad_token_id = tok.pad_token_id
collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False, pad_to_multiple_of=8)

from transformers import TrainingArguments
args = TrainingArguments(
    output_dir=str(ADAPTER_DIR),
    per_device_train_batch_size=1,      # ↓
    gradient_accumulation_steps=16,      # ↑ keeps effective batch
    learning_rate=6e-4,
    num_train_epochs=3,
    fp16=False, bf16=False,              # ← no AMP, no GradScaler
    logging_steps=25,

    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,

    lr_scheduler_type="cosine",
    warmup_ratio=0.10,
    max_grad_norm=0.0,
    eval_accumulation_steps=16,
    gradient_checkpointing=True,
    report_to=[]
)
model.gradient_checkpointing_enable()
model.config.use_cache = False

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds["train"],
    eval_dataset=ds["validation"],
    data_collator=collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
trainer.train()
# save final to Drive
trainer.save_model(str(ADAPTER_DIR/"final"))
tok.save_pretrained(str(ADAPTER_DIR/"final_tok"))
print("Saved to", ADAPTER_DIR)

ck = ADAPTER_DIR/"manual_ckpt_epoch{:.2f}".format(trainer.state.epoch or 0.0)
ck.mkdir(parents=True, exist_ok=True)
trainer.save_model(str(ck))
tok.save_pretrained(str(ck/"tok"))
print("Saved:", ck)

if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
model.config.use_cache = True

"""if t4 disconnects"""

# point to the *latest* dir inside ADAPTER_DIR
last = sorted(ADAPTER_DIR.glob("*"), key=lambda p: p.stat().st_mtime)[-1]
print("Resuming from:", last)
trainer.train(resume_from_checkpoint=str(last))

"""Metrics + Hero"""

# retie for generation
if model.get_output_embeddings() is not None:
    model.get_output_embeddings().weight = model.get_input_embeddings().weight
model.config.use_cache = True

import numpy as np, pandas as pd, json, glob
import umap, matplotlib.pyplot as plt, textwrap, os

# load most recent baseline from RESULTS
bdirs = sorted([p for p in RESULTS.glob("baseline_*")], key=lambda p: p.stat().st_mtime)
ids = np.load(bdirs[-1]/"probe_ids.npy"); P0 = np.load(bdirs[-1]/"P0.npy")
W = model.get_input_embeddings().weight.detach().cpu().numpy()
P1 = W[ids] @ W[ids].T

pip = float(np.linalg.norm(P0 - P1, 'fro') / (np.linalg.norm(P0, 'fro') + 1e-12))
X = W[ids] - W[ids].mean(0, keepdims=True)
C = (X.T @ X) / max(1, len(ids)-1)
evals = np.clip(np.linalg.eigvalsh(C), 1e-12, None)
isotropy = float(np.exp(np.log(evals).mean()) / evals.mean())
topk=10
N0 = np.argsort(-P0,axis=1)[:,:topk]; N1 = np.argsort(-P1,axis=1)[:,:topk]
overlap = float(np.mean([len(set(N0[i]).intersection(set(N1[i]))) / topk for i in range(len(ids))]))
pd.DataFrame([{"pip_loss":pip,"isotropy":isotropy,"top10_overlap":overlap,"num_probe_tokens":int(len(ids))}]).to_csv(RESULTS/"metrics_summary.csv", index=False)
print("Wrote", RESULTS/"metrics_summary.csv")

# Hero
probe_show = list(dict.fromkeys(lex[:40] + ["river","history","book","dream","night","language","irish","dublin"]))
probe_ids = [tok.convert_tokens_to_ids(t) for t in probe_show if tok.convert_tokens_to_ids(t)!=tok.unk_token_id]
vecs = model.get_input_embeddings().weight[probe_ids].detach().cpu().numpy()
xy = umap.UMAP(n_neighbors=8, min_dist=0.1, metric="cosine", random_state=42).fit_transform(vecs)

def complete_portfolio(prompt, max_new_tokens=120):
    ins = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(**ins, max_new_tokens=max_new_tokens, do_sample=True,
                             temperature=0.90, top_p=0.88, repetition_penalty=1.2,
                             no_repeat_ngram_size=4, cache_implementation="static")
    return tok.decode(out[0], skip_special_tokens=True)

samples = {
  "By the river I thought of — Earwicker, the prankquean, and": complete_portfolio("By the river I thought of — Earwicker, the prankquean, and", 120),
  "In the story of this book, HCE remembers that": complete_portfolio("In the story of this book, HCE remembers that", 120),
  "Explain gradient descent in the style of Joyce: riverrun,": complete_portfolio("Explain gradient descent in the style of Joyce: riverrun,", 120),
}

plt.figure(figsize=(10,6))
plt.subplot(1,2,1); plt.scatter(xy[:,0], xy[:,1])
inv = {i:t for t,i in tok.get_vocab().items()}
for pid,(x,y) in zip(probe_ids, xy): plt.text(x,y,inv.get(int(pid),"."),fontsize=8)
plt.title("Wake2vec: neighborhood map (UMAP)")
plt.subplot(1,2,2); y0=1.0
for k,v in samples.items():
    plt.text(0.0,y0,k+"\n"+textwrap.fill(v.replace("\n"," "), width=44), fontsize=8, va='top'); y0-=0.32
plt.axis('off'); plt.title("Sample completions")
plt.tight_layout(); plt.savefig(RESULTS/"hero.png", dpi=200)
print("Saved", RESULTS/"hero.png")

"""tbc"""


import torch, math, textwrap, os, json
from pathlib import Path
from transformers import LogitsProcessor, LogitsProcessorList, NoRepeatNGramLogitsProcessor, RepetitionPenaltyLogitsProcessor

# Logit scaling (boost/suppress token sets)
class LogitScaler(LogitsProcessor):
    def __init__(self, ids, delta): self.ids=set(int(i) for i in ids); self.delta=float(delta)
    def __call__(self, input_ids, scores):
        if self.ids: scores[:, list(self.ids)] += self.delta
        return scores

# Adaptive temperature (entropy-targeted)
class AdaptiveTemp(LogitsProcessor):
    def __init__(self, base_T=0.9, target_H=3.0, k=0.15, T_min=0.6, T_max=1.3):
        self.base_T, self.target_H, self.k, self.T_min, self.T_max = base_T, target_H, k, T_min, T_max
    def __call__(self, input_ids, scores):
        p = torch.softmax(scores, dim=-1)
        H = -(p * (p.clamp_min(1e-12)).log()).sum(-1, keepdim=True)
        T = (self.base_T * torch.exp(self.k * (H - self.target_H))).clamp(self.T_min, self.T_max)
        return scores / T

# Per-token temperature (hot/cool word sets)
class PerTokenTemp(LogitsProcessor):
    def __init__(self, hot_ids, cool_ids, Thot=1.15, Tcool=0.85):
        self.hot, self.cool = set(hot_ids), set(cool_ids); self.Th, self.Tc = float(Thot), float(Tcool)
    def __call__(self, input_ids, scores):
        if self.hot:  scores[:, list(self.hot)]  /= self.Th
        if self.cool: scores[:, list(self.cool)] /= self.Tc
        return scores

# Gate stopword loops
class GateStops(LogitsProcessor):
    def __init__(self, stop_ids, penalty=5.0): self.stop=set(stop_ids); self.penalty=float(penalty)
    def __call__(self, input_ids, scores):
        last = int(input_ids[0, -1])
        if last in self.stop: scores[:, list(self.stop)] -= self.penalty
        return scores

# Entmax wrapper (optional)
try:
    from entmax import entmax15
    class EntmaxAsLogits(LogitsProcessor):
        def __call__(self, input_ids, scores):
            p = entmax15(scores, dim=-1)
            return p.clamp_min(1e-12).log()  # re-logitify for downstream processors
    ENTMAX_OK = True
except Exception:
    ENTMAX_OK = False

# Logit blend (Wake vs Base)
class LogitBlend(LogitsProcessor):
    def __init__(self, base_model, lam=0.2):
        self.base = base_model; self.lam=float(lam)
    @torch.no_grad()
    def __call__(self, input_ids, scores):
        base_scores = self.base(input_ids=input_ids).logits[:, -1, :]
        return (1 - self.lam) * scores + self.lam * base_scores

# Builder
def build_portfolio_processors(tok, new_terms, *,
                               wake_boost=0.35, stop_suppress=0.25,
                               use_adaptive_temp=True, base_T=0.9, target_H=3.0, k=0.15,
                               per_token_temp=True, Thot=1.12, Tcool=0.88,
                               gate_stops=True, gate_penalty=5.0,
                               use_entmax=False,
                               use_blend=False, base_model=None, blend_lambda=0.2):
    # Wake ids (+ ▁ variants)
    wake_ids=set()
    for t in new_terms:
        for form in (t, "▁"+t if not t.startswith("▁") else t):
            tid = tok.convert_tokens_to_ids(form)
            if tid != tok.unk_token_id: wake_ids.add(tid)

    # tiny stopword set
    stopish = "the of and to a in that it is you i me we he she they my your his her our their for on as with was be are this at from or so if but by have has had not do did does can will would could should".split()
    stop_ids = {tok.convert_tokens_to_ids(w) for w in stopish if tok.convert_tokens_to_ids(w)!=tok.unk_token_id}

    procs = LogitsProcessorList([
        RepetitionPenaltyLogitsProcessor(1.20),
        NoRepeatNGramLogitsProcessor(4),
        LogitScaler(wake_ids, +wake_boost),
        LogitScaler(stop_ids, -stop_suppress),
    ])

    if use_adaptive_temp: procs.append(AdaptiveTemp(base_T=base_T, target_H=target_H, k=k))
    if per_token_temp:    procs.append(PerTokenTemp(wake_ids, stop_ids, Thot=Thot, Tcool=Tcool))
    if gate_stops:        procs.append(GateStops(stop_ids, penalty=gate_penalty))
    if use_entmax and ENTMAX_OK: procs.append(EntmaxAsLogits())
    if use_blend:
        assert base_model is not None, "use_blend=True requires base_model"
        procs.append(LogitBlend(base_model, lam=blend_lambda))
    return procs

"""steering vector"""

# Compute a single Joyce direction from embeddings
with torch.no_grad():
    E = model.get_input_embeddings().weight.detach()
    ids_wake = torch.tensor([tok.convert_tokens_to_ids(t) for t in new_terms if tok.convert_tokens_to_ids(t)!=tok.unk_token_id], device=E.device)
    ids_anchor = torch.tensor([tok.convert_tokens_to_ids(t) for t in ["book","river","history","night","language"] if tok.convert_tokens_to_ids(t)!=tok.unk_token_id], device=E.device)
    joyce_dir = (E[ids_wake].mean(0) - E[ids_anchor].mean(0))
    joyce_dir = joyce_dir / (joyce_dir.norm()+1e-9)

orig_forward = model.forward
def forward_steered(*args, strength=0.8, **kwargs):
    out = orig_forward(*args, **kwargs)
    output_hidden_states=True
    if hasattr(out, "hidden_states") and out.hidden_states is not None:
        hs = out.hidden_states[-1] + strength * joyce_dir
        out.logits = model.lm_head(hs)
    return out

model.config.output_hidden_states = True
model.forward = forward_steered

"""helper for sampling"""

def complete_portfolio(prompt, processors, max_new_tokens=120, temperature=0.9, top_p=0.88, top_k=40):
    ins = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **ins, max_new_tokens=max_new_tokens, do_sample=True,
            temperature=temperature, top_p=top_p, top_k=top_k,
            logits_processor=processors, renormalize_logits=True,
            no_repeat_ngram_size=4, cache_implementation="static",
            output_hidden_states=getattr(model.config, "output_hidden_states", False)
        )
    return tok.decode(out[0], skip_special_tokens=True)

"""Minimal ablation grid"""

def save_ablation_grid(new_terms, prompts, base_model=None):
    Path("results").mkdir(exist_ok=True)
    configs = [
        ("baseline", dict(wake_boost=0.0, stop_suppress=0.0, use_adaptive_temp=False, per_token_temp=False, gate_stops=False, use_entmax=False, use_blend=False)),
        ("logit_surgery+adaptT", dict(wake_boost=0.35, stop_suppress=0.25, use_adaptive_temp=True, per_token_temp=True, gate_stops=True)),
        ("entmax", dict(wake_boost=0.35, stop_suppress=0.25, use_adaptive_temp=False, per_token_temp=False, gate_stops=False, use_entmax=True)),
        ("blend_base", dict(wake_boost=0.25, stop_suppress=0.15, use_blend=True)),
    ]
    lines = ["# Ablation Samples\n"]
    for name, kw in configs:
        procs = build_portfolio_processors(tok, new_terms, base_model=base_model, **kw)
        lines.append(f"## {name}\n")
        for p in prompts:
            txt = complete_portfolio(p, procs, max_new_tokens=120)
            lines += [f"### {p}\n", textwrap.fill(txt.replace("\n"," "), width=92), "\n"]
    Path("results/ablation_samples.md").write_text("\n".join(lines), encoding="utf-8")
    print("Wrote results/ablation_samples.md")
