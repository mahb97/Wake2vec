{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/try_again_fail_again_fail_better.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b14524",
      "metadata": {
        "id": "81b14524"
      },
      "source": [
        "# Wake2vec — Token Injection & Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "why do I feel so sad without t4 and so happy with it lol"
      ],
      "metadata": {
        "id": "PSJOfhEXnQlA"
      },
      "id": "PSJOfhEXnQlA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b082761",
      "metadata": {
        "id": "0b082761"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers==4.43.3 accelerate peft==0.11.1 datasets umap-learn faiss-cpu matplotlib==3.8.4 sentencepiece\n",
        "\n",
        "import os, re, math, random, json, unicodedata\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# distilgpt2 for CPU smoke tests\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a8b2c8",
      "metadata": {
        "id": "05a8b2c8"
      },
      "outputs": [],
      "source": [
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=DTYPE, device_map=\"auto\")\n",
        "print(\"Base vocab size:\", tok.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed131f58",
      "metadata": {
        "id": "ed131f58"
      },
      "outputs": [],
      "source": [
        "# wake_lexicon.txt\n",
        "lex_path = None\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    lex_path = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    lex_path = \"/content/wake_lexicon.txt\"\n",
        "\n",
        "raw = []\n",
        "p = Path(lex_path)\n",
        "if p.exists():\n",
        "    raw = p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "else:\n",
        "    print(\"WARNING: wake_lexicon.txt not found; provide a path or upload.\")\n",
        "\n",
        "seen = set(); new_terms = []\n",
        "existing = tok.get_vocab()\n",
        "for w in raw:\n",
        "    t = unicodedata.normalize(\"NFC\", w.strip())\n",
        "    if t and (t not in seen) and (t not in existing):\n",
        "        seen.add(t); new_terms.append(t)\n",
        "\n",
        "print(f\"Loaded {len(new_terms)} NEW tokens.\")\n",
        "print('Sample:', new_terms[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0088dc9c",
      "metadata": {
        "id": "0088dc9c"
      },
      "outputs": [],
      "source": [
        "added = tok.add_tokens(new_terms, special_tokens=False)\n",
        "if added != len(new_terms):\n",
        "    print(f\"Note: only {added}/{len(new_terms)} newly added (collisions or tokenizer rules).\")\n",
        "model.resize_token_embeddings(len(tok))\n",
        "print(\"New vocab size:\", tok.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e6be70",
      "metadata": {
        "id": "c5e6be70"
      },
      "outputs": [],
      "source": [
        "# come to clean the mess: add start-of-word variants for SentencePiece-style tokenizers and tie output head\n",
        "START = \"▁\"\n",
        "prefixed = [START + t for t in new_terms if not t.startswith(START)]\n",
        "prefixed = [t for t in prefixed if t not in tok.get_vocab()]\n",
        "added2 = tok.add_tokens(prefixed, special_tokens=False)\n",
        "model.resize_token_embeddings(len(tok))\n",
        "print(f\"Added {added2} start-of-word variants. Vocab: {tok.vocab_size}\")\n",
        "\n",
        "# Tie lm_head to input embeddings so generations reflect updated embeddings\n",
        "if model.get_output_embeddings() is not None:\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight\n",
        "    print(\"Tied lm_head to input embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe62c99c",
      "metadata": {
        "id": "fe62c99c"
      },
      "outputs": [],
      "source": [
        "emb = model.get_input_embeddings()\n",
        "\n",
        "def init_vec(term: str, noise=0.02):\n",
        "    ids = tok(term, add_special_tokens=False)[\"input_ids\"]\n",
        "    if ids:\n",
        "        base = emb.weight[ids].mean(0, keepdim=True)\n",
        "    else:\n",
        "        base = torch.randn_like(emb.weight[0:1])\n",
        "    return (base + noise * torch.randn_like(base)).squeeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    targets = new_terms + [(\"▁\"+t) for t in new_terms if not t.startswith(\"▁\")]\n",
        "    for t in targets:\n",
        "        idx = tok.convert_tokens_to_ids(t)\n",
        "        if idx != tok.unk_token_id:\n",
        "            emb.weight[idx].copy_(init_vec(t))\n",
        "print(\"Initialised bare + ▁ variants.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf62300",
      "metadata": {
        "id": "eaf62300"
      },
      "outputs": [],
      "source": [
        "# Freeze almost everything, train embeddings (optional tiny LoRA for attention projections)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "model.get_input_embeddings().weight.requires_grad_(True)\n",
        "\n",
        "USE_LORA = True  # if False isolates pure embedding drift\n",
        "if USE_LORA:\n",
        "    lora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "                          target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "                          bias=\"none\", task_type=\"CAUSAL_LM\")\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Trainable params:\", trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a25ab66",
      "metadata": {
        "id": "6a25ab66"
      },
      "outputs": [],
      "source": [
        "# build Wake-saturated paragraph windows, split into train/val\n",
        "fw_path = None\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    fw_path = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    fw_path = \"/content/finnegans_wake.txt\"\n",
        "\n",
        "fw = Path(fw_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "fw = unicodedata.normalize(\"NFC\", fw).replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
        "\n",
        "import re, random\n",
        "paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", fw) if p.strip()]\n",
        "probe = {t.lower() for t in new_terms if len(t) <= 40}\n",
        "\n",
        "hits = [i for i,p in enumerate(paras) if any(t in p.lower() for t in probe)]\n",
        "RADIUS = 2\n",
        "windows = [\" \".join(paras[max(0,i-RADIUS):min(len(paras), i+RADIUS+1)]) for i in hits]\n",
        "windows = list(dict.fromkeys(windows))\n",
        "random.shuffle(windows)\n",
        "if len(windows) < 500:\n",
        "    windows += random.sample(paras, k=min(2000, len(paras)))\n",
        "\n",
        "split = int(0.9 * len(windows))\n",
        "train_texts, val_texts = windows[:split], windows[split:]\n",
        "\n",
        "def tok_fn(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, max_length=512)\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\"text\": train_texts}).map(tok_fn, batched=True, remove_columns=[\"text\"]),\n",
        "    \"validation\": Dataset.from_dict({\"text\": val_texts}).map(tok_fn, batched=True, remove_columns=[\"text\"]),\n",
        "})\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "print(\"Train/Val sizes:\", len(ds[\"train\"]), len(ds[\"validation\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdc4891",
      "metadata": {
        "id": "1bdc4891"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/Wake2vec_adapter\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=6e-4,\n",
        "    num_train_epochs=3.0,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    logging_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[]\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"validation\"],\n",
        "    data_collator=collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/Wake2vec_adapter/final\")\n",
        "tok.save_pretrained(\"/content/Wake2vec_adapter/final_tok\")\n",
        "print(\"Saved adapter + tokenizer to /content/Wake2vec_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add0ba60",
      "metadata": {
        "id": "add0ba60"
      },
      "outputs": [],
      "source": [
        "import unicodedata, re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "LETTER = re.compile(r\"^[A-Za-zÀ-ÖØ-öø-ÿ][A-Za-zÀ-ÖØ-öø-ÿ'’-]*$\")\n",
        "\n",
        "def is_latin(ch):\n",
        "    try: return (\"LATIN\" in unicodedata.name(ch)) or ch in \"'’-\"\n",
        "    except ValueError: return False\n",
        "\n",
        "def is_wordlike(tok):\n",
        "    if \"▁\" in tok: return False\n",
        "    if not LETTER.match(tok): return False\n",
        "    return all(is_latin(c) for c in tok)\n",
        "\n",
        "vocab = tok.get_vocab(); inv = {i:t for t,i in vocab.items()}\n",
        "allowed_ids = [i for i,t in inv.items() if is_wordlike(t) or t in new_terms or (\"▁\"+t) in new_terms]\n",
        "\n",
        "W = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "Wa = W[allowed_ids] / (np.linalg.norm(W[allowed_ids], axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def clean_neighbors(terms, k=8):\n",
        "    out = {}\n",
        "    for t in terms[:30]:\n",
        "        tid = tok.convert_tokens_to_ids(t)\n",
        "        if tid == tok.unk_token_id: continue\n",
        "        v = W[tid:tid+1] / (np.linalg.norm(W[tid:tid+1], axis=1, keepdims=True) + 1e-12)\n",
        "        sims = (v @ Wa.T)[0]\n",
        "        top = np.argpartition(-sims, range(min(k, len(sims))))[:k]\n",
        "        top = top[np.argsort(-sims[top])]\n",
        "        out[t] = [inv[allowed_ids[j]] for j in top]\n",
        "    return out\n",
        "\n",
        "probe = new_terms[:12]\n",
        "nn = clean_neighbors(probe, k=8)\n",
        "for k,v in nn.items():\n",
        "    print(f\"{k:>20} -> {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f9196e",
      "metadata": {
        "id": "c6f9196e"
      },
      "outputs": [],
      "source": [
        "def complete(prompt, max_new_tokens=60):\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=1.15,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "tests = [\n",
        "    \"By the river I thought of\",\n",
        "    \"At night I dream of\",\n",
        "    \"In the story of this book,\",\n",
        "    \"Explain gradient descent in the style of Joyce:\"\n",
        "]\n",
        "for p in tests:\n",
        "    print(\"\\n===\", p, \"\\n\", complete(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53095d1e",
      "metadata": {
        "id": "53095d1e"
      },
      "outputs": [],
      "source": [
        "# Export minimal adapter pack: changed embedding rows (bare + ▁ forms)\n",
        "save_dir = Path(\"/content/Wake2vec_adapter/minipack\"); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "changed = [tok.convert_tokens_to_ids(t) for t in (new_terms + [\"▁\"+t for t in new_terms if not t.startswith(\"▁\")])]\n",
        "changed = [i for i in changed if i != tok.unk_token_id]\n",
        "emb_slice = model.get_input_embeddings().weight[changed].detach().cpu().numpy()\n",
        "np.save(save_dir / \"new_token_ids.npy\", np.array(changed, dtype=np.int32))\n",
        "np.save(save_dir / \"new_token_vectors.npy\", emb_slice)\n",
        "(Path(save_dir / \"README.txt\").write_text(\n",
        "    \"Rows from input embedding for Wake2vec tokens (bare + ▁forms).\\n\"\n",
        "    \"Apply to base model by index assignment.\\n\", encoding=\"utf-8\"))\n",
        "print(\"Mini pack saved:\", save_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}