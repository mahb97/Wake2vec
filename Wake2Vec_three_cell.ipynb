{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc4/wX+UYqUpCFNoxu4w5T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wake2vec/blob/main/Wake2Vec_three_cell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set up & data"
      ],
      "metadata": {
        "id": "35G2w-QzEk6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate datasets adafactor bitsandbytes scikit-learn umap-learn matplotlib\n",
        "from pathlib import Path\n",
        "import json, math, random, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "ROOT = Path(\".\")\n",
        "FW = (ROOT/\"data\"/\"FW_TEXT.txt\").read_text(encoding=\"utf-8\")\n",
        "MORPHEMES = [l.strip().split(\",\") for l in (ROOT/\"data\"/\"morphemes.csv\").read_text().splitlines()[1:]]\n",
        "prefixes = {}\n",
        "suffixes = {}\n",
        "for pfx,sfx,ex in MORPHEMES:\n",
        "    if pfx: prefixes.setdefault(pfx, []).append(ex)\n",
        "    if sfx: suffixes.setdefault(sfx, []).append(ex)\n",
        "\n",
        "def synthetic_words(n=1200, roots=(\"river thunder word sound dance queen storm tree night sun rain book\").split()):\n",
        "    out=set()\n",
        "    for _ in range(n*2):\n",
        "        p = random.choice(list(prefixes.keys()))\n",
        "        s = random.choice(list(suffixes.keys()))\n",
        "        r = random.choice(roots)\n",
        "        out.add(f\"{p}{r}{s}\")\n",
        "    return list(out)[:n]\n",
        "\n",
        "syn = synthetic_words()\n",
        "base = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "tok = AutoTokenizer.from_pretrained(base, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch.float32, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "xj87Ddx_FALK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "expand tokenizer & compose embeddings"
      ],
      "metadata": {
        "id": "IroSO4xuFIpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_vec(terms, emb, tok):\n",
        "    vecs=[]\n",
        "    for t in terms:\n",
        "        ids = tok.encode(t, add_special_tokens=False)\n",
        "        if len(ids)==1:\n",
        "            vecs.append(emb.weight.data[ids[0]])\n",
        "    return torch.stack(vecs).mean(0) if vecs else None\n",
        "\n",
        "new_tokens = [w for w in syn if all(len(tok.encode(w, add_special_tokens=False))>1 for _ in [0])]\n",
        "added = tok.add_tokens(new_tokens, special_tokens=False)\n",
        "model.resize_token_embeddings(len(tok), mean_resizing=False)\n",
        "\n",
        "emb = model.get_input_embeddings()\n",
        "with torch.no_grad():\n",
        "    alpha = 0.25\n",
        "    std = emb.weight.data.std().item()\n",
        "    for w in new_tokens:\n",
        "        # crude split guess: find first prefix and last suffix match\n",
        "        p = next((p for p in prefixes if w.startswith(p)), None)\n",
        "        s = next((s for s in suffixes if w.endswith(s)), None)\n",
        "        root = w[len(p):len(w)-len(s)] if (p and s and len(w)>len(p)+len(s)) else w\n",
        "        vp = avg_vec(prefixes.get(p, []), emb, tok)\n",
        "        vs = avg_vec(suffixes.get(s, []), emb, tok)\n",
        "        vr_ids = tok.encode(root, add_special_tokens=False)\n",
        "        vr = emb.weight.data[vr_ids[0]] if len(vr_ids)==1 else torch.randn(emb.embedding_dim)*std*0.5\n",
        "        comp = (alpha*(vp if vp is not None else vr)\n",
        "                + (1-2*alpha)*vr\n",
        "                + alpha*(vs if vs is not None else vr))\n",
        "        comp = comp + torch.randn_like(comp)*std*0.01\n",
        "        emb.weight.data[tok.convert_tokens_to_ids(w)] = comp\n",
        "\n",
        "# tie head\n",
        "with torch.no_grad():\n",
        "    model.lm_head.weight = emb.weight"
      ],
      "metadata": {
        "id": "5U8wpfC5FB_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "two-phase training + quick eval"
      ],
      "metadata": {
        "id": "ZTwMHrEwFQ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "\n",
        "def make_blocks(text, max_len=2048, stride=1024):\n",
        "    ids = tok.encode(text, add_special_tokens=False)\n",
        "    blocks=[]\n",
        "    for i in range(0, len(ids)-max_len, stride):\n",
        "        blocks.append({\"input_ids\": ids[i:i+max_len]})\n",
        "    return blocks\n",
        "\n",
        "train_text = \"\\n\".join(random.sample(syn, k=400)) + \"\\n\" + FW[:600000]\n",
        "valid_text = FW[600000:630000]\n",
        "\n",
        "train_ds = Dataset.from_list(make_blocks(train_text))\n",
        "valid_ds = Dataset.from_list(make_blocks(valid_text))\n",
        "\n",
        "dc = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "def freeze_all_but_embeddings(m):\n",
        "    for p in m.parameters(): p.requires_grad=False\n",
        "    for p in m.get_input_embeddings().parameters(): p.requires_grad=True\n",
        "    for p in m.lm_head.parameters(): p.requires_grad=True\n",
        "\n",
        "# embeddings only\n",
        "freeze_all_but_embeddings(model)\n",
        "model.config.use_cache = False\n",
        "args1 = TrainingArguments(\n",
        "    output_dir=\"runs/phase1\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "Trainer(model=model, args=args1, train_dataset=train_ds, eval_dataset=valid_ds, data_collator=dc).train()\n",
        "\n",
        "# fine-tune\n",
        "for p in model.parameters(): p.requires_grad=True\n",
        "args2 = TrainingArguments(\n",
        "    output_dir=\"runs/phase2\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=2,\n",
        "    warmup_ratio=0.10,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "Trainer(model=model, args=args2, train_dataset=train_ds, eval_dataset=valid_ds, data_collator=dc).train()\n",
        "\n",
        "# quick neighbor diag (top-5 overlap proxy)\n",
        "with torch.no_grad():\n",
        "    W = model.get_input_embeddings().weight.detach().cpu()\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import numpy as np, json\n",
        "    ids_new = [tok.convert_tokens_to_ids(t) for t in new_tokens]\n",
        "    sim = cosine_similarity(W[ids_new], W)\n",
        "    top5 = np.argsort(-sim, axis=1)[:,1:6]\n",
        "    stats = {\"new_token_count\": len(new_tokens), \"example_top5_ids\": top5[:5].tolist()}\n",
        "    Path(\"runs/phase2/metrics\").mkdir(parents=True, exist_ok=True)\n",
        "    (Path(\"runs/phase2/metrics\")/\"quick_neighbors.json\").write_text(json.dumps(stats, indent=2))\n",
        "print(\"Saved runs/phase2/metrics/quick_neighbors.json\")"
      ],
      "metadata": {
        "id": "DDyLwx06FSf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}